[
  {
    "objectID": "use_cases.html",
    "href": "use_cases.html",
    "title": "RDM use cases",
    "section": "",
    "text": "Here, you will find practical examples demonstrating the application of omics data.\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "RDM use cases"
    ]
  },
  {
    "objectID": "cards/JARomero.html",
    "href": "cards/JARomero.html",
    "title": "Jos√© Alejandro Romero Herrera",
    "section": "",
    "text": "Alex is a former Sandbox data scientist at the University of Copenhagen. He is currently working at Lundbeck as a principal bioinformatician.\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/examples/mkdocs_pages.html",
    "href": "develop/examples/mkdocs_pages.html",
    "title": "Build your GitHub Page using Mkdocs",
    "section": "",
    "text": "Build your GitHub Page using Mkdocs\nInstall MkDocs and MkDocs extensions using the command line. Additional extensions are optional but can be useful if you choose this approach.\npip install mkdocs # create webpages\npip install mkdocs-material # customize webpages\npip install mkdocs-video # add videos or embed videos from other sources\npip install mkdocs-minify-plugin # Minimize html code\npip install mkdocs-git-revision-date-localized-plugin # display last updated date \npip install mkdocs-jupyter # include Jupyter notebooks\npip install mkdocs-bibtex # add references in your text (`.bib`)\npip install neoteroi-mkdocs # create author cards\npip install mkdocs-table-reader-plugin # embed tabular format files (`.tsv`)\n\n\n\n\n\n\nExercise 5: make a project folder and publish a data analysis webpage\n\n\n\n\n\n\n\n\nConfigure your main GitHub Page and its repo\nThe first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps. In a Markdown document, outline the primary objectives of the organization and provide an overview of ongoing research projects. After you have created the organization/usernamegithub.io, it is time to configure your Project repository webpage using MkDocs!\nStart a new project from Cookiecutter or use one from the previous exercise.\nIf you use a Project repo from the first exercise, go to the next paragraph. Using Cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.\nNext, link your data of interest (or create a small fake dataset) and make an example of a data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or R Markdown files. The extensions that we have installed using pip allow you to directly add a Jupyter Notebook file to the mkdocs.yml navigation section. On the other hand, if you are using R Markdown files, you will have to knit your document into either an HTML page or a GitHub document.\nFor the purposes of this exercise, we have already included a basic index.md markdown file that can serve as the intro page of your repo, and a jupyter_example.ipynb with some code in it. You are welcome to modify them further to test them out!\nUse MkDocs to create your webpage\nWhen you are happy with your files and are ready to publish them, make sure to add, commit, and push the changes to the remote. Then, build up your webpage using MkDocs and the mkdocs gh-deploy command from the same directory where the mkdocs.yml file is. For example, if your mkdocs.yml for your Project folder is in /Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml, do cd /Users/JARH/Projects/project1_JARH_20231010/ and then mkdocs gh-deploy. This requires a couple of changes in your GitHub organization settings.\nRemember to make sure that your markdowns, images, reports, etc., are included in the docs folder and properly set up in the navigation section of your mkdocs.yml file.\nFinally, we only need to set up the GitHub Project repo settings.\nPublishing your GitHub Page\nGo to your GitHub repo settings and configure the Page section. Since you are using the mkdocs gh-deploy command to publish your site in the gh-pages branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website. You will need to configure the settings of this repository in GitHub so that the Page is taken from the gh-pages branch and the root folder.\n\n\n\nGitHub Pages setup\n\n\n\nBranch should be gh-pages\nFolder should be root\n\nAfter a couple of minutes, your webpage should be ready! You should be able to see your webpage through the link provided in the Page section!\n\nNow it is also possible to include this repository webpage in your main webpage &lt;organization&gt;.github.io by including the link of the repo website (https://&lt;organization&gt;.github.io/repo-name) in the navigation section of the mkdocs.yml file in the main organizationgithub.io repo.\n\n\n\n\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "General",
      "Build your GitHub Page using Mkdocs"
    ]
  },
  {
    "objectID": "develop/examples/NGS_OS_FAIR.html",
    "href": "develop/examples/NGS_OS_FAIR.html",
    "title": "Applied Open Science and FAIR principles to NGS",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Apply Open Science and FAIR principles to your data\n\n\nYou should consider revisiting these exercises and examples after completing lesson 1 in the course material.\n\n\n\n\n\n\nExercise 1: FAIR principles in Research practice\n\n\n\n\n\n\n\nThink about your current research project or a hypothetical one. How could you apply Open Science and FAIR principles to improve the transparency, accessibility, and reusability of your research data and/or data analyses? Consider aspects such as data sharing, documentation, metadata standards, and licensing. Write down three specific actions you could take to implement Open Science and FAIR principles into your research workflow.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nExperiments/input/raw data\n\nStandardized file formats and data structure\nStandardized metadata of raw files and results (describing origin, format, etc.)\nDepositing raw data in public repositories or data archives\n\nPipelines/workflows\n\nUse data repositories and document your workflows (software tools, parameters/settings, versions‚Ä¶)\nDocumenting all preprocessing scripts and employed pipelines\nDevelopment environments\nUsing version control systems (git, GitHub, GitLab)\n\nSoftware/code\n\nPipeline scripts and software code openly accessible (commented code)\nUse data repositories (and version control)\nProvide a README file (installation and usage instructions, license and citation information, etc.)\nOpen-source licensing\nFollow these recommendations: FAIR software\n\n\n\n\n\n\n\n\n\n\n\nNote that not all data needs to be archived and deposited. NGS data processing generates vast amounts of data that might not need to be shared publicly, as long as you describe how you produced the data. For example, in a usual bulk RNAseq experiment, FASTQ reads are cleaned and subsequently aligned to a reference genome, creating a subset of cleaned FASTQ files and BAM files. After transcript/gene quantification, you can obtain a final count matrix that can be used for data analysis, such as Differential Expression Analysis. If you provide enough documentation on how these files were processed (which software, which versions, and which options), you won‚Äôt need to deposit either the cleaned or aligned reads, only the original FASTQ files and the final result of your preprocessing. This will save quite the computational resources and metadata needed to preserve the intermediary data. Providing documentation on how the data was generated is much simpler if you are using community-curated pipelines such as the ones created by the nf-core community.\n\nOpen Science\nUnless your data is of sensitive nature (human individual samples, patient data, or anything protected), you should always deposit your data with as few restrictions as possible. This includes publishing your manuscript in Open Access journals as well! For your generated NGS data, our suggestion is that you use a license such as Creative Commons license like CC-BY license, which only requires users to attribute the source of the data, but this also depends on the repository that you use for your data. We will see more about it in the lesson 7\n\n\n\nCC licenses\n\n\n\n\nFAIR principles\nNext, we will see how we can apply each of the FAIR principles to your NGS data.\n\nFindable\nTo make your NGS data easy to find, you should deposit it in a domain-specific repository, such as the Gene Expression Omnibus or Annotare. We will see more about them in lesson 7. Both of these repositories will help you give your data a unique identifier, and provide information (metadata) on how the data was generated. The metadata you include in your submission should contain, at least, the minimum necessary information to understand what kind of data it is submitted, and how it was generated. This includes:\n\nSample metadata in tabular format, containing information about the samples used in the experiment as well as variables of interest for the analysis.\nExperiment metadata, including data provenance, that is, how the samples were obtained, from which organism, following what protocols, kits, sequencing libraries, sequencing method, and data preprocessing workflows/pipelines. This is usually submitted as part of a submission form and it depends on the repository.\nKeywords, such as type of NGS data, conditions or diseases studied in the experiment, organisms used, genes studied, etc.\n\n\n\nAccessible\nBoth GEO and Annotare repositories promote the use of unrestricted access to the data. In the case of Annotare, deposited data is under CC0 license, while GEO states deposited data is public domain. Depositing your data will require you to have an account but it does not require authentication from the user to access and download the data.\n\n\n\nAnnotare main page\n\n\n\n\n\n\n\n\nOn sensitive data\n\n\n\nIf you would like to deposit sensitive data that need controlled access, it is possible to do so through the European Genome-phenome Archive (EGA).\n\n\nIn addition, if you have deposited your data with rich metadata, as explained in the previous step, it will be easier for users to query your data by date, author, organism, type of NGS data, etc etc.\n\n\nInteroperable\nBy using standard bioinformatics formats, such as fastq files for raw NGS data, count matrices in tabular format, BED files for peak calling results, etc., you are already complying to this section. In addition, GEO and Annotare repositories are compliant to NGS data standards, such as MIAME/MINSEQE/MINSCE guidelines.\nNonetheless, this is the easy part! Adhering to controlled vocabularies seems to be the most difficult part of the FAIR principles. Here are some cases:\n\nUsing organism names instead of their taxonomy. For example: mouse instead of Mus musculus, or human, instead of Homo sapiens. Even better, we should use a taxonomy ID, such as the NCBI taxonomy ID for humans NCBITaxon_9606, which will unequivocally refer to humans.\nUsing gene names or symbols instead of gene IDs. For example: the gene POU5F1 has many synonyms, like OCT4, OCT, and OTF4. To be explicit, it is better to reference the gene ID, like an ENSEMBL gene ID ENSG00000204531.\nUsing disease names instead of disease IDs. Again, this will reference specifically the disease you mention.\n\nThere are many more stances where you can use controlled vocabularies for other variables of interest, like cell type, tissue, cell cycle, etc. We will see in the metadata lesson where you can find controlled vocabularies for different variables of interest in NGS data.\n\n\nReusable\nIn order for your NGS data to be reusable, you will have to provide thorough documentation on how it was generated, as well as the terms (that is a license) on how the data can be used/retrieved. We have talked already about collecting metadata on how the samples were generated (laboratory protocols, sequencing library, kits, technology, etc) and processed (workflows or pipelines along with the software used, which versions, and options). We also talked about what type of standard file formats you should use, such as fastq files for raw data and tabular formats for sample metadata. Finally, we have discussed in the Open Science section that you should try to license your data as freely as possible, like a CC0 license or CC-BY license. If your data is of sensitive nature and has restricted access or conditions, you should instead provide information on how other people can access the data, as well as any agreements or ethical approvals necessary for its reuse.\n\n\n\nSources\n\nElixir Belgium: https://rdm.elixir-belgium.org/omics_data\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Applied Open Science and FAIR principles to NGS"
    ]
  },
  {
    "objectID": "develop/examples/NGS_metadata.html",
    "href": "develop/examples/NGS_metadata.html",
    "title": "NGS Assay and Project metadata",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nDevelop your metadata\n\n\n\nYou should consider revisiting these examples after completing lesson 4 in the course material. Please review these three tables containing pre-filled data fields for metadata, each serving distinct purposes: sample metadata, project metadata, and experimental metadata.\n\nProject metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation\n\n\n\n\n\n\n\n\n\n\nSample metadata fields\nSome details might be specific to your samples. For example, which samples are treated, which are controlled, which tissue they come from, which cell type, the age, etc. Here is a list of possible metadata fields that you can use:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nsample\nName of the sample\nNA\nNA\ncontrol_rep1, treat_rep1\n\n\nfastq_1\nPath to fastq file 1\nNA\nNA\nAEG588A1_S1_L002_R1_001.fastq.gz\n\n\nfastq_2\nPath to paired fastq file, if it is a paired experiment\nNA\nNA\nAEG588A1_S1_L002_R2_001.fastq.gz\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;unstranded OR forward OR reverse \\&gt;\nNA\nunstranded\n\n\ncondition\nVariable of interest of the experiment, such as \"control\", \"treatment\", etc\nwordWord\ncamelCase\ncontrol, treat1, treat2\n\n\ncell_type\nThe cell type(s) known or selected to be present in the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ntissue\nThe tissue from which the sample was taken\nNA\nUberon\nNA\n\n\nsex\nThe biological/genetic sex of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ncell_line\nCell line of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\norganism\nOrganism origin of the sample\n&lt;Genus species&gt;\nTaxonomy\nMus musculus\n\n\nreplicate\nReplicate number\n&lt;integer\\&gt;\nNA\n1\n\n\nbatch\nBatch information\nwordWord\ncamelCase\n1\n\n\ndisease\nAny diseases that may affect the sample\nNA\nDisease Ontology or MONDO\nNA\n\n\ndevelopmental_stage\nThe developmental stage of the sample\nNA\nNA\nNA\n\n\nsample_type\nThe type of the collected specimen, eg tissue biopsy, blood draw or throat swab\nNA\nNA\nNA\n\n\nstrain\nStrain of the species from which the sample was collected, if applicable\nNA\nontology field - e.g. NCBITaxonomy\nNA\n\n\ngenetic variation\nAny relevant genetic differences from the specimen or sample to the expected genomic information for this species, eg abnormal chromosome counts, major translocations or indels\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nAssay metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\nThe metadata must include key details such as the project‚Äôs short description, author information, creation date, experimental protocol, assay ID, assay type, platform utilized, library details, keywords, sample count, paired-end status, processor information, organism studied, sample origin, and file path.\nIf you would create a database from the metadata files, your table should look like this (each row corresponding to one project):\n\n\n\n\n\n\n\n\n\nassay_ID\nassay_type\nassay_subtype\nowner\nplatform\nextraction_method\nlibrary_method\nexternal_accessions\nkeyword\ndate\nnsamples\nis_paired\npipeline\nstrandedness\nprocessed_by\norganism\norigin\npath\nshort_desc\nELN_ID\n\n\n\n\nRNA_oct4_20200101\nRNAseq\nbulk RNAseq\nSarah Lundregan\nNextSeq 2000\nNA\nNA\nNA\noct4\n20200101\n9\npaired\nnf-core/chipseq 2.3.1\n*\nSL\nMus musculus\ninternal\nNA\nBulk RNAseq of Oct4 knockout\n234\n\n\nCHIP_oct4_20200101\nChIPseq\nbulk ChIPseq\nJose Romero\nNextSeq 2000\nNA\nNA\nNA\noct4\n20200101\n9\nsingle\nnf-core/rnaseq 3.12.0\n*\nJARH\nMus musculus\ninternal\nNA\nBulk ChIPseq of Oct4 overexpression\n123\n\n\nCHIP_med1_20190204\nChIPseq\nbulk ChIPseq\nMartin Proks\nNextSeq 2000\nNA\nNA\nNA\nmed1\n20190204\n12\nsingle\nnf-core/rnaseq 3.12.0\n*\nMP\nMus musculus\ninternal\nNA\nBulk ChIPseq of Med1 overexpression\n345\n\n\nSCR_humanSkin_20210302\nRNAseq\nsingle cell RNAseq\nJose Romero\nNextSeq 2000\nNA\nNA\nNA\nhumanSkin\n20210302\n23123\npaired\nnf-core/scrnaseq 1.8.2\n*\nJARH\nHomo sapiens\nexternal\nNA\nscRNAseq analysis of human skin development\nNA\n\n\nSCR_humanBrain_20220610\nRNAseq\nsingle cell RNAseq\nMartin Proks\nNextSeq 2000\nNA\nNA\nNA\nhumanBrain\n20220610\n1234\npaired\ncustom\n*\nMP\nHomo sapiens\nexternal\nNA\nscRNAseq analysis of human brain development\nNA\n\n\n\n\n\n\n\n\n\n\nSources\n\nTranscriptomics metadata standards and fields\nBiological ontologies for data scientists,Bionty\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "NGS data",
      "NGS Assay and Project metadata"
    ]
  },
  {
    "objectID": "develop/contributors.html",
    "href": "develop/contributors.html",
    "title": "Practical material",
    "section": "",
    "text": "Alba Refoyo Martinez :custom-orcid::simple-github:\nJose Alejandro Romero Herrera :custom-orcid: :simple-github:\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/07_repos.html",
    "href": "develop/07_repos.html",
    "title": "7. Storing and sharing biodata",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nRepositories for managing biological data\nArchive your GitHub data analysis repositories\nWhile platforms like GitHub excel in version control and collaborative coding, repositories like Zenodo, Gene Expression Omnibus, and Annotare specialize in archiving and sharing scientific data, ensuring long-term accessibility for the global research community.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/07_repos.html#data-repositories-and-archives",
    "href": "develop/07_repos.html#data-repositories-and-archives",
    "title": "7. Storing and sharing biodata",
    "section": "Data Repositories and Archives",
    "text": "Data Repositories and Archives\nSpecialized repositories and archives securely store, curate, and disseminate scientific data, ensuring long-term preservation, transparency, and citability of research findings through standardized formats and rigorous curation processes.\nImportance of archiving scientific data\n\nLong-term accessibility and preservation: Ensures data remains accessible for future researchers.\nEnhanced visibility and attribution: Unique identifiers like DOIs enable citation of datasets, enhancing visibility and proper attribution.\nImproved dataset discoverability and interpretability: Comprehensive metadata, including methodology and experimental details, facilitates understanding and usability by other researchers.\nPromotion of Transparency, Reproducibility, and Research Integrity: Mandatory data deposition fosters transparency and upholds research integrity.\nAmplification of Research Impact and Contribution: Archiving data elevates research quality and extends its impact within the scientific community.\nFulfilling Scholarly Obligations: Compliance with requirements set by scientific journals and funding agencies ensures adherence to scholarly standards.\n\nThere are two types of repositories:\n\nGeneral repositories: relevant to a wide range of disciplines (e.g.¬†Zenodo).\nDomain-specific: repositories are customized for specific fields, providing specialized curation and context-specific features (e.g.¬†ENA, GEO, Annotare, etc.).\n\n\n\n\n\n\n\nList of repositories for biological data\n\n\n\n\nEuropean Nucleotide Archive (ENA)\nNCBI Gene Expression Omnibus (GEO)\nSequence Read Archive (SRA)\nProtein Data Bank (PDB)\nProteomics Identifications Database (PRIDE)\nUniversal Protein Resource (UniProt) SPIN\nArrayExpress\nEBI Metagenomics (MGnify)\nPhysioNet\nFunctional Annotation of Animal Genomes (FAANG) Data Repository\n\nFor more data repositories, please refer to the links provided below to find the appropriate repository for your data:\n\nEMBL-EBI data resources here\nNHI data resources here\nELIXIR Core Data Resources here\n\nYour institution might as well have its repositories such as ERDA (Electronic research data archive at the University of Copenhagen).",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/07_repos.html#domain-specific-repositories",
    "href": "develop/07_repos.html#domain-specific-repositories",
    "title": "7. Storing and sharing biodata",
    "section": "Domain-specific repositories",
    "text": "Domain-specific repositories\nThis tailored approach ensures alignment with standards and maximizes the utility and impact of research findings. By catering to particular research areas, these repositories offer researchers a more focused audience, deeper domain expertise, and increased visibility within their specific research community.\nExplore some examples of NGS data repositories below:\n\n\n\n\n\n\nENA (European Nucleotide Archive)\n\n\n\n\n\n\n\nENA: hosted by the European Bioinformatics Institute (EBI), provides researchers with a platform to deposit and access nucleotide sequences along with associated metadata, ensuring data preservation and contextualization. ENA adheres to community standards and guidelines for data submission, including those established by the International Nucleotide Sequence Database Collaboration (INSDC).\n\n\n\n\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus)\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus): curated by the National Center for Biotechnology Information (NCBI), serves as a specialized repository for high-throughput functional genomic data sets, particularly gene expression data across diverse biological conditions and experimental designs. Researchers can easily deposit and access a variety of genomic data, fostering data transparency and reproducibility within the scientific community. GEO assigns unique accession numbers to each dataset, ensuring traceability and proper citation in research publications.\n\n\n\n\n\n\n\n\n\n\n\nAnnotare\n\n\n\n\n\n\n\nArrayExpress/Annotare: hosted by the European Bioinformatics Institute (EBI), is a specialized repository tailored for storing and submitting functional genomics experiments for high-throughput sequencing data. It offers researchers a platform to upload experimental data along with comprehensive metadata ensuring preservation and contextualization. Annotare provides a curated environment aligned with the standards and practices of the field. This specialization enhances data discoverability, promotes collaboration, and facilitates deeper insights into the functional aspects of the genome.\n\n\n\n\n\nThe repositories mentioned earlier adhere to established community standards for data submission and sharing in genomics research such as:\n\nMIAME (Minimum Information About a Microarray Experiment): These guidelines ensure comprehensive and standardized reporting of microarray experiments.\nMIxS (Minimum Information about a high-throughput SeQuencing Experiment): MIxS standards, developed by the Genomic Standards Consortium, ensure consistent reporting of metadata for high-throughput sequencing experiments.\nSequence Read Archive (SRA) Submission Guidelines: They include requirements for data formatting, metadata inclusion, and quality control.\nCommunity-Specific Standards designed to ensure that submitted data meets the specific requirements and expectations of the field.\n\nBy adhering to standards, repositories ensure that submitted data is high quality, well-documented, and compliant with community best practices, promoting data discovery, reproducibility, and interoperability within the scientific community.\nFollowing all the recommendations in this course makes it straightforward to provide the necessary documentation and information for these repositories. For instance, repositories specific to NGS data will require the raw FASTQ files, sample metadata, and protocols as well as final pre-processing results (for instance, read count matrices in BED files).\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that these repositories are not intended for downstream analysis data and associated code. However, you should already have those versions controlled by GitHub, which eliminates any concerns. You can then archive such repositories in a general repository like Zenodo.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/07_repos.html#general-repositories",
    "href": "develop/07_repos.html#general-repositories",
    "title": "7. Storing and sharing biodata",
    "section": "General repositories",
    "text": "General repositories\nZenodo is one of the widely used repositories for a variety of research outputs. It is an open-access digital platform supported by the European Organization for Nuclear Research (CERN) and the European Commission. It caters to various research outputs, including datasets, papers, software, and multimedia files, making it a valuable resource for researchers worldwide. With its user-friendly platform, researchers can easily upload, share, and preserve their research data. Each deposited item receives a unique Digital Object Identifier (DOI), ensuring citability and long-term accessibility. Additionally, Zenodo offers robust metadata capabilities for enriching submissions with contextual information. Moreover, researchers can link their GitHub accounts to Zenodo, simplifying the process of archiving the GitHub repository releases for long-term accessibility and citation.\nOnce your accounts are linked, creating a Zenodo archive becomes as straightforward as tagging a release in your GitHub repository. Zenodo automatically detects the release and generates a corresponding archive, complete with a unique Digital Object Identifier (DOI) for citable reference. Therefore, before submitting your work to a journal, link your data analysis repository to Zenodo, obtain a DOI, and cite it in your manuscript which enhances reproducibility in research.\n\nStep-by-Step Setup Guide\nCheck the practical material where we demonstrate how to link Zenodo and Github (see Exercise 6 in the practical material).",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/07_repos.html#wrap-up",
    "href": "develop/07_repos.html#wrap-up",
    "title": "7. Storing and sharing biodata",
    "section": "Wrap up",
    "text": "Wrap up\nIn this concluding lesson, we‚Äôve covered the process of submitting your data to a domain-specific repository and archiving your data analysis GitHub repositories in Zenodo. By applying the lessons from this workshop, you‚Äôll significantly enhance the FAIRness of your data and improve its organization for future use. These benefits extend beyond yourself to your teammates, group leader, and the wider scientific community.\nWe hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to contact us.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Storing and sharing biodata"
    ]
  },
  {
    "objectID": "develop/06_pipelines.html",
    "href": "develop/06_pipelines.html",
    "title": "6. Processing and analyzing biodata",
    "section": "",
    "text": "In this section, we explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community.\n\n\n\n\n\n\nBefore you start‚Ä¶\n\n\n\n\nChoose a folder structure (e.g., using cookiecutter)\nChoose a file naming system\nAdd a README describing the project (and the naming conventions)\nInstall and set up version control (e.g., Git and Github)\nChoose a coding style!\n\n\nPython: Python‚Äôs PEP or Google‚Äôs style guide\nR: Google‚Äôs style guide or Tidyverse‚Äôs style guide\n\n\n\n\n\nThrough techniques such as scripting, containerization (e.g., Docker), and virtual environments, researchers can create reproducible analyses that enable others to validate and build upon their work. Emphasizing the documentation of data processing steps, parameters, and results ensures transparency and accountability in research outputs. To write clear and reproducible code, take the following approach: write functions, code defensively (such as input validation, error handling, etc.), add comments, conduct testing, and maintain proper documentation.\nTools for reproducibility:\n\nCode notebooks: Utilize tools like Jupyter Notebook and R Markdown to combine code with descriptive text and visualizations, enhancing data documentation.\n\nIntegrated development environments: Consider using platforms such as (knitr or MLflow) to streamline code development and documentation processes.\nPipeline frameworks or workflow management systems: Implement systems like Nextflow and Snakemake to automate data analysis steps (including data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages.\n\n\n\nComputational notebooks (e.g., Jupyter, R Markdown) provide researchers with a versatile platform for exploratory and interactive data analysis. These notebooks facilitate sharing insights with collaborators and documentation of analysis procedures.\n\n\n\nTools such as Nextflow and Snakemake streamline and automate various data analysis steps, enabling parallel processing and seamless integration with existing tools. Remember to create portable code and use relative paths to ensure transferability between users.\n\nNextflow: offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments.\nSnakemake: Utilizing Python-based scripting, Snakemake allows for flexible and automated NGS data analysis pipelines, supporting parallel processing and integration with other tools.\n\nOnce your scientific computational workflow is ready to be shared, publish your scientific computational workflow on WorkflowHub.\n\n\n\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR‚Äôs renv: The ‚Äòrenv‚Äô package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems.\n\n\n\n\nTo maintain clarity and organization in the data analysis process, adopt best practices such as:\n\nData documentation: create a README.md file to provide an overview of the project and its structure, and metadata for understanding the context of your analysis.\nAnnotate your pipelines and comment your code (look for tutorials and templates such as this one from freeCodeCamp).\nUse coding style guides (code lay-out, whitespace in expressions, comments, naming conventions, annotations‚Ä¶) to maintain consistency.\nLabel files numerically to organize the entire data analysis process (scripts, notebooks, pipelines, etc.).\n\n00.preprocessing., 01.data_analysis_step1., etc.\n\nProvide environment files for reproducing the computational environment (such as ‚Äòrequirements.txt‚Äô for Python or ‚Äòenvironment.yml‚Äô for Conda). The simplest way is to document the dependencies by reporting the packages and their versions used to run your analysis.\nData versioning: use version control systems (e.g., Git) and upload your code to a code repository Lesson 5.\nIntegrated development environments (e.g., RStudio, PyCharm) offer tools and features for writing, testing, and debugging code\nLeverage curated pipelines such as the ones developed by the nf-core community, further ensuring adherence to community standards and guidelines.\nAdd a LICENSE file and perform regular updates: clarifying usage permissions and facilitating collaboration.\n\n\n\n\n\n\n\nPractical HPC pipes\n\n\n\nWe provide a hand-on workshop on computational environments and pipelines. Keep an eye on the upcoming events on the Sandbox website. If you‚Äôre interested in delving deeper, check out the HPC best practices module we‚Äôve developed here.",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Processing and analyzing biodata"
    ]
  },
  {
    "objectID": "develop/06_pipelines.html#code-and-pipelines-for-data-analysis",
    "href": "develop/06_pipelines.html#code-and-pipelines-for-data-analysis",
    "title": "6. Processing and analyzing biodata",
    "section": "",
    "text": "In this section, we explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community.\n\n\n\n\n\n\nBefore you start‚Ä¶\n\n\n\n\nChoose a folder structure (e.g., using cookiecutter)\nChoose a file naming system\nAdd a README describing the project (and the naming conventions)\nInstall and set up version control (e.g., Git and Github)\nChoose a coding style!\n\n\nPython: Python‚Äôs PEP or Google‚Äôs style guide\nR: Google‚Äôs style guide or Tidyverse‚Äôs style guide\n\n\n\n\n\nThrough techniques such as scripting, containerization (e.g., Docker), and virtual environments, researchers can create reproducible analyses that enable others to validate and build upon their work. Emphasizing the documentation of data processing steps, parameters, and results ensures transparency and accountability in research outputs. To write clear and reproducible code, take the following approach: write functions, code defensively (such as input validation, error handling, etc.), add comments, conduct testing, and maintain proper documentation.\nTools for reproducibility:\n\nCode notebooks: Utilize tools like Jupyter Notebook and R Markdown to combine code with descriptive text and visualizations, enhancing data documentation.\n\nIntegrated development environments: Consider using platforms such as (knitr or MLflow) to streamline code development and documentation processes.\nPipeline frameworks or workflow management systems: Implement systems like Nextflow and Snakemake to automate data analysis steps (including data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages.\n\n\n\nComputational notebooks (e.g., Jupyter, R Markdown) provide researchers with a versatile platform for exploratory and interactive data analysis. These notebooks facilitate sharing insights with collaborators and documentation of analysis procedures.\n\n\n\nTools such as Nextflow and Snakemake streamline and automate various data analysis steps, enabling parallel processing and seamless integration with existing tools. Remember to create portable code and use relative paths to ensure transferability between users.\n\nNextflow: offers scalable and portable NGS data analysis pipelines, facilitating data processing across diverse computing environments.\nSnakemake: Utilizing Python-based scripting, Snakemake allows for flexible and automated NGS data analysis pipelines, supporting parallel processing and integration with other tools.\n\nOnce your scientific computational workflow is ready to be shared, publish your scientific computational workflow on WorkflowHub.\n\n\n\nEach computer or HPC (High-Performance Computing) platform has a unique computational environment that includes its operating system, installed software, versions of software packages, and other features. If a research project is moved to a different computer or platform, the analysis might not run or produce consistent results if it depends on any of these factors.\nFor research to be reproducible, the original computational environment must be recorded so others can replicate it. There are several methods to achieve this:\n\nContainerization platforms (e.g., Docker, Singularity): allow the researcher to package their software and dependencies into a standardized container image.\nVirtual Machines (e.g., VirtualBox): can share an entire virtualized computing environment (OS, software and dependencies)\nEnvironment managers: provide an isolated environment with specific packages and dependencies that can be installed without affecting the system-wide configuration. These environments are particularly useful for managing conflicting dependencies and ensuring reproducibility. Configuration files can automate the setup of the computational environment:\n\nconda: allows users to export environment specifications (software and dependencies) to YAML files enabling easy recreation of the environment on another system\nPython virtualenv: is a tool for creating isolated environments to manage dependencies specific to a project\nrequirements.txt: may contain commands for installing packages (such as pip for Python packages or apt-get for system-level dependencies), configuring system settings, and setting environment variables. Package managers can be used to install, upgrade and manage packages.\nR‚Äôs renv: The ‚Äòrenv‚Äô package creates isolated environments in R.\n\nEnvironment descriptors\n\nsessionInfo() or devtools::session_info(): In R, these functions provide detailed information about the current session\nsessionInfo(), similarly, in Python. Libraries like NumPy and Pandas have show_versions() methods to display package versions.\n\n\nWhile environment managers are very easy to use and share across different systems, and are lightweight and efficient, offering fast start-up times, Docker containers provide a full env isolation (including the operating system) which ensures consistent behavior across different systems.\n\n\n\n\nTo maintain clarity and organization in the data analysis process, adopt best practices such as:\n\nData documentation: create a README.md file to provide an overview of the project and its structure, and metadata for understanding the context of your analysis.\nAnnotate your pipelines and comment your code (look for tutorials and templates such as this one from freeCodeCamp).\nUse coding style guides (code lay-out, whitespace in expressions, comments, naming conventions, annotations‚Ä¶) to maintain consistency.\nLabel files numerically to organize the entire data analysis process (scripts, notebooks, pipelines, etc.).\n\n00.preprocessing., 01.data_analysis_step1., etc.\n\nProvide environment files for reproducing the computational environment (such as ‚Äòrequirements.txt‚Äô for Python or ‚Äòenvironment.yml‚Äô for Conda). The simplest way is to document the dependencies by reporting the packages and their versions used to run your analysis.\nData versioning: use version control systems (e.g., Git) and upload your code to a code repository Lesson 5.\nIntegrated development environments (e.g., RStudio, PyCharm) offer tools and features for writing, testing, and debugging code\nLeverage curated pipelines such as the ones developed by the nf-core community, further ensuring adherence to community standards and guidelines.\nAdd a LICENSE file and perform regular updates: clarifying usage permissions and facilitating collaboration.\n\n\n\n\n\n\n\nPractical HPC pipes\n\n\n\nWe provide a hand-on workshop on computational environments and pipelines. Keep an eye on the upcoming events on the Sandbox website. If you‚Äôre interested in delving deeper, check out the HPC best practices module we‚Äôve developed here.",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Processing and analyzing biodata"
    ]
  },
  {
    "objectID": "develop/06_pipelines.html#wrap-up",
    "href": "develop/06_pipelines.html#wrap-up",
    "title": "6. Processing and analyzing biodata",
    "section": "Wrap up",
    "text": "Wrap up\nThis lesson emphasized the importance of reproducibility in computational research and provided practical techniques for achieving it. Using annotated notebooks, pipeline frameworks, and community-curated pipelines, such as those developed by the nf-core community, enhances reproducibility and readability.\n\nSources\n\nThe turing way - reproducible research\nRDMkit, Elixir Data Management - Data Analysis\nCode documentation by Johns Hopkins Sheridan libraries. This link includes best practices for code documentation, style guides, R markdown, Jupyter Notebook, version control, and code repository.\nGuide to reproducible code in ecology and evolution\nBest practices for Scientific computing\nElixir Software Best Practices\nfaircookbook workflows\nAtlassian software development tutorial",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Processing and analyzing biodata"
    ]
  },
  {
    "objectID": "develop/03_DOD.html",
    "href": "develop/03_DOD.html",
    "title": "3. Data organization and storage",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nOrganize your data and external resources efficiently\nApply naming conventions for files and folders\nDefine rules for naming results and figures accurately\nSo far, we have covered how to adhere to FAIR and Open Science standards, which primarily focus on data sharing post-project completion. However, effective data management is essential while actively working on the project. Organizing data folders, raw and processed data, analysis scripts and pipelines, and results ensure long-term project success. Without a clear structure, future access and understanding of data become challenging, even more so for collaborators, leading to potential chaos down the line.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#folder-organization",
    "href": "develop/03_DOD.html#folder-organization",
    "title": "3. Data organization and storage",
    "section": "Folder organization",
    "text": "Folder organization\nHere we suggest the use of three main folders:\n\nShared project data folders:\n\n\nThis shared directory is designated for storing unprocessed sequencing data files, with each subfolder representing a separate project.\nEach project folder contains raw data, corresponding metadata, and optionally pre-processed data like quality control reports and processed data.\n\nInclude the pipeline or workflow used for data processing, along with a metadata file.\n\nData in these folders should be locked and set to read-only to prevent unauthorized (‚Äúunwanted‚Äù) modifications.\n\n\nIndividual project folders:\n\n\nThis directory typically belongs to the researcher conducting bioinformatics analyses and encompasses all essential files for a specific research project (data, scripts, software, workflows, results, etc.).\nA project may utilize data from various assays or results obtained from other projects. It‚Äôs important to avoid duplicating datasets; instead, link them from the original source to maintain data integrity and avoid redundancy.\n\n\nResources and databases folders:\n\n\nThis (commonly) shared directory contains common repositories or curated databases that facilitate research (genomics, clinical data, imaging data, and more!). For instance, in genomics, it includes genome references (fasta files), annotations (gtf files) for different species, and indexes for various alignment algorithms.\nEach folder corresponds to a unique reference or database version, allowing for multiple references from the same organism or different species.\n\nEnsure each contains the version of the reference and a metadata file.\nMore subfolders can be created for different data formats.\n\n\n\n\n\n\n\n\nVerify the integrity of downloaded files!\n\n\n\nEnsure that the person downloading the files employs checksums or cryptographic hash functions to verify the integrity and ascertain that files are neither corrupted nor tampered with.\n\nMD5 Checksum: Files with names ending in ‚Äú.md5‚Äù contain MD5 checksums. For instance, ‚Äúfilename.txt.md5‚Äù holds the MD5 checksum of ‚Äúfilename.txt‚Äù.‚Äù\n\n\n\n\n\n\n\n\n\nDatabase\n\n\n\n\n\n\n\nA database is a structured repository for storing, managing, and retrieving information, forming the cornerstone of efficient data organization.\n\n\n\n\n\n\n\n\n\n\n\nCreate shortcuts to public datasets and assays!\n\n\n\nThe use of symbolic links, also referred to as softlinks, is a key practice in large labs where data might used for different purposes and by multiple people.\n\nThey act as pointers, containing the path to the location of the target files/directories.\nThey avoid duplication and they are flexible and lightweight (do not occupy much disk space).\nThey simplify directory structures.\n\nExtra use case: create symbolic links to executable files and libraries!\n\n\n\n\n\n\n\n\n\nExercise: create a softlink link\n\n\n\n\n\n\n\nOpen your terminal and create a softlink using the following command. The first path is the target (directory or file) and the second one is where the symbolic link will be created.\nln -s path/to/dataset/&lt;ASSAY_ID&gt; /path/to/user/&lt;PROJECT_ID&gt;/data/\nNow, access the target file/directory through the symbolic link:\nls /path/to/user/&lt;PROJECT_ID&gt;/data/\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nFollow this example if need extra guidance (change paths!):\n\nCreate the target/original file\n\necho \"This is the content of the original file.\" &gt; /home/users/Documents/original_file.txt\n\nCreate the symbolic link\n\nln -s /home/users/Documents/original_file.txt /home/users/Desktop/original_file.txt\n\nVerify the symbolic link\n\nls -s /home/users/Desktop/original_file.txt\n\nAccess the file through the symbolic link:\n\ncat /home/users/Desktop/original_file.txt\nThe last command will display the contents of the original file.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#navigating-shared-project-data",
    "href": "develop/03_DOD.html#navigating-shared-project-data",
    "title": "3. Data organization and storage",
    "section": "1. Navigating Shared Project Data",
    "text": "1. Navigating Shared Project Data\nLet‚Äôs focus on the shared folders containing experimental datasets generated in-house.\n\nNaming Shared Folders Effectively\nCreate a folder for all your NGS experiments, for instance, named Assay. Each subfolder, denoted by a unique Assay-ID, should be named clearly and comprehensibly. Assay-ID comprises raw files, processed files, and the pipeline used to generate them. Raw files should remain unchanged, while modifications to processed files should be restricted post-preprocessing (e.g., after quality control) to prevent unintended alterations. Check the exercise for efficient naming of Assay-ID:\n\n\n\n\n\n\nExercise: name your Assay-ID\n\n\n\n\n\n\n\n\nHow would you ensure its name is unique and understood at a glance?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nUse an acronym (1) that describes the type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3).\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Assay ID code names\n\n\n\n\n\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\n\nKeep in mind that these folders might be (re)used in different individual projects over many years.\n\n\nOptimizing Folder Structures\nThe provided folder structure is designed to be intuitive for NGS data. The description and metadata files aid in understanding the project‚Äôs origin and structure, crucial for archiving and manuscript preparation. There is a section dedicated to databases in lesson 4. Let‚Äôs explore the example and its folder contents:\n&lt;data_type&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ README.md \n‚îú‚îÄ‚îÄ CHECKSUMS\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n    ‚îú‚îÄ‚îÄ fastqc/\n    ‚îú‚îÄ‚îÄ multiqc/\n    ‚îú‚îÄ‚îÄ final_fastq/\n‚îî‚îÄ‚îÄ raw\n    ‚îú‚îÄ‚îÄ .fastq.gz \n    ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: This file contains general information about the project or experiment, usually in markdown or plain text format. It includes details such as such as the origin of the raw NGS data (including sample information, laboratory protocols used, and the assay‚Äôs objectives). Sometimes, it also outlines the basic directory structure and file naming conventions.\nmetadata.yml: This serves as the metadata file for the project (see this lesson).\npipeline.md: This document describes the pipeline employed to process the raw data, along with the specific commands used to execute the pipeline. The specific format can vary depending on the workflow system employed (e.g., bash, Snakemake, Nextflow, Jupyter Notebooks, etc.) (see this lesson). Employing a standardized pipeline ensures a consistent file organization system (and the corresponding documentation)\nprocessed_data: folder with results of the preprocessing pipeline. The contents may vary depending on the pipeline utilized. For example,\n\nfastqc: quality Control results of the raw fastq files.\nmultiqc: aggregated quality control results across all samples\nfinal_fastq: cleaned and processed files\n\nraw_data: folder with the raw data.\n\n.fastq.gz or other file formats (depending on the field or the experiment)\nsamplesheet.csv: metadata information for the samples. It may contain additional columns that will facilitate downstream analysis. This file is key if are planning to use nf-core pipelines.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#navigating-project-folder",
    "href": "develop/03_DOD.html#navigating-project-folder",
    "title": "3. Data organization and storage",
    "section": "2. Navigating Project folder",
    "text": "2. Navigating Project folder\nIn the Projects folder, usually private to the individual performing the data analysis, each project has its own subfolder containing project information, data analysis scripts and pipelines, and results. It‚Äôs advisable to maintain folders for individual projects, separate from shared data folders, as project-specific files typically aren‚Äôt reused across multiple projects, and more than one dataset might be needed to answer a specific scientific question.\n\nNaming Project Folders Effectively\nThe Project folder should have a unique, easily readable, distinguishable, and instantly understandable name. For instance, consider naming it using the main author‚Äôs initials, a descriptive keyword, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\nOptimizing Folder Structures\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data (symbolic link)\n‚îÇ  ‚îî‚îÄ‚îÄ raw\n‚îÇ  ‚îî‚îÄ‚îÄ processed\n‚îÇ  ‚îî‚îÄ‚îÄ external (third party resources)\n‚îú‚îÄ‚îÄ docs\n‚îÇ  ‚îî‚îÄ‚îÄ project_template.docx\n‚îú‚îÄ‚îÄ notebooks or pipelines/\n‚îÇ  ‚îî‚îÄ‚îÄ data_analysis1.ipynb or data_analysis1.smk\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ logs\n‚îú‚îÄ‚îÄ tmp/scratch\n‚îú‚îÄ‚îÄ environment\n‚îÇ  ‚îî‚îÄ‚îÄ requirements.txt or environment.yml\n‚îú‚îÄ‚îÄ scripts/src\n‚îÇ  ‚îî‚îÄ‚îÄ step1.py \n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;file_name&gt;.html\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ tables/\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: contains symlinks or shortcuts to where the data is (raw, processed, external, etc.), avoiding duplication and modification of original files.\ndocs: a folder containing Word documents, slides, or PDFs related to the project. It also contains your Data Management Plan.\nnotebooks or pipelines: a folder containing notebooks (Jupyter, R markdown, Quarto notebooks) or workflows (Snakemake or Nextflow) with the actual data analysis. Tip: Label them numerically indicating the sequential order.\nREADME.md: detailed description of the project in markdown format.\nlogs: log files.\ntmp/scratch: store temporary or intermediate files (eg. testing).\nenvironment: files for reproducing the analysis environment to reproduce the results, such as a Dockerfile, conda yaml file, or a text file (See 6th lesson for more tips on making your pipelines reproducible). It includes software, libraries/packages, and dependencies (and their versions!).\nscripts: a folder containing helper scripts to run data analysis or source code\nreports: Generated analysis as HTML, PDF, LaTeX, etc. Great for sharing with colleagues and creating formal reports of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. Tip: save the figures under a subfolder named after the notebook/pipeline that created them (you will appreciate this organization when you need to rerun analysis and know which script created each figure!).\n\nresults: results from the data analysis, such as tables and figures, etc. Tip: Create a subfolder named after the notebook or pipeline for storing the results generated by that specific notebook or pipeline.\nmetadata.yml: metadata file describing the dataset, samples, etc. (see this lesson).\n\n\n\n\n\n\n\nExercise: Write your personal data structure\n\n\n\n\n\n\n\n\nCreate your own data structure for one of the projects you are currently working on. Consider how it is similar to the example provided and how it differs. Make sure the data structure is easily understandable and navigable.\nWhat improvements or modifications could be made to enhance clarity and efficiency?",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#template-engine",
    "href": "develop/03_DOD.html#template-engine",
    "title": "3. Data organization and storage",
    "section": "Template engine",
    "text": "Template engine\nSetting up folder structures manually for each new project can be time-consuming. Thankfully, tools like Cookiecutter offer a solution by allowing users to create project templates easily. These templates can ensure consistency across projects and save time. Additionally, using cruft alongside Cookiecutter can assist in maintaining older templates when updates are made (by synchronizing them with the latest version).\n\n\n\n\n\n\nCookiecutter templates\n\n\n\n\nSandbox Project/Data analysis template\nSandbox Data/Assay template\nCookiecutter template for Data science projects\nBrickmanlab template for NGS data: similar to the folder structures in the examples above. You can download and modify it to suit your needs.\n\n\n\n\nQuick tutorial on cookiecutter\n\n\n\n\n\n\nSandbox Tutorial\n\n\n\nLearn how to create your own template here.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#resources-and-databases-folder",
    "href": "develop/03_DOD.html#resources-and-databases-folder",
    "title": "3. Data organization and storage",
    "section": "3. Resources and databases folder",
    "text": "3. Resources and databases folder\nHealth databases are utilized for storing, organizing, and providing access to diverse health-related data, including genomic data, clinical records, imaging data, and more. These resources are regularly updated and released under different versions from various sources. To ensure data reproducibility, it‚Äôs crucial to manage and specify the versions and sources of data within these databases.\n\n\n\n\n\n\nExample NGS: genomic resources\n\n\n\n\n\nFor example, preprocessing NGS data involves utilizing various genomic resources for tasks like aligning and annotating fastq files. Essential resources include reference genomes in FASTA format (e.g., human and mouse), indexed fasta files for alignment tools like STAR and Bowtie, and GTF or GFF files for quantifying reads into genomic regions. One of the latest human reference genome is GRCh38, however many past studies are based on GRCh37.\nHow can you keep track of your resources? Name the folder using the version, or use a reference genome manager such as refgenie.\n\nRefgenie\nIt manages the storage, access, and transfer of reference genome resources. It provides command-line and Python interfaces to download pre-built reference genome ‚Äúassets‚Äù, like indexes used by bioinformatics tools. It can also build assets for custom genome assemblies. Refgenie provides programmatic access to a standard genome folder structure, so software can swap from one genome to another. Check this tutorial to get started.\n\n\n\n\n\nManual Download\nBest practices for downloading data from the source while ensuring the preservation of information about the version and other metadata include:\n\nOrganizing data structure: Create a data structure that allows storing all versions in the same parent directory, and ensure that all lab members follow these practices.\nDocumentation and metadata preservation: Before downloading, carefully review the documentation provided by the database. Download files containing the data version and any associated metadata.\nREADME.md: Record the version of the data in the README.md file.\nChecksums: Check for and use checksums provided by the database to verify the integrity of the downloaded data, ensuring that it hasn‚Äôt been corrupted during transfer. Do the exercise below.\nVerify File size: Check the file size provided by the source. It is not as secure as checksum verification but discrepancies could indicate corruption.\nAutomated Processes: whenever possible, automate the download process to reduce the likelihood of errors and ensure consistency (e.g.¬†use bash script or pipeline).\n\n\n\n\n\n\n\nOptional: Exercise on CHECKSUMS\n\n\n\n\n\nWe recommend the use of md5sum to verify data integrity, especially if you are downloading large datasets. In this example, we use data from the HLA FTP Directory.\n\nInstall md5sum (from coreutils package)\n\n#!/bin/bash\n# On Ubuntu/Debian\napt-get install coreutils\n# On macOS\nbrew install coreutils\n\nCreate a bash script to download the target files (named ‚Äúdw_resources.sh‚Äù in the data structure).\n\n#!/bin/bash\n# Important: go through the README before downloading! Check if a checksums file is included. \n\n# 1. Create or change the directory to the resources dir. \n\n# Check for checksums (e.g.: md5checksum.txt), download, and modify it so that it only contains the checksums of the target files. The file will look like this:\n1a3d12e4e6cc089388d88e3509e41cb3  hla_gen.fasta\n# Finally, save it: \nmd5file=\"md5checksum.txt\"\n\n# Define the URL of the files to download\nurl=\"ftp://ftp.ebi.ac.uk/pub/databases/ipd/imgt/hla/hla_gen.fasta\"\n# \nfilename=$(basename \"$url\")\n\n# (Optional) Define a different filename to save the downloaded file (`wget -O $out_filename`)\n# out_filename = \"imgt_hla_gen.fasta\"\n\n# Download the file\nwget $url && \\\nmd5sum --status --check $md5file\n\nFolder structure\n\ngenomic_resources/\n‚îú‚îÄ‚îÄ specie1/\n‚îÇ  ‚îî‚îÄ‚îÄ version/\n‚îÇ     ‚îú‚îÄ‚îÄ files.txt\n‚îÇ     ‚îî‚îÄ‚îÄ indexes/\n‚îî‚îÄ‚îÄ dw_resources.sh\n\nCreate a md5sum file and share it with collaborators before sharing the data. This allows others to check the integrity of the files.\n\nmd5sum &lt;data&gt;\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a file using md5sums. Choose a file from your favorite dataset or select one from the HLA database (for quick testing, consider using a text file such as Nomenclature_2009.txt).",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#naming-conventions",
    "href": "develop/03_DOD.html#naming-conventions",
    "title": "3. Data organization and storage",
    "section": "Naming conventions",
    "text": "Naming conventions\nConsistent naming conventions play a crucial role in scientific research by enhancing organization and data retrieval. By adopting standardized naming conventions, researchers ensure that files, experiments, or datasets are labeled logically, facilitating easy location and comparison of similar data. The importance of uniform naming conventions extends to various fields, in fields like genomics or health data science, uniform naming conventions for files associated with particular experiments or samples allow for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. Overall, promotes efficiency, collaboration, and the integrity of scientific work.\n\n\n\n\n\n\nGeneral tips for file and folder naming\n\n\n\nRemember to keep the folder structure simple.\n\nKeep it short and meaningful (use understandable abbreviation only, e.g., Cor for correlations or LFC for Log Fold Change)\nConsider including one of these elements: project name, category, descriptor, content, author‚Ä¶\n\nAuthor-based: use initials\n\nUse alphanumeric characters: letters (A-Z) and numbers (0-9)\nAvoid special characters: ~!@#$%^&*()`‚Äú|\nDate-based format: use YYYYMMDD format (year/month/day format helps with sorting and listing files in chronological order)\nUse underscores and hyphens as delimiters and avoid spaces.\n\nNot all search tools may work well with spaces (messy to indicate paths)\nIf the length is a concern, use capital letters to delimit words camelCase.\n\nSequential numbering: Use a two-‚Äëdigit format for single-digit numbers (0‚Äì9) to ensure correct numerical sequence order (for example, 01 and not, 1 if your sequence only goes up to 99)\nVersion control: Indicate the version (‚ÄúV‚Äù) or revision (‚ÄúR‚Äù) as the last element, using the two-digit format (e.g., v01, v02)\nWrite down your naming convention pattern and document it in the README file\n\n\n\n\n\n\n\n\n\nCreate your own naming conventions\n\n\n\n\n\n\n\nConsider the most common types of files and folders you will be working with, such as visualizations, results tables, and processed files. Develop a logical and clear naming system for these files based on the tips provided above. Aim for concise and straightforward names to avoid complexity.\n\n\n\n\n\nTo learn more about naming conventions for NGS analysis and see additional examples, click here.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#wrap-up",
    "href": "develop/03_DOD.html#wrap-up",
    "title": "3. Data organization and storage",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we have learned some practical tips and examples about how to organize your data and bring some order to chaos! Complete the practical tutorial on using cookiecutter as a template engine to be able to create your own templates and reuse them as much as you need.\n\nSources\n\nUK Data Service: https://ukdataservice.ac.uk/learning-hub/research-data-management/format-your-data/organising/\nOakland University: https://library.oakland.edu/services/research-data/file-org.html\nCessda guidelines: https://dmeg.cessda.eu/Data-Management-Expert-Guide/2.-Organise-Document/File-naming-and-folder-structure.\nRDMkit Elixir Europe: https://rdmkit.elixir-europe.org/data_organisation",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Data organization and storage"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html",
    "href": "develop/01_RDM_intro.html",
    "title": "1. Introduction to RDM",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nFundamentals of Research Data Management\nEffective Research Data Management Guidelines\nData Lifecycle Management and phases\nFAIR principles and Open Science",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#guidelines-and-benefits-of-effective-rdm",
    "href": "develop/01_RDM_intro.html#guidelines-and-benefits-of-effective-rdm",
    "title": "1. Introduction to RDM",
    "section": "Guidelines and benefits of effective RDM",
    "text": "Guidelines and benefits of effective RDM\nRDM ensures ethical and legal compliance with research requirements. Effective RDM can significantly benefit research and provide advantages for individual researchers:\n\nDetailed data management planning helps in identifying and addressing potential uses, alining expectations among collaborators, and clarifying data rights and ownership\nTransparent and Structured Data Management enhances the reliability and credibility of research findings\nData documentation and data sharing promotes discoverability and facilitates collaborations. Clear documentation of research also streamlines access to previous work, enhancing efficiency, building upon existing knowledge, maximizing research value, accelerating scientific discoveries, and improving visibility and impact\nRisk assessments and strategies for data storage and security can prevent data loss, breaches, or misuse and safeguard sensitive data\nLong-Term Preservation. Data accessibility well after the project‚Äôs completion contributes to data accessibility and continued research relevance\n\n\n\n\n\n\n\nConsequences of poor RDM\n\n\n\n\n\nSeveral surveys have shown that data scientists spend almost half of their time loading and cleaning data, becoming the most consuming, and what many would call tedious, tasks of their jobs (‚ÄúThe State of Data Science 2020 Moving from Hype Toward Maturity‚Äù 2020; ‚ÄúCleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says‚Äù 2016).\nCan you consider why we dedicate such a significant amount of time? Perhaps these images look familiar to you!\n Caption: Top-left: Photo by Wonderlane on Unsplash; Top-right: From Stanford Center for Reproducible Neuroscience; Bottom: Messy folder structure, by J.A.HR\nIneffective data management practices can have significant consequences that affect your future self, colleagues, or collaborators who may have to deal with your data. The implications of poor data management include:\n\nDifficulty in Data Retrieval: Without proper organization and documentation, finding specific data files or understanding their content becomes challenging and time-consuming, leading to inefficiency.\nLoss of Data: Inadequate backup and storage strategies increase the risk of data loss (hardware failures, accidental deletions‚Ä¶), potentially erasing months or years of work.\nData Incompleteness and Errors: Insufficient documentation leads to ambiguity and errors in data interpretation undermining research credibility.\nDifficulty in Reproducibility: Poor data management hinders scientific progress by impeding the reproduction of research results.\nDelayed or Compromised Collaboration: disorganized data slows down collaborative research projects, hindering communication.\nData Security and Privacy Risks: Inadequate security measures measures expose sensitive information to breaches, risking privacy.\nWasted Time and Resources: Poor management diverts resources from research tasks, increasing labor costs (additional time on data management).\nFinancial Implications: Time-consuming data management tasks lead to increased labor costs and potential project delays. Data loss can also have negative implications.\nReputational Damage: Inaccurate or irreproducible research outcomes harm a researcher‚Äôs credibility in the scientific community.\n\nTo address these challenges, prioritizing and investing in effective RDM practices like organization, documentation, backup strategies, and data security and preservation protocols, can prevent setbacks, ensure data integrity, and enhance scientific research reliability.\n\n\n\n\n\n\n\n\n\nExercise 1: RDM practices\n\n\n\n\n\n\n\nThink about situations that you believe are consequences of poor data management that may have occurred in your research environment, or discuss if you have encountered any of the following.\n\nA researcher struggles over time with disorganized data, hindering efficient locating of files and causing delays in analysis.\nInadequate documentation of data collection leads to misinterpretations and errors in analysis by other researchers or colleagues.\nPoorly organized data requires extensive cleaning, wasting valuable research time.\nLack of proper documentation and data availability in a groundbreaking study raises doubts about the validity of its findings (from the lack of reproducibility).\n\nHow would you approach these issues differently or what steps would you take to address them?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nImplementation of a clear and consistent folder structure with descriptive file names. Additionally, using version control systems, such as Git, for code and analysis files can help track changes and facilitate easy retrieval of previous versions of analyses and results.\nProper data documentation, including detailed metadata, could have been maintained throughout the data collection process, providing necessary context and reducing the risk of incomplete or ambiguous data.\nFollowing FAIR principles (Findable, Accessible, Interoperable, Reusable) by making their data, along with detailed methods and documentation, openly accessible in a reputable data repository.\nImplementation of management strategies from the outset of the research project saves time and resources later on, ensuring that data is well-organized and properly documented.",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#research-data-cycle",
    "href": "develop/01_RDM_intro.html#research-data-cycle",
    "title": "1. Introduction to RDM",
    "section": "Research Data Cycle",
    "text": "Research Data Cycle\nThe Research Data Life Cycle is a structured framework depicting the stages of data from its creation or collection to its eventual archiving or disposal. Comprising inception, collection, processing, analysis, sharing, and preservation stages, the cycle parallels a living organism‚Äôs growth, demanding tailored management at each phase. Mastering this cycle is vital for researchers to maintain data integrity, accessibility, and long-term (re)usability, as it fosters transparency, reproducibility, and collaboration in scientific research.\nThe data life cycle is described in 6 phases:\n\nPlan: definition of the objectives, and data requirements, and develop a data management plan outlining data collection, storage, sharing, and ethical/legal considerations.\nCollect and Document: data is gathered according to the plan, and important details such as source, collection methods, and modifications are documented to ensure quality and facilitate future use.\nProcess and Analyse: data is processed and analyzed using various methods and tools to extract meaningful insights. This involves transforming, cleaning, and formatting data for analysis.\nStore and Secure: data is stored securely to prevent loss, unauthorized access, or corruption. Researchers select appropriate storage solutions and implement security measures to protect sensitive information.\nPublish and Share: sharing data openly, following Open Science and FAIR principles, to foster collaboration, increase research visibility, and enable data reuse by others.\nPreserve: Valuable data is preserved in trusted repositories or archives to ensure long-term accessibility and usability for the scientific community.\n\n\n\n\nResearch Data Life Cycle, University of Copenhagen RDM guidelines\n\n\n\n\n\n\n\n\nRead more on your institution‚Äôs website\n\n\n\n\n\n\n\n\nUniversity of Copenhagen and pdf link\nAarhus University\nUniversity of Southern Denmark, SDU\nAalborg University and FAIR link\nUniversity of Southern Denmark, SDU\n\n\n\n\n\n\nTo delve deeper into this topic, click below and explore each phase of the data life cycle. You will find tips and links to future material.\n\n\n Phases of the data life cycle in detail\n\n\n\n1. Plan\nThe management of research data must be thoroughly considered before physical materials and digital data are collected, observed, generated, created, or reused. This includes developing and documenting data management plans (DMP) in electronic format.DMPs should be updated when significant changes occur and stored alongside the corresponding research data. It‚Äôs essential to discuss DMPs with project collaborators, research managers, and supervisors to establish responsibilities for data management activities during and after research projects.\n\n\n\n\n\n\nTip\n\n\n\nCheck out next lesson to learn more about creating effective DMPs.\n\n\n\n\n2. Collect and Document\nResearch data collection and processing should be in line with the best practices within the respective research discipline. Research projects should be documented in a way that enables reproducibility by others. This entails providing clear and accurate descriptions of project methodology, software, and code utilized. Additionally, workflows for data preprocessing and file structuring should be outlined.\nResearch data should be described in metadata to enable effective searching, identification, and interpretation of the data, with metadata linked to the research data for as long as they exist.\n\n\n\n\n\n\nTip\n\n\n\n\nWe will cover strategies for organizing your files and folder in lesson 3.\nWe will discuss different types of metadata in lesson 4\n\n\n\n\n\n3. Process and analyze\nDuring this phase, researchers employ computational methods and bioinformatics tools to extract meaningful information from the data. Good coding practices ensure well-documented and reproducible analyses. For example, code notebooks and version control tools, such as Git, are essential for transparency and sharing results with the scientific community.\nTo streamline and standardize the data analysis process, researchers often implement workflows and pipelines, automating multiple analysis steps to enhance efficiency and consistency while promoting reproducibility.\n\n\n\n\n\n\nTip\n\n\n\n\nCollaborative efforts by the nf-core community provide curated pipelines across different areas of bioinformatics and genomics.\nLearn more about version control in lesson 5\nIf you want to implement your own pipelines, we have the course for you [IN DEVELOPMENT].\n\n\n\n\n\n4. Store and Secure\nResearch data must be classified based on sensitivity and the potential impact to the research institution from unauthorized disclosure, alteration, or destruction. Risks to data security and data loss should be assessed accordingly. This includes evaluating:\n\nPhysical and digital access to research data\nRisks associated with data management procedures\nBackup requirements and backup procedures\nExternal and internal threats to data confidentiality, integrity and accessibility\nFinancial, regulatory, and technical consequences of working with data, data storage, and data preservation\n\n\n\n\n\n\n\nWarning\n\n\n\nThis step is very specific to the setup used in your environment so we cannot include it in a comprehensive guideline on this matter.\n\nEnroll in the next GDPR course offered by the Center for Health Data Science to learn more about data protection and GDPR compliance.\n\n\n\n\n\n5. Share and publish\nLegislation or agreements may restrict research data sharing and require obtaining relevant approvals and establishing agreements allowing sharing. By default, research data should be made openly available post-project, especially for data underlying research publications. This approach balances openness with considerations like intellectual property, personal data protection, and national interests in accordance with the principle of ‚Äòas open as possible, as closed as necessary‚Äô. When data cannot be openly shared, sharing associated metadata is encouraged.\nAdherence to FAIR principles (findable, accessible, interoperable, and reusable) is crucial, which includes:\n\nProviding open access to data (Open Data) by depositing data in a data repository, or by providing access to information on whether, when, how, and to what extent data can be accessed if data sets cannot be made openly available.\nUsing persistent identifiers (PID) and metadata (such as descriptive keywords) that help locate the data set.\nCommunicating terms for data reuse, for example by attaching a data license.\nOffering the necessary information to understand the process of data creation, purpose, and structure.\n\n\n\n\n\n\n\nTip\n\n\n\n\nMore on FAIR and OS principles in the next section\n\n\n\n\n\n6. Preserve\nArrangements for long-term preservation (data and metadata) must adhere to legislation and agreements. This should include:\n\nInformation on research data: At least the data sets supporting published research must be preserved to address objections or criticisms.\nPreservation duration: Retain data supporting research publications for at least five years post-project or publication.\nChoose preservation format and location: Determine format, location, and associated metadata.\nDelete/destroy data if excluded by legislation or agreements, or if preservation isn‚Äôt necessary or possible (for example, when research data can easily be reproduced or is too costly to store or when material quality will deteriorate over time).\nAssign responsibility: Appoint individuals or roles to safeguard data integrity post-project.\nDetermine access rights: Establish rights for accessing and using preserved data sets.\n\nCheck with your institution their requirements for data preservation, such as keeping copies accessible to research managers and peers at the institution‚Äôs premises.\n\n\n\n\n\n\nExample - University of Copenhagen\n\n\n\n\n\n\n\nFor example, the UCPH mandates that a copy of data sets and associated metadata must remain at UCPH after the project ends and/or when employment with the University ceases, in a way in which they are accessible to research managers and understandable for research managers and peers, unless legislation or agreements determine otherwise.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe will check about which repositories you can use to preserve your NGS data in the lesson 7\n\n\n\n\nTo guarantee effective RDM, researchers should follow the FAIR principles.",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#fair-and-open-science",
    "href": "develop/01_RDM_intro.html#fair-and-open-science",
    "title": "1. Introduction to RDM",
    "section": "FAIR and Open Science",
    "text": "FAIR and Open Science\nOpen Science and FAIR principles have become essential frameworks for promoting transparency, accessibility, and reusability in scientific research. While Open Science advocates for unrestricted access to research outputs, data, and methodologies, FAIR principles emphasize making data Findable, Accessible, Interoperable, and Reusable. Together, they foster collaboration, transcend disciplinary boundaries, and support long-term data preservation. However, they were not directly relevant to software until recently. Governments and funding agencies worldwide increasingly recognize their value and are actively promoting their adoption in academia. In this section, you will learn how to apply these principles to your research.\n\nOpen Science\nOpen Science facilitates wider accessibility of research outputs, fosters collaboration, and promotes transparency and reproducibility, thus enhancing responsible research conduct. Economically, promoting Open Access and data sharing maximizes the return on research investments and fuels innovation and entrepreneurship. This approach enables businesses and startups to leverage research findings, while researchers can integrate large datasets for new discoveries.\n\n\n\nPillars of the Open Science according to UNESCO‚Äôs 2021 Open Science recommendation.\n\n\n\n\n\n\n\n\nExamples of Open Science Initiatives\n\n\n\n\n\n\n\n\nNational Institutes of Health (NIH): the USA encourages Open Science practices, including data sharing, through policies like the NIH Data Sharing Policy.\nWellcome Trust: mandates open access globally to research outputs funded by the foundation.\nEuropean Molecular Biology Organization (EMBO): supports Open Access and provides guidelines for data sharing.\nBill & Melinda Gates Foundation: advocates for Open Access and data sharing to maximize the impact of its research.\nEuropean Research Council (ERC): promotes Open Access to research publications and adheres to the FAIR principles for data management.\n\n\n\n\n\n\n\n\n\n\n\n\nBenefits of Open Science for Researchers\n\n\n\n\nIncreased Visibility and Impact: more people can access and engage with your findings.\nFacilitated Collaboration: leading to the development of innovative ideas and impactful projects.\nEnhanced Credibility: sharing data and methods openly allows for validation of research findings by others.\nAccelerated Research Progress:: by enabling researchers to build upon each other‚Äôs work and leverage shared data.\nStimulation of New Research: shared data can inspire novel research questions and discoveries.\nAttract Funding Opportunities: adhering to Open Science principles may make you eligible for additional funding opportunities.\nTransparency and Accountability: promoting responsible conduct in research.\nPLong-Term Data Preservation: by archiving research data in repositories.\n\n\n\n\n\nFAIR principles\nThe FAIR principles complementing Open Science, aim to improve research data management, sharing, and usability. FAIR stands for Findable, Accessible, Interoperable, and Reusable, enhancing the value, impact, and sustainability of research data. Adhering to FAIR principles benefits individual researchers and fosters collaboration, data-driven discoveries, knowledge advancement, and long-term preservation. However, achieving FAIR compliance is nuanced, with some aspects being more complex, especially concerning metadata standards and controlled vocabularies.\nWe strongly endorse these recommendations for those developing software or performing data analyses: https://fair-software.nl/endorse.\n\n\nBreaking down the FAIR Principles\n\n\n\nFindable\n\nResearch data should be easily identifiable, achieved through the following key components:\n\nAssign Persistent and Unique Identifiers: such as Digital Object Identifiers (DOIs) or Uniform Resource Identifiers (URIs).\nCreate Rich and Standard Metadata: describing the content, context, and characteristics of the data (such as origin, format, version, licensing, and the conditions for reuse).\nUse a Data Repository or Catalog: following FAIR principles to deposit your data, enhancing data discoverability and access from a centralized and reliable source.\n\nClear and comprehensive metadata facilitates data discovery by researchers and machines through various search engines and data repositories.\n\n\nAccessible\n\nResearch data should be accessible with minimal restrictions on access and downloading, to facilitate collaboration, verification of findings, and ensuring transparency. Key elements to follow:\n\nOpen Access: Ensure data is freely accessible without unnecessary barriers. Choose suitable licenses for broad data reuse (such as MIT, and Apache-2.0).\nAuthentication and Authorization: Implement secure mechanisms for access control, especially for sensitive data\nMetadata: Deposit metadata even when data access is restricted, providing valuable information about the dataset (version control systems).\n\n\n\n\n\n\n\nImportant note: As open as possible, as closed as necessary\n\n\n\n\n\nEnsure data accessibility aligns with privacy regulations like GDPR, and when necessary, limit access to sensitive data. When dealing with sensitive data, share query details and data sources to maintain transparency.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nYou are working with sensitive data from a national authority regarding personal information. You won‚Äôt be able to publish or deposit the data but you can describe what was the query you used to obtain the data and its source (e.g., sample population: people between 20-30 years old, smoker).\n\n\n\n\n\n\n\n\n\n\nInteroperable\n\nInteroperability involves structuring and formatting data to seamlessly integrate with other datasets. Utilizing standard data formats, widely accepted vocabularies and ontologies enables integration and comparison across various sources. Key components to follow:\n\nStandard Data Formats: facilitating data exchange and interoperability with various software tools and platforms.\nVocabularies and Ontologies: commonly used by the scientific community ensuring data can be understood and combined with other datasets more effectively.\nLinked Data: to related data and resources, to enrich contextualization of datasets, facilitating integration and discovery of interconnected information.\n\n\n\nReusable\n\nData should be thoroughly documented and prepared, with detailed descriptions of data collection, processing, and methodology provided for replication by other researchers. Clear statements on licensing and ethical considerations are essential for enabling data reuse. Key components to follow:\n\nDocumentation and Provenance: Comprehensive documentation on data collection, processing, and analysis. Provenance information elucidates data origin and processing history.\nEthical and Legal Considerations: related to data collection and use. Additionally, adherence to legal requirements ensures responsible and ethical data reuse.\nData Licensing: Clearly stated licensing terms facilitate data reuse, specifying usage, modification, and redistribution while respecting intellectual property rights and legal constraints.\n\n\n\n\n\n\n\n\n\nTest your knowledge: use cases\n\n\n\nBefore moving on to the next lesson, take a moment to explore this practical example if you‚Äôre working with NGS data. Here, you‚Äôll find exercises and examples to reinforce the knowledge you‚Äôve acquired in this lesson.\n\n\n\nKey online links\n\nHow to FAIR DK\nFAIR principles\nFAIR software\nOpen AIRE\nDeiC - RDMElearn",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#wrap-up",
    "href": "develop/01_RDM_intro.html#wrap-up",
    "title": "1. Introduction to RDM",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we‚Äôve covered the definition of RDM, the advantages of effective RDM practices the phases of the research data life cycle, and the FAIR principles and Open Science. While much of the guidelines are in the context of omics data, it‚Äôs worth noting its applicability to other fields and institutions. Nonetheless, we recommend exploring these guidelines further at your institution (links provided above).\nIn the next lessons, we will explore different resources, tools, and guidelines that can be applied to all kinds of data and how to apply them specifically to biological (with a focus on NGS) data.\n\nSources\n\nRDMkit: ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/02_DMP.html",
    "href": "develop/02_DMP.html",
    "title": "2. Data Management Plan",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nLearn what is a DMP\nLearn about the different DMP templates\nHow to write a DMP focused on NGS data\nThe process of data management involves implementing tailored best practices for your data but how do you ensure comprehensive coverage of the decisions and that data is well-managed throughout its life cycle. To achieve this, a Data Management Plan (DMP) is essential.\nA DMP serves as a comprehensive document detailing strategies for handling project data, code, and documentation across its life cycle. It includes plans for data collection, documentation, organization, and preservation.",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#wrap-up",
    "href": "develop/02_DMP.html#wrap-up",
    "title": "2. Data Management Plan",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we covered Data Management Plans (DMPs) and introduced tools to aid in their creation, along with a template for your projects. DMPs serve as valuable aids in project planning, data preservation, and facilitating future use by yourself, collaborators, or the wider scientific community. Next, we‚Äôll delve into organizing data efficiently and provide helpful tools for the task.\n\nSources\n\nNBISweden workshop on RDM practices\nStanford University Library Data Management Services website\nRDM Guide, Elixir Belgium",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/04_metadata.html",
    "href": "develop/04_metadata.html",
    "title": "4. Documentation for biodata",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nThe role of documentation in effective management\nBest practices to create metadata and README files\nSources for controlled vocabularies\n(Optional) Create a database and a catalog browser\nIn bioinformatics data management, documentation plays a critical role in ensuring clarity and reproducibility. Documentation and metadata are essential components in ensuring your data adheres to FAIR principles.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#data-documentation",
    "href": "develop/04_metadata.html#data-documentation",
    "title": "4. Documentation for biodata",
    "section": "Data documentation",
    "text": "Data documentation\nEssential documentation comes in different forms and flavors, serving various purposes in research. Examples include protocols outlining experimental procedures, detailed lab journals recording experimental conditions and observations, codebooks explaining concepts, variables, and abbreviations used in the analysis, information about the structure and content of a dataset, software installation, and usage manual, code explanation within files or methodological information outlining data processing steps.\n From ontotext.com\nData documentation provides essential context and structure to (primary) data, enabling researchers to understand its significance and facilitate efficient data management. Some common elements found in metadata for bioinformatics data include:\n\nData collection information: source (e.g., organism, tissue or location), date (YYYY-MM-DD format) and time, collection methods employed or experimental conditions.\nData processing information: data content, data format, data cleaning and transformation such as filtering and normalizations techniques, and software and tools used.\nData description: variables and attributes, and data types (e.g., categorical, numerical, or textual).\nBiological context: experimental design, biological purpose and relevance and implications in the broader context.\nData ownership and access: authorship, licensing of the data and details on accessing and sharing.\nProvenance and tracking: version control information over time and citations, such as links to publications or studies that reference the data.\n\nData documentation also serves as a crucial guide in navigating the complex landscape of data, akin to a cheat sheet for piecing together the puzzle of information. Much like identifying puzzle pieces, metadata provides essential details about data origin, structure, and context, such as sample collection details, experimental procedures, and equipment used. Metadata enables data exploration, interpretation, and future accessibility, promoting effective management and facilitating data usability and reuse.\n\n\n\n\n\n\nBenefits of collecting proper documentation\n\n\n\n\nData Context and Interpretation: Aiding in understanding experimental conditions, sample origins, and processing methods, is crucial for accurate results interpretation.\nData Discovery and Access: Documentation enables easy locating and accessing of specific datasets by quickly identifying relevant data through sample identifiers, experimental parameters, and timestamps.\nReproducibility and Collaboration: Documentation facilitates experiment replication and validation by enabling colleagues to reproduce analyses, compare results, and collaborate effectively, enhancing the integrity of scientific findings.\nQuality Control and Validation: Documentation supports data quality assessment by tracking the origin and handling of NGS data, allowing the identification of errors or biases to validate analysis accuracy and reliability.\nLong-Term Data Preservation: Documentation ensures preservation over time, facilitating future understanding and utilization of archived datasets for continued scientific impact as research progresses.\n\n\n\n\nStreamlining Metadata Collection\nData and project directories should both include metadata and a README file. Metadata delivers descriptive information about a dataset or project, offering insights for interpreting, using, and sharing the data effectively. README files offer an overview and purpose of the project or dataset, providing instructions and guidance for setting up, running, and using the data or tools. While metadata concentrates on the data itself, README files provide a broader perspective on the overall project or resource.\n\n\n\n\n\n\nPractical tips\n\n\n\n\nImplement a logical structure with clear and descriptive file names.\nUse of controlled vocabularies and ontologies to ensure consistency and efficient data management and interpretation.\nUse a repository and a versioning system\nMake it Machine-readable, -actionable, and -interpretable.\nDevelop standards further within your research environment FAIRsharing standards.\nInclude all information for others to comprehend and effectively utilize the data.\n\n\n\n\n\nREADME.md\n\n\n\n\n\n\nFile formats\n\n\n\nLink to the file format database\n\nMarkdown (.md): commonly used because is easy to read and write and is compatible across platforms (e.g., GitHub, GitLab). Supports formatting like headings, lists, links, images, and code blocks.\nPlain Text (.txt): Simple and straightforward format without any rich formatting and great for basic instructions. Lack the ability of structure content effectively.\nReStructuredText (.rst): commonly used for python projects. Supports advanced formatting (takes, links, images and code blocks) .\n\nOthers such as HTML, YAML and Notebooks.\n\n\nThe README.md file, written in markdown format, provides a detailed description of the folder‚Äôs content. It includes information such as the purpose of the data, collection methods, and relevant details. The content might differ based on the purpose of the data.\n\n\n\n\n\n\nExercise 1: Identify README.md key components.\n\n\n\n\n\n\n\nSelect one of the examples below and reflect on how effectively the README communicates important information about the project. Please note that some of the links lead to README files describing databases, while others pertain to software and tools.\n\n1000 Genomes Project. You will find several readme files here.\n\nHomo Sapiens, fasta GRCh38\nIPD-IMGT/HLA Database\nDocker\nPython pandas\n\n\n\n\n\n\nStructure for bioinformatics projects.\n\nDescription and relevance the project\nObjectives and aims\nDatasets and software requirements\nInstruction for data interpretation\nSummary of results\nContributions\nAdditional comments or notes\n\n\n\nmetadata.yml\n\n\n\n\n\n\nFile formats\n\n\n\n\nXML (eXtensible Markup Language): uses custom tags to describe data and allows for a hierarchical structure.\nJSON (JavaScript Object Notation): lightweight and human-readable format that is easy to parse and generate.\nCSV (Comma-Separated Values) or TSV (tabulate-separate values): simple and widely supported for representing tabular formats. Easy to manipulate using software or programming languages. It is often use for sample metadata.\nYAML (YAML Ain‚Äôt Markup Language): human-readable data serialization format, commonly used as project configuration files.\n\nOthers such as RDF or HDF5.\n\n\nLink to the file format database.\nMetadata can be written in many file formats (commonly used: YAML, TXT, JSON, and CSV). We recommend YAML format, which is a text document that contains data formatted using a human-readable data format for data serialization. However, choose the format that best suits the project‚Äôs needs. The content will be specific to the type of project.\nmetadata:\n  project: \"Title\"\n  author: \"Name\"\n  date: \"YYYYMMDD\"\n  description: \"Project short description\"\n  version: \"1.0\"\n  analysis:\n    tool: \"software\"\n    version: \"1.1.1\"\nSome general metadata fields used across different disciplines:\n\nProject Title: A concise and informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. Include ORCID for identification.\nDate Created: The date when the dataset was originally generated or compiled, in YYYY-MM-DD format.\nDate Modified: The date when the dataset was last updated or modified (YYYY-MM-DD).\nObject ID: The project or assay ID for tracking and reference purposes.\nDescription: A short narrative explaining the content, purpose, and context of the project.\nKeywords: Descriptive terms or phrases capturing the main topics and attributes.\nEthical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.\nVersion: The version number or identifier, useful for tracking changes.\nRelated Publications: Links or references to scientific publications associated with the folder. Always add the DOI.\nFunding Source: Details about the funding agency or source that supported the research or data generation.\nLicense: The type of license or terms of use associated with the dataset/project.\nContact Information: Contact details for individuals who can provide further information about the dataset/project.\n\n\n\n\n\n\n\nTip\n\n\n\nThere is an exercise in the practical material to streamline the creation of metadata files using Cookiecutter, a template-based scaffolding tool.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nCreate a metadata file with the following description fields: name, date, description, version, authors, keywords, license. Fill it up at the start of the project, when you generate the file structure.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#controlled-vocabularies-and-ontologies",
    "href": "develop/04_metadata.html#controlled-vocabularies-and-ontologies",
    "title": "4. Documentation for biodata",
    "section": "Controlled vocabularies and ontologies",
    "text": "Controlled vocabularies and ontologies\nResearchers encountering inconsistent and non-standardized terms (e.g., gene names, disease names, cell types, protein domains, etc.) across datasets may face challenges in data integration. Thus, requiring additional curation time to enable meaningful comparisons. Standardized vocabularies streamline integration, improving consistency and comparability in analysis. Leveraging widely accepted ontologies in the documentation ensures consistent capture of experiment details in metadata fields, aiding data interpretation.\n\n\n\n\n\n\nExamples of ontology services\n\n\n\n\nBiological ontologies for data scientists - Bionty\nAnatomy - Uberon\nTissue - Uberon\nChemical compoundsChemical Entities of Biological Interest\nExperimentalFactor - Experimental Factor Ontology\nSpecies - NCBI Taxonomy, Ensembl Species\nDisease - Mondo, Human Disease\nGene - Ensembl, NCBI Gene, Gene ontology,Microarray Gene Expression Society Ontology (MGED)\nProtein - Uniprot\nCellLine - Cell Line Ontology\nCellType - Cell Ontology\nCellMarker - CellMarker\nPhenotype - Human Phenotype, Phecodes, PATO, Mammalian Phenotype, Zebrafish Phenotype\nPathway - Gene Ontology, Pathway Ontology\nDevelopmentalStage - Human Developmental Stages, Mouse Developmental Stages\nDrug - Drug Ontology\nEthnicity - Human Ancestry Ontology\nBFXPipeline - largely based on nf-core\nBioSample - NCBI BioSample attributes\nArticles Indexing Medical Subject Headings (MeSH)\n\n\n\n\n\n\n\n\n\nOntology definition\n\n\n\n\n\n\n\nAn ontology is a structured framework representing concepts, attributes, and relationships within a specific domain, aiding knowledge organization and integration. Employing standardized vocabularies, it facilitates effective communication and reasoning between humans and computers. Ontologies are crucial for knowledge representation, data integration, and semantic interoperability, enhancing understanding and collaboration across complex domains.\n\n\n\n\n\nStandardization improves data discoverability and interoperability, enabling robust analysis, accelerating knowledge sharing, and facilitating cross-study comparisons. Ontologies act as universal translators, fostering harmonious data interpretation and collaboration across scientific disciplines.\nYou can find three examples of metadata tailored for different purposes NGS data examples: sample metadata, project metadata, and experimental metadata. We suggest exploring controlled vocabularies and metadata standards within your field and seeking additional specialized sources. You will find a few sources at the end of the page.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#database-and-data-catalogs",
    "href": "develop/04_metadata.html#database-and-data-catalogs",
    "title": "4. Documentation for biodata",
    "section": "Database and data catalogs",
    "text": "Database and data catalogs\nMetadata can be used to create data catalogs, particularly beneficial for the efficient organization of experimental or sequencing data generated by researchers. While databases can range from simple tabular formats like Excel to sophisticated DataBase Management Systems (DBMS) like SQLite, the choice depends on factors such as complexity and volume of data. Leveraging a DBMS offers advantages like efficient data storage, enhanced security, and rapid data querying capabilities.\n\nTables as databases\nA browsable table can be created by recursively navigating through a project‚Äôs folder hierarchy using a script and generating a TSV file (tab-separated values) named, for example, database_YYYYMMDD.tsv. This table acts as a centralized repository for all project data, simplifying access and organization. Consistency in metadata structure across projects is vital for efficient data management and integration, as it aids in tracking all conducted assays. Adhering to a uniform metadata format enables the seamless inclusion of essential information from YAML files into the browsable table.\n\n\n\n\n\n\nExercise 2: Generate database tables from metadata\n\n\n\n\n\n\n\nWrite a script (R or Python) that recursively fetches metadata.yml files in a given path. It is important that each subdirectory contains its corresponding metadata.yml.\nRequirements:\n\nData folder structure: containing all project folders\nYAML metadata files associated with each project\n\nClick on the hint to reveal the solution and a code example for the exercise, which may serve as inspiration.\nYou can find a thorough guided exercise in the practical material - Exercise 4.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# Load required packages\npackages &lt;- c(\"yaml\", \"ggplot2\", \"lubridate\")\n\n# Function to recursively fetch YAML files files, read and convert them to a data frame\n\ndf = lapply(file_list, yaml::yaml.load_file)\n\n# Save the data frame as a TSV file\n\n\n\n\n\n\n\n\n\n\n\n\nSQLite database\nAn alternative to the tabular format is SQLite, a lightweight and self-contained relational database management system known for its simplicity and efficiency. SQLite operates without the need for a separate server, making it ideal for scenarios requiring minimal resource usage. It excels in tasks involving structured data storage and retrieval, making it suitable for managing experiment metadata. Similar to the previous example, you can use a script that records all the information from the YAML file in a SQLite database.\n\n\n\n\n\n\nAdvantages of using SQLite database\n\n\n\n\nEfficient Querying: SQLite databases optimize querying and data retrieval, enabling fast and efficient extraction of specific information.\nStructured Organization: Databases provide structured and organized data storage, ensuring easy access and maintenance.\nData Integrity: SQLite databases enforce data integrity through constraints and validations, minimizing errors and inconsistencies.\nConcurrency and Multi-User Support: SQLite supports concurrent read access from multiple users, ensuring accessibility without compromising data integrity.\nScalability: It can handle growing volumes of data without significant performance degradation.\nModularity and Portability: Databases are self-contained and modular, simplifying data distribution and portability.\nSecurity and Access Control: SQLite offers security features like password protection and encryption, with granular control over user access.\nIndexing: Support for indexing accelerates data retrieval based on specific columns, particularly beneficial for large datasets.\nData Relationships: Databases allow for the establishment of relationships between tables, facilitating storage of interconnected data, such as project, assay, and sample information.\n\n\n\n\n\n\n\n\n\nExercise 3: Generate a SQLite database from metadata\n\n\n\n\n\n\n\nClick on the hint to reveal the necessary libraries and some functions, which may serve as inspiration.\nYou can find a thorough guided exercise, complete with code example, in the practical material - Exercise 4, option B.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# Load required packages\npackages &lt;- c(\"yaml\", \"ggplot2\", \"lubridate\", \"DBI\")\n\n# Function to recursively fetch YAML files files, read and convert them to a data frame\n\ndf = lapply(file_list, yaml::yaml.load_file)\n\n# Create an SQLite database from a dataframe and insert data\ndbConnect(SQLite(), \"filenameXXX.sqlite\")\ndbWriteTable()\n\n\n\n\n\n\n\n\n\n\n\n\nCatalog browser\nTo further optimize the use of your metadata and improve the integration of all your lab metadata, you can design a user-friendly catalog browser for your database using tools like Rshiny or Panel. These frameworks provide interfaces for dynamic search, filtering, and visualization, facilitating efficient exploration of database contents.\nCreating such a tool with RShiny is straightforward and does not require extensive development knowledge, whether using a TSV file or a SQLite database. In the practical materials, we demonstrate both scenarios and showcase various functionalities for inspiration. SQLite files are particularly advantageous for data fetching and other operations due to their efficient querying and indexing capabilities.\nHere‚Äôs an example of an SQLite database catalog created by the Brickman Lab at the Center for Stem Cell Medicine. It‚Äôs simple yet effective! Clicking on a data row opens the metadata.yml file, allowing access to detailed metadata for that assay.\n\n\nVideo\ntype:video\n\n\n\n\n\n\n\n\nExercise 4: Create your first catalog browser using Rshiny\n\n\n\n\n\n\n\nGo to the practical material for complete exercise instructions and solutions. The code provided can serve as inspiration for you to adapt as needed.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nThese are some of the libraries required: install.packages(c(\"shiny\", \"DT\", \"DBI\"))\nYou need to define both a user interface (UI) and a server function. The UI (fluidPage()) outlines the app‚Äôs layout using for example, the sidebarLayout() and mainPanel() functions for input controls and output displays.\nThe server function manages data manipulation and user interactions. Use shinyApp() to launch the app once the UI and server are set up.\nHere is a simple example of a server function settup including the main parts (additional components provide advanced functionalities):\n  server &lt;- function(input, output, session) {\n    # Define a reactive expression for data based on user inputs\n    data &lt;- reactive({\n        req(input$dataInput)  # Ensure data input is available\n        # Load or manipulate data here\n    })\n\n    # Define an output table based on data\n    output$dataTable &lt;- renderTable({\n        data()  # Render the data as a table\n    })\n\n    # Observe a button click event and perform an action\n    observeEvent(input$actionButton, {\n        # Perform an action when the button is clicked\n    })\n\n    # Define cleanup tasks when the app stops\n    onStop(function() {\n        # Close connections or save state if necessary\n    })\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5: Add complex features to your catalog browser\n\n\n\n\n\n\n\nOnce you‚Äôve finished the previous exercise, consider implementing these additional ideas to maximize the utility of your catalog browser.\n\nAdd a functionality to only select certain columns uiOutput(\"column_select\")\nAdd buttons to order numeric columns ascending or descending using radioButtons()\nUse SQL aggregation functions (e.g., SUM, COUNT, AVG) to perform custom data summaries and calculations.\nAdd a tab tabPanel() to create a project directory interactively (and fill up the metadata fields), tips: dir.create(), data.frame(), write.table()\nModify existing entries\nVisualize results using Cirrocumulus, an interactive visualization tool for large-scale single-cell genomics data.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nExplore this example with advanced features such as a two-tab layout, filtering by numeric values and matching strings, and a color-customized dashboard here.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\n\nFor R Enthusiasts Explore demosfrom the R Shiny community to kickstart your projects or for inspiration.\nFor python Enthusiasts If you want to dive deeper into Shiny apps and their various uses (such as dynamic plots or other interactive widgets), Shiny for Python provides live, interactive code throughout its entire tutorial. Additionally, it offers a great tool called Playground, where you can code and test your own app to explore how different features render.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#wrap-up",
    "href": "develop/04_metadata.html#wrap-up",
    "title": "4. Documentation for biodata",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we‚Äôve covered the importance of attaching metadata to your data for future reusability and comprehension. We briefly introduced various controlled vocabularies and provided several sources for inspiration. Implementing ontologies is optional, as their usage complexity varies.\nOptionally, if you‚Äôve gone through the lesson, you‚Äôve learned how to use the metadata YAML files to create a database and a catalog browser using Shiny apps. This makes it easy to manage all assays together.\n\nSources\n\nRMDKit: https://rdmkit.elixir-europe.org/data_brokering#collecting-and-processing-the-metadata-and-data\nFAIRsharing.org: provide a searchable database of metadata standards for a wide variety of disciplines\n\nOther sources:\n\nJohns Hopkins Sheridan libraries, RDM. They provide a list of medical metadata standards resources.\n\nKU Leuven Guidance: https://www.kuleuven.be/rdm/en/guidance/documentation-metadata\nTranscriptomics metadata standards and fields\nNIH standardizing data collection\nObservational Health Data Sciences and Informatics (OHDSI) OMOP Common Data Model\n\n\n\nTools and software\n\nRightfield: open source tool facilitates the integration of ontology terms into Excel spreadsheet.\nOwlready2: Python package, enables the loading of ontologies as Python objects. This versatile tool allows users to manipulate and store ontology classes, instances, and properties as needed.\nShiny Apps: easy interactive web apps for data science",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Documentation for biodata"
    ]
  },
  {
    "objectID": "develop/05_VC.html",
    "href": "develop/05_VC.html",
    "title": "5. Version Control with Git and GitHub",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nVersion control essentials and practices\nGit and Github repositories\nCreate repositories\nGitHub page to showcase your data analysis reports",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Version Control with Git and GitHub"
    ]
  },
  {
    "objectID": "develop/05_VC.html#best-practices-in-data-analysis",
    "href": "develop/05_VC.html#best-practices-in-data-analysis",
    "title": "5. Version Control with Git and GitHub",
    "section": "Best Practices in Data Analysis",
    "text": "Best Practices in Data Analysis\nThis lesson introduces version control with Git and Github and its significance in research. You will gain the ability to create Git repositories, and skills to build GitHub pages for showcasing data analysis.\nVersion control systematically tracks project changes, documenting alterations for understanding project evolution. It holds significant importance in research data management, software development, and data analysis, offering numerous advantages.\n\n\n\n\n\n\nAdvantages of using version control\n\n\n\n\nDocument Progress: Detailed change history aids understanding of project development and modifications.\nEnsure Data Integrity: Prevents accidental data loss or corruption, with each change tracked for easy recovery.\nFacilitate Collaboration: Enables seamless collaboration among team members, allowing multiple individuals to work concurrently without conflicts.\nReproducibility: Preserves project state for accurate validation and analysis.\nBranching and Experimentation: Allows the creation of alternative project versions for experimentation, without altering the main branch.\nGlobal Accessibility: Platforms like GitHub provide visibility for sharing, feedback, and contribution to open science.\n\n\n\n\n\n\n\n\n\nTake our course on Git & Github\n\n\n\nif you‚Äôre interested in delving deeper, explore our course on Git and GitHub.\nAlternatively, here are some examples and online resources to expand your understanding:\n\nGit and GitHub online resources\nGitHub documentation\nGit documentation\n\n\n\n\nVersion control using Git\nGit is a widely adopted version control system that empowers developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, track changes, and ensure data integrity. Git operates on core principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, storing the complete project history for independent work.\nSnapshots, Not Files: Git captures snapshots of the entire project at different points instead of tracking individual file changes, ensuring data consistency.\nCommits: Users create ‚Äòcommits‚Äô as snapshots of the project at specific moments, recording changes made to files along with explanatory commit messages.\nBranching: Git supports branching, enabling users to create separate lines of development for new features or bug fixes without affecting the main branch.\nMerging: Changes from one branch can be merged into another, facilitating the incorporation of new features or bug fixes back into the main project with a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means each user‚Äôs local repository is a complete copy of the project, enabling offline work and ensuring data redundancy.\nRemote Repositories: Users can connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub, facilitating collaboration and project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others and ‚Äòpull‚Äô changes made by others into their local repository to stay updated.\nConflict Resolution: Git provides tools to resolve conflicts manually in cases of conflicting changes, ensuring data integrity during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history such as major releases or significant milestones.\n\n\n\nGitHub Hosting for Git\nIn addition to exploring Git, we will also explore GitHub, a collaborative platform for hosting Git repositories. GitHub enhances Git‚Äôs capabilities by offering features like issue tracking, security measures to protect repositories, and GitHub Pages for creating project websites. Additionally, GitHub provides the option to set repositories as private until you are ready to share your work publicly.\n\n\n\n\n\n\nAlternatives flows for collaborative projects\n\n\n\n\nGitLab\nBitBucket\n\nWe will focus on GitHub for the remainder of this lesson due to its widespread usage and compatibility.\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will discuss repositories for archiving experimental or large datasets in lesson 7.\n\n\n\nFrom Project folders to Git repositories\nMoving from Git to GitHub involves transitioning from a local version control setup to a remote hosting platform. You will need a GitHub account for the exercise in this section.\n\n\n\n\n\n\nCreate a GitHub account\n\n\n\n\nIf you don‚Äôt have a GitHub account yet, click here\nInstall Git from Git webpage\n\n\n\nYou have two options when it comes to creating a repository for your project. First, you can start from scratch by creating a new repository and adding files to it as your project progresses. Alternatively, if you already have an existing folder structure for your project, you can initialize a repository directly from that folder. It is crucial to initiate version control in the early stages of a project to facilitate easy tracking of changes and effective management of the project‚Äôs version history from the beginning.\n\nConverting Folders to Git Repositories\nIf you completed all the exercises in lesson 3, you should have a project data structure prepared. Otherwise, consider using one of your existing projects or creating a small toy example for practice using cookiecutter (see practical_workshop).\n\n\n\n\n\n\nGithub documentation link\n\n\n\n\nAdding locally hosted code to Github\n\n\n\n\n\n\n\n\n\nExercise 1: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nInitialize the repository: Begin by running the command git init in your project directory. This command sets up a new Git repository in the current directory and is executed only once, even for collaborative projects. See (git init) for more details.\nCreate a remote repository: Once the local repository is initialized, create am empty new repository on GitHub.\nConnect the remote repository: Add the GitHub repository URL to your local repository using the command git remote add origin &lt;URL&gt;. This associates the remote repository with the name ‚Äúorigin.‚Äù\nCommit changes: If you have files you want to add to your repository, stage them using git add ., then create a commit to save a snapshot of your changes with git commit -m \"add local folder\".\nPush to GitHub: To synchronize your local repository with the remote repository and establish a tracking relationship, push your commits to the GitHub repository using git push -u origin main.\n\n\n\n\n\n\n\n\nSetting Up a Git Repository and copying an existing folder\nAlternatively to converting folders to repositories, you can create a new repository remotely, and then clone (git clone) it locally. Here, git init is not needed. You can move the files into the repository locally (git add, git commit, and git push). If you are creating a collaborative repository, you can now share it with your colleagues.\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nWrite useful and clear Git commits. Check out this post for tips.\n\n\n\n\n\n\nGithub pages\nAfter setting up your repository on GitHub, take advantage of the opportunity to enhance it by adding your data analysis reports. Whether they are in Jupyter Notebooks, R Markdown files, or HTML reports, you can showcase them on a GitHub Page.\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, R Markdown files, or HTML reports, in a GitHub Page website. Creating a GitHub page is very simple, and we recommend that you follow the nice tutorial that GitHub has put for you.\nFor simplicity, we recommend using Quarto or MkDocs. Visit their websites and follow the instructions to get started.\n\n\n\n\n\n\nTutorial links\n\n\n\n\nGet started in quarto: https://quarto.org/docs/get-started/. We recommend using the VS code tool, if you do, follow this tutorial.\nMkDocs materials to further customize MkDocs websites.\n\n\n\n\n\nStep-by-Step Setup Guide\nWe provide an example of setting up Git, Quarto, and a GitHub account, enabling you to replicate the process independently! (see Exercise 5 in the practical material)",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Version Control with Git and GitHub"
    ]
  },
  {
    "objectID": "develop/05_VC.html#wrap-up",
    "href": "develop/05_VC.html#wrap-up",
    "title": "5. Version Control with Git and GitHub",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we explored version control and utilized Git and GitHub to establish data analysis repositories from our Project folders. Additionally, we delved into creating a GitHub organization and leveraging GitHub Pages to showcase data analysis scripts and notebooks publicly. Remember to complete the corresponding exercise from the practical workshop to reinforce your knowledge.\n\nSources\n\nVersion Control and Code Repository Link\nGit cheat sheet",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Version Control with Git and GitHub"
    ]
  },
  {
    "objectID": "develop/practical_workshop.html",
    "href": "develop/practical_workshop.html",
    "title": "Practical material",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nOrganize and structure your data and data analysis with Cookiecutter templates\nDefine metadata fields and collect metadata when creating a Cookiecutter folder\nEstablish naming conventions for your data\nCreate a catalog of your data\nUse GitHub repositories of your data analysis and display them as GitHub Pages\nArchive GitHub repositories on Zenodo\nThis practical version covers practical aspects of RDM applied to biodata. The exercises provided here aim to help you organize and structure your datasets and data analyses. You‚Äôll learn how to manage your experimental metadata effectively and safely version control and archive your data analyses using GitHub repositories and Zenodo. Through these guided exercises and step-by-step instructions, we hope you will acquire essential skills for managing and sharing your research data efficiently, thereby enhancing the reproducibility and impact of your work."
  },
  {
    "objectID": "develop/practical_workshop.html#organize-and-structure-your-datasets-and-data-analysis",
    "href": "develop/practical_workshop.html#organize-and-structure-your-datasets-and-data-analysis",
    "title": "Practical material",
    "section": "1. Organize and structure your datasets and data analysis",
    "text": "1. Organize and structure your datasets and data analysis\nEstablishing a consistent file structure and naming conventions will help you efficiently manage your data. We will classify your data and data analyses into two distinct types of folders to ensure the data can be used and shared by many lab members while preventing modifications by any individual:\n\nData folders (assay or external databases and resources): They house the raw and processed datasets, alongside the pipeline/workflow used to generate the processed data, the provenance of the raw data, and quality control reports of the data. The data should be locked and set to read-only to prevent unintended modifications. This applies to experimental data generated in your lab as well as external resources. Provide an MD5 checksum file when you download them yourself to verify their integrity.\nProject folders: They contain all the essential files for a specific research project. Projects may use data from various resources or experiments, or build upon previous results from other projects. The data should not be copied or duplicated, instead, it should be linked directly from the source.\n\nData and data analysis are kept separate because a project may utilize one or more datasets to address a scientific question. Data can be reused in multiple projects over time, combined with other datasets for comparison, or used to build larger datasets. Additionally, data may be utilized by different researchers to answer various research questions.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nWhen organizing your data folders, separate assays from external resources and maintain a consistent structure. For example, organize genome references by species and further categorize them by versions. Make sure to include all relevant information, and refer to this lesson for additional tips on data organization.\nThis will help you to keep your data tidied up, especially if you are working in a big lab where assays may be used for different purposes and by different people!\n\n\n\n\n\n\nData folders\nWhether your lab generates its own experimental data, receives it from collaborators, or works with previously published datasets, the data folder should follow a similar structure to the one presented here. Create a separate folder for each dataset, including raw files and processed files alongside the corresponding documentation and pipeline that generated the processed data. Raw files should remain untouched, and you should consider locking modifications to the final results once data preprocessing is complete. This precaution helps prevent unwanted changes to the data. Each subfolder should be named in a way that is distinct, easily readable and clear at a glance. Check this lesson for tips on naming conventions.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nUse an acronym (1) that describes the type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3).\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example CHIP_Oct4_20230101 is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by the eye.\n\n\n\n\n\nLet‚Äôs explore a potential folder structure and the types of files you might encounter within it.\n&lt;data_type&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ README.md \n‚îú‚îÄ‚îÄ CHECKSUMS\n‚îú‚îÄ‚îÄ pipeline\n    ‚îú‚îÄ‚îÄ pipeline.md\n    ‚îú‚îÄ‚îÄ scripts/\n‚îú‚îÄ‚îÄ processed\n    ‚îú‚îÄ‚îÄ fastqc/\n    ‚îú‚îÄ‚îÄ multiqc/\n    ‚îú‚îÄ‚îÄ final_fastq/\n‚îî‚îÄ‚îÄ raw\n    ‚îú‚îÄ‚îÄ .fastq.gz \n    ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: This file contains a detailed description of the dataset commonly in markdown format. It should include the provenance of the raw data (such as samples, laboratory protocols used, the aim of the project, folder structure, naming conventions, etc.).\nmetadata.yml: This metadata file outlines different keys and essential information, usually presented in YAML format. For more details, refer to this lesson.\npipeline.md: This file provides an overview of the pipeline used to process raw data, as well as the commands to run the pipeline. The pipeline itself and all the required scripts should be collected in the same directory.\nprocessed: This folder contains the results from the preprocessing pipeline. The content vary depending on the specific pipeline used (create additional subdirectories as needed).\nraw: This folder holds the raw data.\n\n.fastq.gz: For example, in NGS assays, there should be ‚Äòfastq‚Äô files.\nsamplesheet.csv: This file holds essential metadata for the samples, including sample identification, experimental variables, batch information, and other metrics crucial for downstream analysis. It is important that this file is complete and current, as it is key to interpreting results. If you are considering running nf-core pipelines, this file will be required.\n\n\n\n\nProject folders\nOn the other hand, we have another type of folder called Projects which refers to data analyses that are specific to particular tasks, such as those involved in preparing a potential article. In this folder, you will create a subfolder for each project that you or your lab is working on. Each Project subfolder should include project-specific information, data analysis pipelines, notebooks, and scripts used for that particular project. Additionally, you should include an environment file with all the required software and dependencies needed for the project, including their versions. This helps ensure that the analyses can be easily replicated and shared with others.\nThe Project folder should be named in a way that is unique, easy to read, distinguishable, and clear at a glance. For example, you might name it based on the main author‚Äôs initials, the dataset being analyzed, the project name, a unique descriptive element related to the project, or the part of the project you are responsible for, along with the date:\n&lt;project&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\n\n\n\n\nNaming examples\n\n\n\n\n\n\n\n\nRNASeq_Mouse_Brain_20230512: a project RNA sequencing data from a mouse brain experiment, created on May 12, 2023\nEHR_COVID19_Study_20230115: a project around electronic health records data for a COVID-19 study, created on January 15, 2023.\n\n\n\n\n\n\nNow, let‚Äôs explore an example of a folder structure and the types of files you might encounter within it.\n&lt;project&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;ID&gt;_&lt;keyword&gt;_YYYYMMDD &lt;- symbolic link\n‚îú‚îÄ‚îÄ documents\n‚îÇ  ‚îî‚îÄ‚îÄ research_project_template.docx\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_processing.rmd\n‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis.rmd\n‚îÇ  ‚îî‚îÄ‚îÄ 03_data_visualization.rmd\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_processing.html\n‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis.html\n‚îÇ  ‚îú‚îÄ‚îÄ 03_data_visualization.html\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ tables\n‚îú‚îÄ‚îÄ requirements.txt // env.yaml\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îú‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis/\n‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ heatmap_sampleCor_20230102.png\n‚îÇ  ‚îú‚îÄ‚îÄ tables\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 02_data_analysis/\n‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ DEA_treat-control_LFC1_p01.tsv\n‚îÇ  ‚îÇ    ‚îî‚îÄ‚îÄ SumStats_sampleCor_20230102.tsv\n‚îú‚îÄ‚îÄ pipeline\n‚îÇ  ‚îú‚îÄ‚îÄ rules // processes \n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ step1_data_processing.smk\n‚îÇ  ‚îî‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ scratch\n‚îî‚îÄ‚îÄ scripts\n\ndata: This folder contains symlinks or shortcuts to the actual data files, ensuring that the original files remain unaltered.\ndocuments: This folder houses Word documents, slides, or PDFs associated with the project, including data and project explanations, research papers, and more. It also includes the Data Management Plan.\n\nresearch_project_template.docx. If you download our template you will find a is a pre-filled Data Management Plan based on the Horizon Europe guidelines named ‚ÄòNon-sensitive_NGS_research_project_template.docx‚Äô.\n\nmetadata.yml: metadata file describing various keys of the project or experiment (see this lesson).\nnotebooks: This folder stores Jupyter, R Markdown, or Quarto notebooks containing the data analysis. Figures and tables used for the reports are organized under subfolders named after the notebook that created them for provenance purposes.\nREADME.md: A detailed project description in markdown or plain-text format.\nreports: Notebooks rendered as HTML, docx, or PDF files for sharing with colleagues or as formal data analysis reports.\n\nfigures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.\n\nrequirements.txt: This file lists the necessary software, libraries, and their versions required to reproduce the code. If you‚Äôre using conda environments, you will also find the env.yaml file here, which outlines the specific environment configuration.\nresults: This folder contains analysis results, such as figures and tables. Organizing results by the pipeline, script, or notebook that generated them will make it easier to locate and interpret the data.\npipeline: A folder containing pipeline scripts or workflows for processing and analyzing data.\nscratch: A folder designated for temporary files or workspace for experiments and development.\nscripts: Folder for helper scripts needed to run data analysis or reproduce the work.\n\n\n\nTemplate engine\nCreating a folder template is straightforward with cookiecutter a command-line tool that generates projects from templates (called cookiecutters). For example, it can help you set up a Python package project based on a Python package project template.\n\n\n\n\n\n\nCookiecutter templates\n\n\n\nHere are some template that you can use to get started, adapt and modify them to your own needs:\n\nPython package project\nSandbox test\nData science\nNGS data\n\nCreate your own template from scratch.\n\n\n\nQuick tutorial on cookiecutter\nBuilding a Cookiecutter template from scratch requires defining a folder structure, crafting a cookiecutter.json file, and outlining placeholders (keywords) that will be substituted when generating a new project. Here‚Äôs a step-by-step guide on how to proceed:\n\nStep 1: Create a Folder Template\nFirst, begin by creating a folder structure that aligns with your desired template design. For instance, let‚Äôs set up a simple Python project template:\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\n|__ cookiecutter.json # created in step 2\nIn this example, {cookiecutter.project_name} is a placeholder that will be replaced with the actual project name when the template is used. This directory contains a python script (‚Äòmain.py‚Äô), a subdirectory (‚Äòtests‚Äô) with a second python script named after the project (‚Äòtest_{{cookiecutter.project_name}}.py‚Äô) and a ‚ÄòREADME.md‚Äô file.\n\n\nStep 2: Create cookiecutter.json\nIn the root of your template folder, create a file named cookiecutter.json. This file will define the variables (keywords) and default values that users will be prompted to fill in during the template initialization. For our Python project template, it might look like this:\n{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\nWhen users generate a project based on your template, they will be prompted with these questions. The provided values (‚Äúresponses‚Äù) will be used to substitute the placeholders in your template files.\nBeyond substituting placeholders in file and directory names, Cookiecutter can automatically populate text file contents with information. This feature is useful for offering default configurations or code file templates. Let‚Äôs enhance our earlier example by incorporating a placeholder within a text file:\nFirst, modify the my_template/main.py file to include a placeholder inside its contents:\n\n\nmain.py\n\n# main.py\ndef hello():\n    print(\"Hello, {{cookiecutter.project_name}}!\")\n\nThe ‚Äò{{cookiecutter.project_name}}‚Äô placeholder is now included within the main.py file. When you execute Cookiecutter, it will automatically replace the placeholders in both file and directory names and within text file contents.\nAfter running Cookiecutter, your generated ‚Äòmain.py‚Äô file could appear as follows:\n# main.py, assuming \"MyProject\" was entered as the project_name\ndef hello():\n    print(\"Hello, MyProject!\") \n\n\nStep 3: Use Cookiecutter\nOnce your template is prepared, you can utilize Cookiecutter to create a project from it. Open a terminal and execute:\ncookiecutter path/to/your/template\nCookiecutter will prompt you to provide values for project_name, author_name, and description. Once you input these values, Cookiecutter will replace the placeholders in your template files with the entered values.\n\n\nStep 4: Review the Generated Project\nAfter the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will find a project structure with the placeholders replaced by the values you provided.\n\n\n\n\n\n\nExercise 1: Create your own template\n\n\n\n\n\n\n\nUse Cookiecutter to create custom templates for your folders. You can do it from scratch (see Exercise 1, part B) or opt for one of our pre-made templates available as a Github repository (recommended for this workshop). Feel free to tailor the template to your specific requirements‚Äîyou don‚Äôt have to follow our examples exactly.\nRequirements\nWe assume you have already gone through the requirements at the beginning of the practical lesson. This includes installing the necessary tools and setting up accounts as needed.\nProject\n\nGo to our Cookicutter template and click on the Fork button at the top-right corner of the repository page to create a copy of the repository on your own GitHub account or organization. \nOpen a terminal on your computer, copy the URL of your fork and clone the repository to your local machine (the URL should look something like https://github.com/your_username/cookiecutter-template):\ngit clone &lt;your URL to the template&gt;\nIf you have a GitHub Desktop, click Add and select ‚ÄúClone repository‚Äù from the options\nOpen the repository and navigate through the different directories\nModify the contents of the repository as needed to fit your project‚Äôs requirements. You can change files, add new ones. remove existing one or adjust the folder structure. For inspiration, review the data structure above under ‚ÄòProject folder‚Äô. For instance, this template is missing the ‚Äòreports‚Äô directory and add the ‚Äòrequirements.txt‚Äô file. Consider creating it, along with a subdirectory named ‚Äòreports/figures‚Äô.\n‚îú‚îÄ‚îÄ results/\n‚îÇ   ‚îú‚îÄ‚îÄ figures/\n‚îú‚îÄ‚îÄ requirements.txt\nHere‚Äôs an example of how to do it:\n# Open your terminal and navigate to your template directory. Then: \ncd \\{\\{\\ cookiecutter.project_name\\ \\}\\}/  \nmkdir reports \ntouch requirements.txt\nCommit and push changes when you are done with your modifications\n\n\nStage the changes with git add\nCommit the changes with a meaningful commit message git commit -m \"update cookicutter template\"\nPush the changes to your forked repository on Github git push origin main (or the appropriate branch name)\n\n\nTest your template by using cookiecutter &lt;URL to your GitHub repository \"cookicutter-template\"&gt;\nFill up the variables and verify that the new structure (and folders) looks like you would expect. Have any new folders been added, or have some been removed?\n\n\n\n\n\n\n\n\n\n\n\n\nOptional Exercise 1, part B\n\n\n\n\n\n\n\nCreate a template from scratch using this tutorial scratch, it can be as basic as this one below or ‚ÄòData folder‚Äô:\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\n\nStep 1: Create a directory for the template.\nStep 2: Write a cookiecutter.json file with variables such as project_name and author.\nStep 3: Set up the folder structure by creating subdirectories and files as needed.\nStep 4: Incorporate cookiecutter variables in the names of files.\nStep 5: Use cookiecutter variables within scripts, such as printing a message that includes the project name."
  },
  {
    "objectID": "develop/practical_workshop.html#data-documentation",
    "href": "develop/practical_workshop.html#data-documentation",
    "title": "Practical material",
    "section": "2. Data documentation",
    "text": "2. Data documentation\nData documentation involves organizing, describing, and providing context for datasets and projects. While metadata concentrates on the data itself, README files provide a broader perspective on the overall project or resource.\n\nMetadata\n\n\n\n\n\n\nmetadata.yml\n\n\n\nChoose the format that best suits the project‚Äôs needs. In this workshop, we will focus on YAMl as it is highly used for configuration files (e.g., in conda or pipelines).\n\n\n\n\n\n\nFile formats\n\n\n\n\n\n\n\n\nXML (eXtensible Markup Language): uses custom tags to describe data and allows for a hierarchical structure.\nJSON (JavaScript Object Notation): lightweight and human-readable format that is easy to parse and generate.\nCSV (Comma-Separated Values) or TSV (tabulate-separate values): simple and widely supported for representing tabular formats. Easy to manipulate using software or programming languages. It is often use for sample metadata.\nYAML (YAML Ain‚Äôt Markup Language): human-readable data serialization format, commonly used as project configuration files.\n\nOthers such as RDF or HDF5.\n\n\n\n\n\nLink to the file format database.\n\n\nMetadata in biological datasets refers to the information that describes the data and provides context for how the data was collected, processed, and analyzed. Metadata is crucial for understanding, interpreting, and using biological datasets effectively. It also ensures that datasets are reusable, reproducible and understandable by other researchers. Some of the components may differ depending on the type of project, but there are general concepts that will always be shared across different projects:\n\nSample information and collection details\nBiological context (such experimental conditions if applicable)\nData description\nData processing steps applied to the raw data\nAnnotation and Ontology terms\nFile metadata (file type, file format, etc.)\nEthical and Legal Compliance (ownership, access, provenance)\n\n\n\n\n\n\n\nMetadata and controlled vocabularies\n\n\n\nTo maximize the usefulness of metadata, aim to use controlled vocabularies across all fields. Read more about data documentation and find ontology services examples in lesson 4. We encourage you to begin implementing them systematically on your own (under the ‚Äúsources‚Äù section, you will find some helpful links to guide you putting them in practice).\nIf you work with NGS data, check out this recommendations and examples of metadata for samples, projects and datasets.\n\n\n\n\nREADME file\n\n\n\n\n\n\nREADME.md\n\n\n\nChoose the format that best suits the project‚Äôs needs. In this workshop, we will focused on Markdown as it is the most used format due to its balance of simplicity and expressive formatting options.\n\n\n\n\n\n\nFile formats\n\n\n\n\n\n\n\n\nMarkdown (.md): commonly used because is easy to read and write and is compatible across platforms (e.g., GitHub, GitLab). Supports formatting like headings, lists, links, images, and code blocks.\nPlain Text (.txt): Simple and straightforward format without any rich formatting and great for basic instructions. Lack the ability of structure content effectively.\nReStructuredText (.rst): commonly used for python projects. Supports advanced formatting (takes, links, images and code blocks) .\n\nOthers such as HTML, YAML and Notebooks.\n\n\n\n\n\nLink to the file format database\n\n\nThe README.md file is a markdown file that provides a comprehensive description of the data within a folder. Its rich text format (including bold, italic, links, etc.) allows you to explain the contents of the folder, as well as the reasons and methods behind its creation or collection. The content will vary depending on what it described (data or assays, project, software‚Ä¶).\nHere is an example of a README file for a bioinformatics project:\n\n\n\n\n\n\nREADME\n\n\n\n\n\n# TITLE\nClear and descriptive.\n# OVERVIEW\nIntroduction to the project including its aims, and its significance. Describe the main purpose and the biological questions being addressed.\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n\n\n\n\n# TABLE OF CONTENTS (optional but helpful for others to navigate to different sections)\n# INSTALLATION AND SETUP\nList all prerequisites, software, dependencies, and system requirements needed for others to reproduce the project. If available, you may link to a Docker image, Conda YAML file, or requirements.txt file.\n# USAGE\nInclude command-line examples for various functionalities or steps and path for running a pipeline, if applicable.\n# DATASETS\nDescribe the data,, including its sources, format, and how to access it. If the data has undergone preprocessing, provide a description of the processes applied or the pipeline used.\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n\n\n\n\n# RESULTS\nSummarize the results and key findings or outputs.\n\n\n\n\n\n\nExample text\n\n\n\n\n\n\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n\n\n\n\n# CONTRIBUTIONS AND CONTACT INFO\n# LICENSE\n\n\n\n\n\n\n\n\n\n\nExercise 2: modify the metadata.yml file in your Cookiecutter template\n\n\n\n\n\n\n\nIt is time now to customize your Cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n\nConsider changing variables (add/remove) in the metadata.yml file from the cookicutter template.\nModify the cookiecutter.json file. You could add new variables or change the default key and/or values:\n\n\ncookiecutter.json\n\n{\n\"project_name\": \"myProject\",\n\"project_slug\": \"{{ cookiecutter.project_name.lower().replace(' ', '_').replace('-', '_') }}\",\n\"authors\": \"myName\",\n\"start_date\": \"{% now 'utc', '%Y%m%d' %}\",\n\"short_desc\": \"\",\n\"version\": \"0.1.0\"\n}\n\nThe metadata file will be filled out accordingly.\nOptional: You can customize or remove this prompt message entirely, allowing you to tailor the text to your preferences for a unique experience each time you use the template.\n\n\ncookiecutter.json\n\n\"__prompts__\": {\n    \"project_name\": \"Project directory name [Example: project_short_description_202X]\",\n    \"author\": \"Author of the project\",\n    \"date\": \"Date of project creation, default is today's date\",\n    \"short_description\": \"Provide a detailed description of the project (context/content)\"\n},\n\nModify the metadata.yml file so that it includes the metadata recorded by the cookiecutter.json file. Hint below (short_desc, authors, etc.):\n\n\nmetadata.yml\n\nproject: {{ cookiecutter.project_name }}\nauthors: {{ cookiecutter.authors }}\ndate: {{ cookiecutter.date }}\ndescription: {{ cookiecutter.short_desc }}\n\nModify the README.md file so that it includes the short description recorded by the cookiecutter.json file and the metadata at the top of the markdown file (top between lines of dashed).\n\n\nREADME.md\n\n---\ntitle: {{ cookiecutter.project_name }}\ndate: \"{{ cookiecutter.date }}\"\nauthor: {{ cookiecutter.author }}\nversion: {{ cookiecutter.version }}\n---\n\nProject description\n----\n\n{{ cookiecutter.short_description }}\n\nCommit and push changes when you are done with your modifications\n\n\nStage the changes with git add\nCommit the changes with a meaningful commit message git commit -m \"update cookicutter template\"\nPush the changes to your forked repository on Github git push origin main (or the appropriate branch name)\n\n\nTest your template by using cookiecutter &lt;URL to your GitHub repository \"cookicutter-template\"&gt;\nFill up the variables and verify that the modified information looks like you would expect."
  },
  {
    "objectID": "develop/practical_workshop.html#naming-conventions",
    "href": "develop/practical_workshop.html#naming-conventions",
    "title": "Practical material",
    "section": "3. Naming conventions",
    "text": "3. Naming conventions\nAs discussed in lesson 3, consistent naming conventions are key for interpreting, comparing, and reproducing findings in scientific research. Standardized naming helps organize and retrieve data or results, allowing researchers to locate and compare similar types of data within or across large datasets.\n\n\n\n\n\n\nExercise 3: Define your file name conventions\n\n\n\n\n\n\n\nAvoid long and complicated names and ensure your file names are both informative and easy to manage:\n\nFor saving a new plot, a heatmap representing sample correlations\nWhen naming the file for the document containing the Research Data Management Course Objectives (Version 2, 2nd May 2024) from the University of Copenhagen\nConsider the most common file types you work with, such as visualizations, figures, tables, etc., and create logical and clear file names\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nheatmap_sampleCor_20240101.png\nKU_RDM-objectives_20240502_v02.doc or KU_RDMObj_20240502_v02.doc"
  },
  {
    "objectID": "develop/practical_workshop.html#create-a-catalog-of-your-data-folder",
    "href": "develop/practical_workshop.html#create-a-catalog-of-your-data-folder",
    "title": "Practical material",
    "section": "4. Create a catalog of your data folder",
    "text": "4. Create a catalog of your data folder\nThe next step is to collect all the datasets that you have created in the manner explained above. Since your folders all should contain the metadata.yml file in the same place with the same metadata, it should be very easy to iteratively go through all the folders and merge all the metadata.yml files into a one single table. he table can be easily viewed in your terminal or even with Microsoft Excel.\n\n\n\n\n\n\nExercise 4: create a metadata.tsv catalog\n\n\n\n\n\n\n\nWe will make a small script in R (or you can make one with Python) that recursively goes through all the folders inside an input path (like your Assays folder), fetches all the metadata.yml files, merges them and writes a TSV file as an output.\n\nCreate a folder called dataset and change directory cd dataset\nFork this repository: a Cookiecutter template designed for NGS datasets.While you are welcome to create your own template from scratch, we recommend using this one to save time. Then, git clone &lt;URL to Gihub repo&gt;.\nRun the cookiecutter cc-data-template command at least twice to create multiple datasets or projects. Use different values each time to simulate various scenarios (do this in the dataset directory that you have previously created). Importantly, we recommend assigning values to all entries. If an entry does not apply, use ‚Äúnan.‚Äù This will be better for processing metadata later on. If the path to the cc-data-template is elsewhere than the datasets dir, change the command e.g.:cookiecutter /home/myTemplates/cc-data-template\nExecute the script below using R (or create your own script in Python). Adjust the folder_path variable so that it matches the path to the Assays folder. The resulting table will be saved in the same folder_path.\nOpen your database_YYYYMMDD.tsv table in a text editor from the command-line, or view it in Excel for better visualization.\n\n\nSolution A. From a TSV\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# R version 4.3.2\n# RScript to read all yaml files in directory and save the metadata into a dataframe\nquiet &lt;- function(package_name) {\n  # Suppress warnings and messages while checking and installing the package\n  suppressMessages(suppressWarnings({\n    # Check if the package is available and load it\n    if (!requireNamespace(package_name, quietly = TRUE)) {\n      install.packages(package_name)\n    }\n    # Load the package\n    library(package_name, character.only = TRUE)\n  }))\n}\n\n# Check and install necessary libraries\nquiet(\"yaml\")\nquiet(\"dplyr\")\nquiet(\"lubridate\")\n\n\nread_yaml &lt;- function(file_path) {\n  # Read the YAML file and convert it to a data frame\n  df &lt;- yaml::yaml.load_file(file_path) %&gt;% as.data.frame(stringsAsFactors = FALSE)\n  \n  # Return the data frame\n  return(df)\n}\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n\n  metadata_list &lt;- lapply(file_list, read_yaml)\n  \n  # Combine the list of data frames into a single data frame using dplyr::bind_rows()\n  combined_metadata &lt;- bind_rows(metadata_list)\n\n  return(combined_metadata)\n}\n\n# Specify the folder path\nfolder_path &lt;- \"./\" #/path/to/your/folder\n\n# Fetch metadata from the specified folder\ndf &lt;- get_metadata(folder_path)\n\n# Save the data frame as a TSV file\noutput_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\nwrite.table(df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", output_file, \"\\n\")\n\n\n\n\n\nExercise 4, option B: create a SQLite database \nAlternatively, create a SQLite database from a metadata. If you opt for this option in the exercise, you must still complete the first three steps outlined above. Read more from the RSQLite documentation.\n\nSolution B. SQLite database\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nprint(\"Assuming the libraries from Exercise 4 are already loaded and a dataframe has been generated from the YAML files...\")\n\n# check_and_install() form Exercise 4, and load the other packages. \nquiet(\"DBI\")\nquiet(\"RSQLite\")\n\n# Initialize a temporary in memory database and copy the data.frame into it\n\ndb_file_path &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".sqlite\")\ncon &lt;- dbConnect(RSQLite::SQLite(), db_file_path)\n\ndbWriteTable(con, \"metadata\", df,  overwrite=TRUE) #row.names = FALSE,append =\n\n# Print confirmation message\ncat(\"Database saved as\", db_file_path, \"\\n\")\n\n# Close the database connection\ndbDisconnect(con)\n\n\n\n\n\n\n\n\n\n\n\nShiny apps\nTo get the most out of your metadata file and the ones from other colleagues, you can combine them and explore them by creating an interactive catalog browser. You can create interactive web apps straight from R or Python. Whether you have generated a tabulated-file or a sqlite database, browse through the metadata using Shiny. Shiny apps are perfect for researchers because they enable you to create interactive visualizations and dashboards with dynamic data inputs and outputs without needing extensive web development knowledge. Shiny provides a variety of user interface components such as forms, tables, graphs, and maps to help you organize and present your data effectively. It also allows you to filter, sort, and segment data for deeper insights.\n\n\n\n\n\n\nTip\n\n\n\n\nFor R Enthusiasts\n\nExplore demos from the R Shiny community to kickstart your projects or for inspiration.\n\nFor python Enthusiasts\n\nShiny for Python provides live, interactive code throughout its entire tutorial. Additionally, it offers a great tool called Playground, where you can code and test your own app to explore how different features render.\n\n\n\n\n\n\n\n\nExercise 5: Skill Booster, build an interactive catalog browser\n\n\n\n\n\n\n\nBuild an interactive web app straight from R or Python. Below, you will find an example of an R shiny app. In either case, you will need to define a user interface (UI) and a server function. The UI specifies the layout and appearance of the app, including input controls and output displays. The server function contains the app‚Äôs logic, handling data manipulation, and responding to user interactions. Once you set up the UI and server, you can launch the app!\nHere‚Äôs the UI and server function structure for an R Shiny app:\n# Don't forget to load shiny and DT libraries!\n\n# Specify the layout\nui &lt;- fluidPage(\n    titlePanel(...)\n    # Define the appearance of the app\n    sidebarLayout(\n        sidebarPanel(...)\n        mainPanel(...)\n    )\n)\n\nserver &lt;- function(input, output, session) {\n    # Define a reactive expression for data based on user inputs\n    data &lt;- reactive({\n        req(input$dataInput)  # Ensure data input is available\n        # Load or manipulate data here\n    })\n\n    # Define an output table based on data\n    output$dataTable &lt;- renderTable({\n        data()  # Render the data as a table\n    })\n\n    # Observe a button click event and perform an action\n    observeEvent(input$actionButton, {\n        # Perform an action when the button is clicked\n    })\n\n    # Define cleanup tasks when the app stops\n    onStop(function() {\n        # Close connections or save state if necessary\n    })\n}\n# Run the app\nshinyApp(ui, server)\nIf you need more assistance, take a look at the code below (Hint).\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n# R version 4.3.2\nprint(\"Assuming the libraries from Exercise 4 are already loaded and a dataframe has been generated from the YAML files...\")\n\n# check_and_install() form Exercise 4. \nquiet(\"shiny\")\nquiet(\"DT\")\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"TSV File Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Choose a TSV file\", accept = c(\".tsv\")),\n      selectInput(\"filter_column\", \"Filter by Column:\", choices = c(\"n_samples\", \"technology\"), selected = \"technology\"),\n      textInput(\"filter_value\", \"Filter Value:\", value = \"\"),\n      # if only numbers, numericInput()\n      radioButtons(\"sort_order\", \"Sort Order:\", choices = c(\"Ascending\", \"Descending\"), selected = \"Ascending\")\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  data &lt;- reactive({\n    req(input$file)\n    df &lt;- read.delim(input$file$datapath, sep = \"\\t\")\n    print(str(df))\n\n    # Filter the DataFrame based on user input\n    if (input$filter_column != \"\" && input$filter_value != \"\") {\n      # Check if the column is numeric, and filter for value\n      if (is.numeric(df[[input$filter_column]])) {\n        df &lt;- df[df[[input$filter_column]] &gt;= as.numeric(input$filter_value), ]\n      }\n      # Check if the column is a string\n      else if (is.character(df[[input$filter_column]])) {\n        df &lt;- df[df[[input$filter_column]] == input$filter_value, ]\n      }\n    }\n    \n    # Sort the DataFrame based on user input\n    sort_order &lt;- if (input$sort_order == \"Ascending\") TRUE else FALSE\n    df &lt;- df[order(df[[input$filter_column]], decreasing = !sort_order), ]\n    df\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\n\n\nIn the optional exercise below, you‚Äôll find a code example for using an SQLite database as input instead of a tabulated file.\n\n\n\n\n\n\n\n\n\n\n\nExercise (optional)\n\n\n\n\n\n\n\nOnce you‚Äôve finished the previous exercise, consider implementing these additional ideas to maximize the utility of your catalog browser.\n\nUse SQLite databases as input\nAdd a functionality to only select certain columns uiOutput(\"column_select\")\nFilter columns by value using column_filter_select()\nAdd multiple tabs using tabsetPanel()\nAdd buttons to order numeric columns ascending or descending using radioButtons()\nUse SQL aggregation functions (e.g., SUM, COUNT, AVG) to perform custom data summaries and calculations.\nAdd a tab tabPanel() to create a project directory interactively (and fill up the metadata fields), tips: dir.create(), data.frame(), write.table()\nModify existing entries\nVisualize results using Cirrocumulus, an interactive visualization tool for large-scale single-cell genomics data.\n\nIf you need some assistance, take a look at the code below (Hint).\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nExplore an example with advanced features such as a two-tab layout, filtering by numeric values and matching strings, and a color-customized dashboard here."
  },
  {
    "objectID": "develop/practical_workshop.html#version-control-using-git-and-github",
    "href": "develop/practical_workshop.html#version-control-using-git-and-github",
    "title": "Practical material",
    "section": "5. Version control using Git and GitHub",
    "text": "5. Version control using Git and GitHub\nVersion control involves systematically tracking changes to a project over time, offering a structured way to document revisions and understand the progression of your work. In research data management and data analytics, it plays a critical role and provides numerous benefits.\nGit is a distributed version control system that helps developers and researchers efficiently manage project history, collaborate seamlessly, and maintain data integrity. On the other hand, GitHub is a web-based platform that builds on Git‚Äôs functionality by providing a centralized, collaborative hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allow you to create websites to showcase your projects.\n\n\n\n\n\n\nCreate a GitHub organization for your lab or department\n\n\n\nGitHub users can create organizations, allowing groups to collaborate or create repositories under the same organization umbrella. You can create an educational organization on Github for free, by setting up a Github account for your lab.\nFollow these instructions to create a GitHub organization.\nOnce you‚Äôve established your GitHub organization, be sure to create your repositories within the organization‚Äôs space rather than under your personal user account. This keeps your projects centralized and accessible to the entire group. Best practices for managing an organization on GitHub include setting clear access permissions, regularly reviewing roles and memberships, and organizing repositories effectively to keep your projects structured and easy to navigate.\n\n\n\nSetting up a GitHub repository for your project folder\nVersion controlling your data analysis folders becomes straightforward once you‚Äôve established your Cookiecutter templates. After you‚Äôve created several folder structures and metadata using your Cookiecutter template, you can manage version control by either converting those folders into Git repositories or copying a folder into an existing Git repository. Both approaches are explained in Lesson 5.\n\n\n\n\n\n\nExercise 6: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nInitialize the repository: Begin by running the command git init in your project directory. This command sets up a new Git repository in the current directory and is executed only once, even for collaborative projects. See (git init) for more details.\nCreate a remote repository: Once the local repository is initialized, create an empty new repository on GitHub (website or Github Desktop).\nConnect the remote repository: Add the GitHub repository URL to your local repository using the command git remote add origin &lt;URL&gt;. This associates the remote repository with the name ‚Äúorigin.‚Äù\nCommit changes: If you have files you want to add to your repository, stage them using git add ., then create a commit to save a snapshot of your changes with git commit -m \"add local folder\".\nPush to GitHub: To synchronize your local repository with the remote repository and establish a tracking relationship, push your commits to the GitHub repository using git push -u origin main.\n\n\n\n\n\n\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nIf you would like to know more about Git commits and the best way to make clear Git messages, check out this post!\n\n\n\n\nGitHub Pages\nAfter creating your repository and hosting it on GitHub, you can now add your data analysis reports‚Äîsuch as Jupyter Notebooks, R Markdown files, or HTML reports‚Äîto a GitHub Page website. Setting up a GitHub Page is straightforward, and we recommend following GitHub‚Äôs helpful tutorial. However, we will go through the key steps in the exercise below. There are several ways to create your web pages, but we suggest using Quarto as a framework to build a sleek, professional-looking website with ease. The folder templates from the previous exercise already contain the necessary elements to launch a webpage. Familiarizing yourself with the basics of Quarto will help you design a webpage that suits your preferences. Other common options include MkDocs. If you want to use MkDocs instead, click here and follow the instructions.\n\n\n\n\n\n\nTip\n\n\n\nHere are some useful links to get started with Github Pages:\n\nGithub Pages\nQuarto Github Pages\n\n\n\n\n\n\n\n\n\nExercise 7: Create a Github Page using Quarto\n\n\n\n\n\n\n\n\nHead over to GitHub and create a new public repository named username.github.io, where username is your username (or organization name) on GitHub. If the first part of the repository doesn‚Äôt exactly match your username, it won‚Äôt work, so make sure to get it right.\nGo to the folder where you want to store your project, and clone the new repository: git clone https://github.com/username/username.github.io (or use Github Desktop)\nCreate a new file named _quarto.yml\n\n\n_quarto.yml\n\nproject:\n    type: website\n\nOpen the terminal ```{.bash filename=‚ÄúTerminal‚Äù} # Add a .nojekyll file to the root of the repository not to do additional processing of your published site touch .nojekyll #copy NUL .nojekyll for windows\n# Render and push it to Github quarto render git commit -m ‚ÄúPublish site to docs/‚Äù git push ```\nIf you do not have a gh-pages, you can create one as follows\n\n\nTerminal\n\ngit checkout --orphan gh-pages\ngit reset --hard # make sure all changes are committed before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\n\nBefore attempting to publish you should ensure that the Source branch for your repository is gh-pages and that the site directory is set to the repository root (/)\n\nIt is important to not check your _site directory into version control, add the output directory of your project to .gitignore\n\n\n.gitignore\n\n/.quarto/\n/_site/\n\nNow is time to publish your website\n\n\n.Terminal\n\nquarto publish gh-pages\n\nOnce you‚Äôve completed a local publish, add a publish.yml GitHub Action to your project by creating this YAML file and saving it to .github/workflows/publish.yml. Read how to do it here"
  },
  {
    "objectID": "develop/practical_workshop.html#archive-github-repositories-on-zenodo",
    "href": "develop/practical_workshop.html#archive-github-repositories-on-zenodo",
    "title": "Practical material",
    "section": "6. Archive GitHub repositories on Zenodo",
    "text": "6. Archive GitHub repositories on Zenodo\nArchives are specialized digital platforms that provide secure storage, curation, and dissemination of scientific data. They play a crucial role in the research community by acting as trusted repositories for preserving valuable datasets. With standardized formats and thorough curation processes, they ensure the long-term accessibility and citability of research findings. Researchers globally rely on these repositories to share, discover, and validate scientific information, promoting transparency, collaboration, and knowledge growth across various fields.\nIn the next practical exercise, you will archive your Project folder, which contains data analyses (software, code and pipelines), in a repository such as Zenodo. This can be done by linking your Zenodo account to your GitHub account.\n\n\n\n\n\n\nArchiving data‚Ä¶\n\n\n\nData should be deposited in a domain-specific archive. If you want to know more about these archives, check out this lesson.\n\n\n\nZenodo\nZenodo is an open-access digital repository that supports the archiving of scientific research outputs, including datasets, papers, software, and multimedia files. Affiliated with CERN and backed by the European Commission, Zenodo promotes transparency, collaboration, and the advancement of knowledge globally. Researchers can easily upload, share, and preserve their data on its user-friendly platform. Each deposit receives a unique DOI for citability and long-term accessibility. Zenodo also offers robust metadata options and allows linking your GitHub account to archive a specific release of your GitHub repository directly to Zenodo. This integration streamlines the process of preserving a snapshot of your project‚Äôs progress.\n\n\n\n\n\n\nExercise 6: Archive a Project GitHub repo in Zenodo\n\n\n\n\n\n\n\n\nIn order to archive your GitHub repos in Zenodo, link your Zenodo and GitHub accounts\nOnce your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive. \nCreating a Zenodo archive is now as simple as making a release in your GitHub repository. Remember to create a proper tag and specify the version.\nNOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo! \nZenodo will automatically detect the release and it should appear on your Zenodo upload page: My dashboard &gt; Uploads.\nThis archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. \n\nBefore submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI, and cite it in your manuscript!"
  },
  {
    "objectID": "develop/practical_workshop.html#wrap-up",
    "href": "develop/practical_workshop.html#wrap-up",
    "title": "Practical material",
    "section": "Wrap up",
    "text": "Wrap up\nIn this small workshop, we have learned how to improve the FAIRability of your data, as well as organize and structure it in a way that will be much more useful in the future. These advantages do not serve yourself only, but your teammates, group leader, and the general scientific population! We hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to contact us."
  },
  {
    "objectID": "develop/cheatSheet.html",
    "href": "develop/cheatSheet.html",
    "title": "Cheat sheet",
    "section": "",
    "text": "Click on the image to enlarge it or use the download button to save it.\n\n\n\n\n Download me \n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/examples/NGS_management.html",
    "href": "develop/examples/NGS_management.html",
    "title": "Effective RDM Practices in NGS Analysis",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nNGS data strategies\nFile naming conventions examples",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Effective RDM Practices in NGS Analysis"
    ]
  },
  {
    "objectID": "develop/examples/NGS_management.html#practical-tips-for-computational-research",
    "href": "develop/examples/NGS_management.html#practical-tips-for-computational-research",
    "title": "Effective RDM Practices in NGS Analysis",
    "section": "Practical tips for computational research",
    "text": "Practical tips for computational research\n\n1. Experiments / raw data\nThoroughly document your datasets and the experimental setup to ensure reproducibility. Adhering to standards will ensure interoperability. Data types‚Äô examples:\n\nElectronic Laboratory Notebook (ELN): digital description of the experimental design, and measurement devices. ELNs offer features like data entry, text editing, file attachments, collaboration tools, and search capabilities.\nLaboratory protocols: methodologies to prepare and manage samples.\nSamples: refers to the biological material (extraction of DNA, RNA, or proteins). Specification of sample identifier, sample type, source organism, etc.\nSequencing: details on the platform (e.g., Illumina, Oxford Nanopore), library preparation method, coverage, quality control metrics (e.g., Phred score)‚Ä¶\nRaw sequencing data: sequences and quality scores (e.g., FASTQ files)\n\n\n\n\n\n\n\nNote\n\n\n\nA metadata file is crucial during data analysis as it contains information about the experimental conditions (such as sequencing details, treatment, sample type, time points, tissue‚Ä¶).\n\n\n\n\n2. Input / Pre- and post-processing data\nExamples of data types generated during processing:\n\nQuality control metrics: to filter out potential artifacts and ensure the reliability of downstream analyses (e.g., bioinformatics tool like FastQC or MultiQC for results‚Äô aggregation)\nData alignments: in genomics to determine the location of the read in the genome and in transcriptomics to identify gene expression levels.\nDNA analysis results: such as variant calling, genome annotation, functional genomics, phylogenetics, metagenomics, etc. Results are usually presented in tabular format.\nRNA Expression analysis results: from differential gene expression, gene ontology (GO) enrichment, alternative splicing, pathway analysis, etc. Results are usually presented in tabular format.\nEpigenetic profiling outputs: to assess gene regulation and chromatin structure (e.g., ChIP-Seq). Usually presented in BED format.\n\nThe interpretation of NGS data relies heavily on the results of data analysis, which are pivotal for understanding the biological significance of the findings and formulating hypotheses for further exploration. Clear and effective visualization methods are crucial for communicating and interpreting the vast amount of information generated by NGS experiments.\n\n\n\n\n\n\nOther types of data: databases and visualizations\n\n\n\n\n\n\n\n\nKnowledge databases\nA knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\nVisualizations\n\n\nHeatmaps: frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across samples/conditions.\nVolcano Plots: commonly used in differential gene expression analysis\nGenome Browser Snapshots: display alignments and genomic features in genomic regions (e.g., gene annotations, ChIP-Seq peaks)\nNetwork Visualizations:utilized to explore gene regulatory networks or protein-protein interaction\nGenomic Annotations: to annotate genetic variations (functional impact on genes, genomic regions, or regulatory element)\n\n\n\n\n\n\n\n\n3. Software and code:\nBest practices for software and code management (don‚Äôt forget to read about FAIR software):\n\nCommenting your code: to enhance readability and comprehension\nMake your source code accessible using a repository (GitHub, GitLab, Bitbucket, SourceForge, etc.) that provides version control (VC) solutions. This step is one of the most important ones as version control systems (Git or SVN) track changes in your code over time and enable collaboration and easy version management. Most Danish institutions provide courses on Git/GitHub, check yours! We also highly recommend reading this paper (Perez-Riverol et al. 2016).\nREADME file: with comprehensive information about the project including installation instructions, usage examples or tutorials, licensing details, citation information, etc.\nRegister your code in a research software registry and include a clear and accessible software usage license: enabling other researchers to discover and reuse software packages (alongside metadata). More recommendations here.\nUse domain-relevant community standards to ensure consistency and interoperability (e.g., CodeMeta).\n\n\n\n\n\n\n\nGit and Github courses and other resources\n\n\n\n\n\n\n\n\nUniversity of Copenhagen\nAarhus University\nAalborg University\nDTU Git guidelines Find more resources on the Berkeley Library website\n\n\n\n\n\n\n\n\n4. Pipelines and workflows\nYou might use standard workflows or generate new ones during data processing and data analysis steps.\n\nCode notebooks: tools for data documentation (e.g.¬†Jupyter Notebook, Rmarkdown) enabling the combination of code with descriptive text and visualizations.\n\nIntegrated development environments (knitr or MLflow).\nPipeline frameworks or workflow management systems: designed to streamline and automate various steps involved in data analysis (data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages. There are two very popular systems, Nextflow and Snakemake.\n\nA great example of community-curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.\n\n\n\n\n\n\nCourse on pipelines and workflows\n\n\n\nTake our course on Reproducible Research Practices LINK",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Effective RDM Practices in NGS Analysis"
    ]
  },
  {
    "objectID": "develop/examples/NGS_management.html#wrap-up",
    "href": "develop/examples/NGS_management.html#wrap-up",
    "title": "Effective RDM Practices in NGS Analysis",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we have taken a look a the vast and diverse landscape of bioinformatics data.",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Effective RDM Practices in NGS Analysis"
    ]
  },
  {
    "objectID": "develop/examples/proteomics_metadata.html",
    "href": "develop/examples/proteomics_metadata.html",
    "title": "Protemics metadata (In development)",
    "section": "",
    "text": "Protemics metadata (In development)\nCheck this link for more details on different metadata file formats and standarised practices.\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "Proteomics",
      "Protemics metadata (*In development*)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to RDM for biodata",
    "section": "",
    "text": "Welcome to RDM for biodata\n\n\n\n\n\n\n\nPractical RDM workshop\n\n\n\nWe offer workshops on practical RDM for biodata. Keep an eye on the upcoming events on the Sandbox website.\n\n\nThe course ‚ÄúResearch Data Management (RDM) for biological data‚Äù is designed to provide participants with foundational knowledge and practical skills in handling the extensive data generated by modern studies, with a focus on Next Generation Sequencing (NGS) data. It emphasizes the importance of Open Science and FAIR principles in managing data effectively. This course covers essential principles and best practices guidelines in data organization, metadata annotation, version control, and data preservation. These principles are explored from a computational perspective, ensuring participants gain hands-on experience in applying them to real-world scenarios in their research labs. Additionally, the course delves into FAIR principles and Open Science, promoting collaboration and reproducibility in research endeavors. By the course‚Äôs conclusion, attendees will possess essential tools and techniques to address the data challenges prevalent in today‚Äôs NGS research landscape, as well as in other related fields to health and bioinformatics.\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\n\nData Lifecycle Management\nData Management Plans (DMPs)\nData Organization and storage\nDocumentation standards for biodata\nVersion Control and Collaboration\nProcessing and analyzing biodata\nStoring and sharing biodata\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: Ph.D., MSc, anyone interested in RDM for NGS data or other related fields within bioinformatics.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\nThis course offers participants with an in-depth introduction to effectively managing the vast amounts of data generated in modern studies. Throughout the program, emphasis is placed on practical understanding of RDM principles and the importance of efficient handling of large datasets. In this context, participants will learn the necessity of adopting Open Science and FAIR principles for enhancing data accessibility and reusability.\nParticipants will acquire practical skills for organizing data, including the creation of folder and file structures, and the implementation of metadata to facilitate data discoverability and interpretation. Special attention is given to the development of Data Management Plans (DMPs) with examples tailored to omics data, ensuring compliance with institutional and funding agency requirements while maintaining data integrity. Attendees will also gain insights into the establishment of simple databases and the use of version control systems to track changes in data analysis, thereby promoting collaboration and reproducibility.\nThe course concludes with a focus on archiving and data repositories, enabling participants to learn strategies for preserving and sharing data for long-term scientific usage. By the end of the course, attendees will be equipped with essential tools and techniques to effectively navigate the challenges prevalent in today‚Äôs research landscape. This will not only foster successful data management practices but also enhance collaboration within the scientific community.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nFamiliarize Yourself with FAIR and Open Science Principles\nDraft a Data Management Plan for your own Data\nEstablish File and Folder Naming Conventions\nEnhance Data with Descriptive Metadata\nImplement Version Control for Data Analysis\nSelect an Appropriate Repository for Data Archiving\nMake your data analysis and workflows reproducible and FAIR\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a computational workshop that focuses primarily on the digital aspect of our data. While wet lab Research Data Management (RDM) involving protocols, instruments, reagents, ELM or LIMS systems is integral to the entire RDM process, it won‚Äôt be covered in this course.\nAs part of effective data management, it‚Äôs crucial to prioritize strategies that ensure security and privacy. While these aspects are important, please note that they won‚Äôt be covered in our course. However, we highly recommend enrolling in the GDPR course offered by Center for Health Data Science, specially if you‚Äôre working with sensitive data. This course specifically focuses on GDPR compliance and will provide you with valuable insights and skills in managing data privacy and security.\n\n\n\nDanish institutional RDM links\n\nUniversity of Copenhagen\nUniversity Library of Southern Denmark\nTechnical University of Denmark\nAalborg University\nAarhus University\n\n\n\nAcknowledgements\n\nRDMkit, ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden.\n\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "cards/AlbaMartinez.html",
    "href": "cards/AlbaMartinez.html",
    "title": "Alba Refoyo Martinez",
    "section": "",
    "text": "Alba is a Sandbox data scientist based at the University of Copenhagen. During her academic background as a PhD and Postdoc she has developed a solid expertise in large-scale genomics and pipelines development on computing clusters.\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  }
]