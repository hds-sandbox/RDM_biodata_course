[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Computational Research Data Management",
    "section": "",
    "text": "The course ‚ÄúResearch Data Management (RDM) for biological data‚Äù is designed to provide participants with foundational knowledge and practical skills in handling the extensive data generated by modern studies, with a focus on Next Generation Sequencing (NGS) data. It emphasizes the importance of Open Science and FAIR principles in managing data effectively. This course covers essential principal and best practices guidelines in data organization, metadata annotation, version control, and data preservation. These principles are explored from a computational perspective, ensuring participants gain hands-on experience in applying them to real-world scenarios in their research labs. Additionally, the course delves into FAIR principles and Open Science, promoting collaboration and reproducibility in research endeavors. By the course‚Äôs conclusion, attendees will possess essential tools and techniques to address the data challenges prevalent in today‚Äôs NGS research landscape, as well as in other related fields to health and bioinformatics.\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\n\nData Lifecycle Management\nData Management Plans (DMPs)\nData Organization and Documentation\nMetadata standards and data description\nVersion Control and Collaboration\nCode and Pipelines for Data Analysis\nData sharing and data preservation\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: PhD, MsC, anyone interested in RDM for NGS data or other related fields within bioinformatics.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\nThis course offers participants with an in-depth introduction to effectively managing the vast amounts of data generated in modern studies. Throughout the program, emphasis is placed on practical understanding of RDM principles and the importance of efficient handling of large datasets. In this context, participants will learn the necessity of adopting Open Science and FAIR principles for enhancing data accessibility and reusability.\nParticipants will acquire practical skills for organizing data, including the creation of folder and file structures, and the implementation of metadata to facilitate data discoverability and interpretation. Special attention is given to the development of Data Management Plans (DMPs) with examples tailored to omics data, ensuring compliance with institutional and funding agency requirements while maintaining data integrity. Attendees will also gain insights into the establishment of simple databases and the use of version control systems to track changes in data analysis, thereby promoting collaboration and reproducibility.\nThe course concludes with a focus on archiving and data repositories, enabling participants to learn strategies for preserving and sharing data for long-term scientific usage. By the end of the course, attendees will be equipped with essential tools and techniques to effectively navigate the challenges prevalent in today‚Äôs research landscape. This will not only foster successful data management practices but also enhance collaboration within the scientific community.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nFamiliarize Yourself with FAIR and Open Science Principles\nDraft a Data Management Plan for your own Data\nEstablish File and Folder Naming Conventions\nEnhance Data with Descriptive Metadata\nImplement Version Control for Data Analysis\nSelect an Appropriate Repository for Data Archiving\nMake your data analysis and workflows reproducible and FAIR\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a computational workshop that focuses primarily on the digital aspect of our data. While wet lab Research Data Management (RDM) involving protocols, instruments, reagents, ELM or LIMS systems is integral to the entire RDM process, it won‚Äôt be covered in this course.\nAs part of effective data management, it‚Äôs crucial to prioritize strategies that ensure security and privacy. While these aspects are important, please note that they won‚Äôt be covered in our course. However, we highly recommend enrolling in the GDPR course offered by Center for Health Data Science, specially if you‚Äôre working with sensitive data. This course specifically focuses on GDPR compliance and will provide you with valuable insights and skills in managing data privacy and security.\n\n\n\n\n\nUniversity of Copenhagen\nUniversity Library of Southern Denmark\nTechnical University of Denmark\nAalborg University\nAarhus University\n\n\n\n\n\nRDMkit, ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden."
  },
  {
    "objectID": "index.html#research-data-management-for-biological-data",
    "href": "index.html#research-data-management-for-biological-data",
    "title": "Computational Research Data Management",
    "section": "",
    "text": "The course ‚ÄúResearch Data Management (RDM) for biological data‚Äù is designed to provide participants with foundational knowledge and practical skills in handling the extensive data generated by modern studies, with a focus on Next Generation Sequencing (NGS) data. It emphasizes the importance of Open Science and FAIR principles in managing data effectively. This course covers essential principal and best practices guidelines in data organization, metadata annotation, version control, and data preservation. These principles are explored from a computational perspective, ensuring participants gain hands-on experience in applying them to real-world scenarios in their research labs. Additionally, the course delves into FAIR principles and Open Science, promoting collaboration and reproducibility in research endeavors. By the course‚Äôs conclusion, attendees will possess essential tools and techniques to address the data challenges prevalent in today‚Äôs NGS research landscape, as well as in other related fields to health and bioinformatics.\n\n\n\n\n\n\nCourse Overview\n\n\n\n\nüìñ Syllabus:\n\n\nData Lifecycle Management\nData Management Plans (DMPs)\nData Organization and Documentation\nMetadata standards and data description\nVersion Control and Collaboration\nCode and Pipelines for Data Analysis\nData sharing and data preservation\n\n\n‚è∞ Total Time Estimation: X hours\n\nüìÅ Supporting Materials:\n\nüë®‚Äçüíª Target Audience: PhD, MsC, anyone interested in RDM for NGS data or other related fields within bioinformatics.\nüë©‚Äçüéì Level: Beginner.\nüîí License: Creative Commons Attribution-ShareAlike 4.0 International (CC BY-SA 4.0) license.\n\nüí∞ Funding: This project was funded by the Novo Nordisk Fonden (NNF20OC0063268).\n\n\n\n\n\n\n\n\n\n\nCourse Requirements\n\n\n\n\nBasic understanding Next Generation Sequencing data and formats.\nCommand Line experience\nBasic programming experience\nQuarto or Mkdocs tools\n\n\n\nThis course offers participants with an in-depth introduction to effectively managing the vast amounts of data generated in modern studies. Throughout the program, emphasis is placed on practical understanding of RDM principles and the importance of efficient handling of large datasets. In this context, participants will learn the necessity of adopting Open Science and FAIR principles for enhancing data accessibility and reusability.\nParticipants will acquire practical skills for organizing data, including the creation of folder and file structures, and the implementation of metadata to facilitate data discoverability and interpretation. Special attention is given to the development of Data Management Plans (DMPs) with examples tailored to omics data, ensuring compliance with institutional and funding agency requirements while maintaining data integrity. Attendees will also gain insights into the establishment of simple databases and the use of version control systems to track changes in data analysis, thereby promoting collaboration and reproducibility.\nThe course concludes with a focus on archiving and data repositories, enabling participants to learn strategies for preserving and sharing data for long-term scientific usage. By the end of the course, attendees will be equipped with essential tools and techniques to effectively navigate the challenges prevalent in today‚Äôs research landscape. This will not only foster successful data management practices but also enhance collaboration within the scientific community.\n\n\n\n\n\n\nCourse Goals\n\n\n\nBy the end of this workshop, you should be able to apply the following concepts in the context of Next Generation Sequencing data:\n\nUnderstand the Importance of Research Data Management (RDM)\nFamiliarize Yourself with FAIR and Open Science Principles\nDraft a Data Management Plan for your own Data\nEstablish File and Folder Naming Conventions\nEnhance Data with Descriptive Metadata\nImplement Version Control for Data Analysis\nSelect an Appropriate Repository for Data Archiving\nMake your data analysis and workflows reproducible and FAIR\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis is a computational workshop that focuses primarily on the digital aspect of our data. While wet lab Research Data Management (RDM) involving protocols, instruments, reagents, ELM or LIMS systems is integral to the entire RDM process, it won‚Äôt be covered in this course.\nAs part of effective data management, it‚Äôs crucial to prioritize strategies that ensure security and privacy. While these aspects are important, please note that they won‚Äôt be covered in our course. However, we highly recommend enrolling in the GDPR course offered by Center for Health Data Science, specially if you‚Äôre working with sensitive data. This course specifically focuses on GDPR compliance and will provide you with valuable insights and skills in managing data privacy and security.\n\n\n\n\n\nUniversity of Copenhagen\nUniversity Library of Southern Denmark\nTechnical University of Denmark\nAalborg University\nAarhus University\n\n\n\n\n\nRDMkit, ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).\nUniversity of Copenhagen Research Data Management Team.\nMartin Proks and Sarah Lundregan, Brickman Lab, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nRichard Dennis, Data Steward, NNF Center for Stem Cell Biology (reNEW), University of Copenhagen.\nNBISweden."
  },
  {
    "objectID": "cards/AlbaMartinez.html",
    "href": "cards/AlbaMartinez.html",
    "title": "Alba Refoyo Martinez",
    "section": "",
    "text": "Alba is a Sandbox data scientist based at the University of Copenhagen. During her academic background as a PhD and Postdoc she has developed a solid expertise in large-scale genomics and pipelines development on computing clusters.\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "People",
    "section": "",
    "text": "Alba Refoyo Martinez\n\n\n\n\n\nData Scientist, Copenhagen University\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nJos√© Alejandro Romero Herrera\n\n\n\n\n\nPrincipal Bioinformatician, Lundbeck\n\n\n\n\n\n\n\n\n\nNo matching items\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/contributors.html",
    "href": "develop/contributors.html",
    "title": "Practical material",
    "section": "",
    "text": "Alba Refoyo Martinez :custom-orcid::simple-github:\nJose Alejandro Romero Herrera :custom-orcid: :simple-github:\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "develop/06_pipelines.html",
    "href": "develop/06_pipelines.html",
    "title": "6. Code and Pipelines for Data Analysis",
    "section": "",
    "text": "In this section, we explore essential elements of reproducibility and efficiency in computational research, highlighting techniques and tools for creating robust and transparent coding and workflows. By prioritizing reproducibility and replicability, researchers can enhance the credibility and impact of their findings while fostering collaboration and knowledge dissemination within the scientific community.",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Code and Pipelines for Data Analysis"
    ]
  },
  {
    "objectID": "develop/06_pipelines.html#wrap-up",
    "href": "develop/06_pipelines.html#wrap-up",
    "title": "6. Code and Pipelines for Data Analysis",
    "section": "Wrap up",
    "text": "Wrap up\nThis lesson emphasized the importance of reproducibility in computational research and provided practical techniques for achieving it. Using annotated notebooks, pipeline frameworks and community-curated pipelines, such as those developed by the nf-core community, enhances reproducibility and readability.\n\nSources\n\nCode documentation by Johns Hopkins sharidan libraries: https://guides.library.jhu.edu/c.php?g=1096705&p=8066729. This link inlcudes, best practices for code documentation, style guides, R markdown, jupyter notebook and version control and code repository.\n[Guide to reproducible code in ecology and evolution]https://www.britishecologicalsociety.org/wp-content/uploads/2017/12/guide-to-reproducible-code.pdf\nBest practices for Scientific computing",
    "crumbs": [
      "Course material",
      "Key practices",
      "6. Code and Pipelines for Data Analysis"
    ]
  },
  {
    "objectID": "develop/05_VC.html",
    "href": "develop/05_VC.html",
    "title": "5. Data Analysis with Version Control",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nVersion control essentials and practices\nGit and Github repositories\nCreate repositories\nGitHub page to showcase your data analysis reports\n\n\n\nThis lesson introduces version control with Git and Github and its significance in research. You will gain the ability to create Git repositories, and skills to build GitHub pages for showcasing data analysis.\n\n\nVersion control systematically tracks project changes, documenting alterations for understanding project evolution. It holds significant importance in research data management, software development, and data analysis, offering numerous advantages.\n\n\n\n\n\n\nAdvantages of using version control\n\n\n\n\nDocument Progress: Detailed change history aids understanding of project development and modifications.\nEnsure Data Integrity: Prevents accidental data loss or corruption, with each change tracked for easy recovery.\nFacilitate Collaboration: Enables seamless collaboration among team members, allowing multiple individuals to work concurrently without conflicts.\nReproducibility: Preserves project state for accurate validation and analysis.\nBranching and Experimentation: Allows the creation of alternative project versions for experimentation, without altering the main branch.\nGlobal Accessibility: Platforms like GitHub provide visibility for sharing, feedback, and contribution to open science.\n\n\n\n\n\n\n\n\n\nTake our course on Git & Github\n\n\n\nif you‚Äôre interested in delving deeper, explore our course on Git and GitHub.\nAlternatively, here are some examples and online resources to expand your understanding:\n\n[Git and Github online resources] https://docs.github.com/en/get-started/start-your-journey/git-and-github-learning-resources\nGitHub documentation\nGit documentation\n\n\n\n\n\nGit is a widely adopted version control system that empowers developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, track changes, and ensure data integrity. Git operates on core principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, storing the complete project history for independent work.\nSnapshots, Not Files: Git captures snapshots of the entire project at different points instead of tracking individual file changes, ensuring data consistency.\nCommits: Users create ‚Äòcommits‚Äô as snapshots of the project at specific moments, recording changes made to files along with explanatory commit messages.\nBranching: Git supports branching, enabling users to create separate lines of development for new features or bug fixes without affecting the main branch.\nMerging: Changes from one branch can be merged into another, facilitating the incorporation of new features or bug fixes back into the main project with a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means each user‚Äôs local repository is a complete copy of the project, enabling offline work and ensuring data redundancy.\nRemote Repositories: Users can connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub, facilitating collaboration and project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others and ‚Äòpull‚Äô changes made by others into their local repository to stay updated.\nConflict Resolution: Git provides tools to resolve conflicts manually in cases of conflicting changes, ensuring data integrity during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history such as major releases or significant milestones.\n\n\n\n\nIn addition to exploring Git, we will also explore GitHub, a collaborative platform for hosting Git repositories. GitHub enhances Git‚Äôs capabilities by offering features like issue tracking, security measures to protect repositories, and GitHub Pages for creating project websites. Additionally, GitHub provides the option to set repositories as private until you are ready to share your work publicly.\n\n\n\n\n\n\nAlternatives flows for collaborative projects\n\n\n\n\nGitLab\nBitBucket We will focus on GitHub for the remainder of this lesson due to its widespread usage and compatibility.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will discuss repositories for archiving experimental or large datasets in lesson 7.\n\n\n\n\nMoving from Git to GitHub involves transitioning from a local version control setup to a remote hosting platform. You will need a GitHub account for the exercise in this section.\n\n\n\n\n\n\nCreate a GitHub account\n\n\n\n\nIf you don‚Äôt have a GitHub account yet, click here.\nInstall Git from Git webpage\n\n\n\nYou have two options when it comes to creating a repository for your project. First, you can start from scratch by creating a new repository and adding files to it as your project progresses. Alternatively, if you already have an existing folder structure for your project, you can initialize a repository directly from that folder. It is crucial to initiate version control in the early stages of a project to facilitate easy tracking of changes and effective management of the project‚Äôs version history from the beginning.\n\n\nIf you completed all the exercises in lesson 3, you should have a project data structure prepared. Otherwise, consider using one of your existing projects or creating a small toy example for practice using cookiecutter (see practical_workshop).\n\n\n\n\n\n\nGithub documentation link\n\n\n\n\nAdding locally hosted code to Github\n\n\n\n\n\n\n\n\n\nExercise 1: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nFirst, initialize the repository using the command git init. This command is run only once, even in collaborative projects (git init).\nOnce the repository is initialized, create a remote repository on GitHub.\nAdd the remote URL to your local git repository using git remote add origin &lt;URL&gt;`. This associates the remote URL with the name ‚Äúorigin‚Äù.\nEnsure you have at least one commit in your history by staging existing files with git add and then creating a snapshot, known as committing, with git commit.\nFinally, push your local commits to the remote repository and establish a tracking relationship using git push -u origin master.\n\n\n\n\n\n\n\n\n\nAlternatively to converting folders to repositories, you can create a new repository remotely, and then clone (git clone) it locally. Here, git init is not needed. You can move the files into the repository locally (git add, git commit, and git push). If you are creating a collaborative repository, you can now share it with your colleagues.\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nWrite useful and clear Git commits. Check out this post for tips.\n\n\n\n\n\n\n\nAfter setting up your repository on GitHub, take advantage of the opportunity to enhance it by adding your data analysis reports. Whether they are in Jupyter Notebooks, Rmarkdowns, or HTML reports, you can showcase them on a GitHub Page.\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns, or HTML reports, in a GitHub Page website. Creating a GitHub page is very simple, and we recommend that you follow the nice tutorial that GitHub has put for you.\nFor simplicity, we recommend using Quarto or MkDocs. Visit their websites and follow the instructions to get started.\n\n\n\n\n\n\nTutorial links\n\n\n\n\nGet started in quarto: https://quarto.org/docs/get-started/. We recommend using VS code tool, if you do, follow this tutorial.\nMkDocs materials to further customize MkDocs websites.\n\n\n\n\n\n\nWe provide an example of setting up Git, MkDocs, and a GitHub account, enabling you to replicate the process independently! (see Exercise 5 in the practical material)\n\n\n\n\nIn this lesson, we explored version control and utilized Git and GitHub to establish data analysis repositories from our Project folders. Additionally, we delved into creating a GitHub organization and leveraging GitHub Pages to showcase data analysis scripts and notebooks publicly. Remember to complete the corresponding exercise from the practical workshop to reinforce your knowledge.\n\n\n\nVersion Control and Code Repository Link.",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/05_VC.html#version-control",
    "href": "develop/05_VC.html#version-control",
    "title": "5. Data Analysis with Version Control",
    "section": "",
    "text": "Version control systematically tracks project changes, documenting alterations for understanding project evolution. It holds significant importance in research data management, software development, and data analysis, offering numerous advantages.\n\n\n\n\n\n\nAdvantages of using version control\n\n\n\n\nDocument Progress: Detailed change history aids understanding of project development and modifications.\nEnsure Data Integrity: Prevents accidental data loss or corruption, with each change tracked for easy recovery.\nFacilitate Collaboration: Enables seamless collaboration among team members, allowing multiple individuals to work concurrently without conflicts.\nReproducibility: Preserves project state for accurate validation and analysis.\nBranching and Experimentation: Allows the creation of alternative project versions for experimentation, without altering the main branch.\nGlobal Accessibility: Platforms like GitHub provide visibility for sharing, feedback, and contribution to open science.\n\n\n\n\n\n\n\n\n\nTake our course on Git & Github\n\n\n\nif you‚Äôre interested in delving deeper, explore our course on Git and GitHub.\nAlternatively, here are some examples and online resources to expand your understanding:\n\n[Git and Github online resources] https://docs.github.com/en/get-started/start-your-journey/git-and-github-learning-resources\nGitHub documentation\nGit documentation\n\n\n\n\n\nGit is a widely adopted version control system that empowers developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, track changes, and ensure data integrity. Git operates on core principles and mechanisms:\n\nLocal Repository: Each user maintains a local repository on their computer, storing the complete project history for independent work.\nSnapshots, Not Files: Git captures snapshots of the entire project at different points instead of tracking individual file changes, ensuring data consistency.\nCommits: Users create ‚Äòcommits‚Äô as snapshots of the project at specific moments, recording changes made to files along with explanatory commit messages.\nBranching: Git supports branching, enabling users to create separate lines of development for new features or bug fixes without affecting the main branch.\nMerging: Changes from one branch can be merged into another, facilitating the incorporation of new features or bug fixes back into the main project with a smooth merging process.\nDistributed Architecture: Git‚Äôs distributed nature means each user‚Äôs local repository is a complete copy of the project, enabling offline work and ensuring data redundancy.\nRemote Repositories: Users can connect and synchronize their local repositories with remote repositories hosted on platforms like GitHub, facilitating collaboration and project sharing.\nPush and Pull: Users ‚Äòpush‚Äô their local changes to a remote repository to share with others and ‚Äòpull‚Äô changes made by others into their local repository to stay updated.\nConflict Resolution: Git provides tools to resolve conflicts manually in cases of conflicting changes, ensuring data integrity during collaboration.\nVersioning and Tagging: Git offers versioning and tagging capabilities, allowing users to mark specific points in history such as major releases or significant milestones.\n\n\n\n\nIn addition to exploring Git, we will also explore GitHub, a collaborative platform for hosting Git repositories. GitHub enhances Git‚Äôs capabilities by offering features like issue tracking, security measures to protect repositories, and GitHub Pages for creating project websites. Additionally, GitHub provides the option to set repositories as private until you are ready to share your work publicly.\n\n\n\n\n\n\nAlternatives flows for collaborative projects\n\n\n\n\nGitLab\nBitBucket We will focus on GitHub for the remainder of this lesson due to its widespread usage and compatibility.\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWe will discuss repositories for archiving experimental or large datasets in lesson 7.\n\n\n\n\nMoving from Git to GitHub involves transitioning from a local version control setup to a remote hosting platform. You will need a GitHub account for the exercise in this section.\n\n\n\n\n\n\nCreate a GitHub account\n\n\n\n\nIf you don‚Äôt have a GitHub account yet, click here.\nInstall Git from Git webpage\n\n\n\nYou have two options when it comes to creating a repository for your project. First, you can start from scratch by creating a new repository and adding files to it as your project progresses. Alternatively, if you already have an existing folder structure for your project, you can initialize a repository directly from that folder. It is crucial to initiate version control in the early stages of a project to facilitate easy tracking of changes and effective management of the project‚Äôs version history from the beginning.\n\n\nIf you completed all the exercises in lesson 3, you should have a project data structure prepared. Otherwise, consider using one of your existing projects or creating a small toy example for practice using cookiecutter (see practical_workshop).\n\n\n\n\n\n\nGithub documentation link\n\n\n\n\nAdding locally hosted code to Github\n\n\n\n\n\n\n\n\n\nExercise 1: initialize a repository from an existing folder:\n\n\n\n\n\n\n\n\nFirst, initialize the repository using the command git init. This command is run only once, even in collaborative projects (git init).\nOnce the repository is initialized, create a remote repository on GitHub.\nAdd the remote URL to your local git repository using git remote add origin &lt;URL&gt;`. This associates the remote URL with the name ‚Äúorigin‚Äù.\nEnsure you have at least one commit in your history by staging existing files with git add and then creating a snapshot, known as committing, with git commit.\nFinally, push your local commits to the remote repository and establish a tracking relationship using git push -u origin master.\n\n\n\n\n\n\n\n\n\nAlternatively to converting folders to repositories, you can create a new repository remotely, and then clone (git clone) it locally. Here, git init is not needed. You can move the files into the repository locally (git add, git commit, and git push). If you are creating a collaborative repository, you can now share it with your colleagues.\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nWrite useful and clear Git commits. Check out this post for tips.\n\n\n\n\n\n\n\nAfter setting up your repository on GitHub, take advantage of the opportunity to enhance it by adding your data analysis reports. Whether they are in Jupyter Notebooks, Rmarkdowns, or HTML reports, you can showcase them on a GitHub Page.\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns, or HTML reports, in a GitHub Page website. Creating a GitHub page is very simple, and we recommend that you follow the nice tutorial that GitHub has put for you.\nFor simplicity, we recommend using Quarto or MkDocs. Visit their websites and follow the instructions to get started.\n\n\n\n\n\n\nTutorial links\n\n\n\n\nGet started in quarto: https://quarto.org/docs/get-started/. We recommend using VS code tool, if you do, follow this tutorial.\nMkDocs materials to further customize MkDocs websites.\n\n\n\n\n\n\nWe provide an example of setting up Git, MkDocs, and a GitHub account, enabling you to replicate the process independently! (see Exercise 5 in the practical material)",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/05_VC.html#wrap-up",
    "href": "develop/05_VC.html#wrap-up",
    "title": "5. Data Analysis with Version Control",
    "section": "",
    "text": "In this lesson, we explored version control and utilized Git and GitHub to establish data analysis repositories from our Project folders. Additionally, we delved into creating a GitHub organization and leveraging GitHub Pages to showcase data analysis scripts and notebooks publicly. Remember to complete the corresponding exercise from the practical workshop to reinforce your knowledge.\n\n\n\nVersion Control and Code Repository Link.",
    "crumbs": [
      "Course material",
      "Key practices",
      "5. Data Analysis with Version Control"
    ]
  },
  {
    "objectID": "develop/07_repos.html",
    "href": "develop/07_repos.html",
    "title": "7. Repositories for bio data",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nRepositories for managing biological data\nArchive your GitHub data analysis repositories\nWhile platforms like GitHub excel in version control and collaborative coding, repositories like Zenodo, Gene Expression Omnibus, and Annotare specialize in archiving and sharing scientific data, ensuring long-term accessibility for the global research community.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for bio data"
    ]
  },
  {
    "objectID": "develop/07_repos.html#data-repositories-and-archives",
    "href": "develop/07_repos.html#data-repositories-and-archives",
    "title": "7. Repositories for bio data",
    "section": "Data Repositories and Archives",
    "text": "Data Repositories and Archives\nSpecialized repositories and archives securely store, curate, and disseminate scientific data, ensuring long-term preservation, transparency, and citability of research findings through standardized formats and rigorous curation processes.\nImportance of archiving scientific data\n\nLong-term accessibility and preservation: Ensures data remains accessible for future researchers.\nEnhanced visibility and attribution: Unique identifiers like DOIs enable citation of datasets, enhancing visibility and proper attribution.\nImproved dataset discoverability and interpretability: Comprehensive metadata, including methodology and experimental details, facilitates understanding and usability by other researchers.\nPromotion of Transparency, Reproducibility, and Research Integrity: Mandatory data deposition fosters transparency and upholds research integrity.\nAmplification of Research Impact and Contribution: Archiving data elevates research quality and extends its impact within the scientific community.\nFulfilling Scholarly Obligations: Compliance with requirements set by scientific journals and funding agencies ensures adherence to scholarly standards.\n\nThere are two types of repositories:\n\nGeneral repositories: relevant to a wide range of disciplines (e.g.¬†Zenodo).\nDomain-specific: repositories are customized for specific fields, providing specialized curation and context-specific features (e.g.¬†ENA, GEO, Annotare etc.).\n\n\n\n\n\n\n\nList of repositories for biological data\n\n\n\n\nEuropean Nucleotide Archive (ENA)\nNCBI Gene Expression Omnibus (GEO)\nSequence Read Archive (SRA)\nProtein Data Bank (PDB)\nKyoto Encyclopedia of Genes and Genomes (KEGG)\nUniversal Protein Resource (UniProt)\nHuman Protein Atlas\nArrayExpress\nModel Organism Databases (MODs)\nEBI Metagenomics (MGnify)\nPhysioNet\nFunctional Annotation of Animal Genomes (FAANG) Data Repository\nCheck ELIXIR Core Data Resources here\n\nYour institution might as well have their own repositories such as ERDA (Electronic research data archive at University of Copenhagen).",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for bio data"
    ]
  },
  {
    "objectID": "develop/07_repos.html#domain-specific-repositories",
    "href": "develop/07_repos.html#domain-specific-repositories",
    "title": "7. Repositories for bio data",
    "section": "Domain-specific repositories",
    "text": "Domain-specific repositories\nThis tailored approach ensures alignment with standards and maximizes utility and impact of research findings. By catering to particular research areas, these repositories offer researchers a more focused audience, deeper domain expertise, and increased visibility within their specific research community.\nExplore some examples of NGS data repositories below:\n\n\n\n\n\n\nENA (European Nucleotide Archive)\n\n\n\n\n\n\n\nENA: hosted by the European Bioinformatics Institute (EBI), provides researchers with a platform to deposit and access nucleotide sequences along with associated metadata, ensuring data preservation and contextualization. ENA adheres to community standards and guidelines for data submission, including those established by the International Nucleotide Sequence Database Collaboration (INSDC).\n\n\n\n\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus)\n\n\n\n\n\n\n\nGEO (Gene Expression Omnibus): curated by the National Center for Biotechnology Information (NCBI), serves as a specialized repository for high-throughput functional genomic data sets, particularly gene expression data across diverse biological conditions and experimental designs. Researchers can easily deposit and access a variety of genomic data, fostering data transparency and reproducibility within the scientific community. GEO assigns unique accession numbers to each dataset, ensuring traceability and proper citation in research publications.\n\n\n\n\n\n\n\n\n\n\n\nAnnotare\n\n\n\n\n\n\n\nArrayExpress/Annotare: hosted by the European Bioinformatics Institute (EBI), is a specialized repository tailored for storing and submitting functional genomics experiments for high-throughput sequencing data. It offers researchers a platform to upload experimental data along with comprehensive metadata ensuring preservation and contextualization. Annotare provides a curated environment aligned with the standards and practices of the field. This specialization enhances data discoverability, promotes collaboration, and facilitates deeper insights into the functional aspects of the genome.\n\n\n\n\n\nThe repositories mentioned earlier adhere to established community standards for data submission and sharing in genomics research such as:\n\nMIAME (Minimum Information About a Microarray Experiment): These guidelines ensure comprehensive and standardized reporting of microarray experiments.\nMIxS (Minimum Information about a high-throughput SeQuencing Experiment): MIxS standards, developed by the Genomic Standards Consortium, ensure consistent reporting of metadata for high-throughput sequencing experiments.\nSequence Read Archive (SRA) Submission Guidelines: They include requirements for data formatting, metadata inclusion, and quality control.\nCommunity-Specific Standards designed to ensure that submitted data meets the specific requirements and expectations of the field.\n\nBy adhering to standards, repositories ensure that submitted data is high quality, well-documented, and compliant with community best practices, promoting data discovery, reproducibility, and interoperability within the scientific community.\nFollowing all the recommendations in this course makes it straightforward to provide the necessary documentation and information for these repositories. For instance, repositories specific to NGS data will require the raw FASTQ files, sample metadata and protocols as well as final pre-processing results (for instance, read count matrices in BED files).\n\n\n\n\n\n\nWarning\n\n\n\nKeep in mind that these repositories are not intended for downstream analysis data and associated code. However, you should already have those version controlled by GitHub, which eliminates any concerns. You can then archive such repositories in a general repository like Zenodo.",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for bio data"
    ]
  },
  {
    "objectID": "develop/07_repos.html#general-repositories",
    "href": "develop/07_repos.html#general-repositories",
    "title": "7. Repositories for bio data",
    "section": "General repositories",
    "text": "General repositories\nZenodo is one of the widely used repositories for a variety of research outputs. It is an open-access digital platform supported by the European Organization for Nuclear Research (CERN) and the European Commission. It caters to various research outputs, including datasets, papers, software, and multimedia files, making it a valuable resource for researchers worldwide.With its user-friendly platform, researchers can easily upload, share, and preserve their research data. Each deposited item receives a unique Digital Object Identifier (DOI), ensuring citability and long-term accessibility. Additionally, Zenodo offers robust metadata capabilities for enriching submissions with contextual information. Moreover, researchers can link their GitHub accounts to Zenodo, simplifying the process of archiving GitHub repository releases for long-term accessibility and citation.\nOnce your accounts are linked, creating a Zenodo archive becomes as straightforward as tagging a release in your GitHub repository. Zenodo automatically detects the release and generates a corresponding archive, complete with a unique Digital Object Identifier (DOI) for citable reference. Therefore, before submitting your work to a journal, link your data analysis repository to Zenodo, obtain a DOI, and cite it in your manuscript which enhances reproducibility in research.\n\nStep-by-Step Setup Guide\nCheck the practical material where we demonstrate how to link Zenodo and Github (see Exercise 6 in the practical material).",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for bio data"
    ]
  },
  {
    "objectID": "develop/07_repos.html#wrap-up",
    "href": "develop/07_repos.html#wrap-up",
    "title": "7. Repositories for bio data",
    "section": "Wrap up",
    "text": "Wrap up\nIn this concluding lesson, we‚Äôve covered the process of submitting your data to a domain-specific repository and archiving your data analysis GitHub repositories in Zenodo. By applying the lessons from this workshop, you‚Äôll significantly enhance the FAIRness of your data and improve its organization for future use. These benefits extend beyond yourself to your teammates, group leader, and the wider scientific community.\nWe hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to answer this form!",
    "crumbs": [
      "Course material",
      "Key practices",
      "7. Repositories for bio data"
    ]
  },
  {
    "objectID": "develop/examples/NGS_metadata.html",
    "href": "develop/examples/NGS_metadata.html",
    "title": "NGS Assay and Project metadata",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Develop your own metadata\n\n\nYou should consider revisiting these examples after completing lesson 4 in the course material. Please review these three tables containing pre-filled data fields for metadata, each serving distinct purposes: sample metadata, project metadata, and experimental metadata.\n\nSample metadata fields\nSome details might be specific to your samples. For example, which samples are treated, which are control, which tissue they come from, which cell type, the age, etc. Here is a list of possible metadata fields that you can use:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nsample\nName of the sample\nNA\nNA\ncontrol_rep1, treat_rep1\n\n\nfastq_1\nPath to fastq file 1\nNA\nNA\nAEG588A1_S1_L002_R1_001.fastq.gz\n\n\nfastq_2\nPath to paired fastq file, if it is a paired experiment\nNA\nNA\nAEG588A1_S1_L002_R2_001.fastq.gz\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;unstranded OR forward OR reverse \\&gt;\nNA\nunstranded\n\n\ncondition\nVariable of interest of the experiment, such as \"control\", \"treatment\", etc\nwordWord\ncamelCase\ncontrol, treat1, treat2\n\n\ncell_type\nThe cell type(s) known or selected to be present in the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ntissue\nThe tissue from which the sample was taken\nNA\nUberon\nNA\n\n\nsex\nThe biological/genetic sex of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\ncell_line\nCell line of the sample\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\norganism\nOrganism origin of the sample\n&lt;Genus species&gt;\nTaxonomy\nMus musculus\n\n\nreplicate\nReplicate number\n&lt;integer\\&gt;\nNA\n1\n\n\nbatch\nBatch information\nwordWord\ncamelCase\n1\n\n\ndisease\nAny diseases that may affect the sample\nNA\nDisease Ontology or MONDO\nNA\n\n\ndevelopmental_stage\nThe developmental stage of the sample\nNA\nNA\nNA\n\n\nsample_type\nThe type of the collected specimen, eg tissue biopsy, blood draw or throat swab\nNA\nNA\nNA\n\n\nstrain\nStrain of the species from which the sample was collected, if applicable\nNA\nontology field - e.g. NCBITaxonomy\nNA\n\n\ngenetic variation\nAny relevant genetic differences from the specimen or sample to the expected genomic information for this species, eg abnormal chromosome counts, major translocations or indels\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nProject metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation\n\n\n\n\n\n\n\n\n\n\nAssay metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\nIt is important that the metadata includes key details such as the project‚Äôs short description, author information, creation date, experimental protocol, assay ID, assay type, platform utilized, library details, keywords, sample count, paired-end status, processor information, organism studied, sample origin, and file path.\nIf you would create a database from the metadata files, your table should look like this (each row corresponding to one project):\n\n\n\n\n\n\n\n\n\nassay_ID\nassay_type\nassay_subtype\nowner\nplatform\nextraction_method\nlibrary_method\nexternal_accessions\nkeyword\ndate\nnsamples\nis_paired\npipeline\nstrandedness\nprocessed_by\norganism\norigin\npath\nshort_desc\nELN_ID\n\n\n\n\nRNA_oct4_20200101\nRNAseq\nbulk RNAseq\nSarah Lundregan\nNextSeq 2000\nNA\nNA\nNA\noct4\n20200101\n9\npaired\nnf-core/chipseq 2.3.1\n*\nSL\nMus musculus\ninternal\nNA\nBulk RNAseq of Oct4 knockout\n234\n\n\nCHIP_oct4_20200101\nChIPseq\nbulk ChIPseq\nJose Romero\nNextSeq 2000\nNA\nNA\nNA\noct4\n20200101\n9\nsingle\nnf-core/rnaseq 3.12.0\n*\nJARH\nMus musculus\ninternal\nNA\nBulk ChIPseq of Oct4 overexpression\n123\n\n\nCHIP_med1_20190204\nChIPseq\nbulk ChIPseq\nMartin Proks\nNextSeq 2000\nNA\nNA\nNA\nmed1\n20190204\n12\nsingle\nnf-core/rnaseq 3.12.0\n*\nMP\nMus musculus\ninternal\nNA\nBulk ChIPseq of Med1 overexpression\n345\n\n\nSCR_humanSkin_20210302\nRNAseq\nsingle cell RNAseq\nJose Romero\nNextSeq 2000\nNA\nNA\nNA\nhumanSkin\n20210302\n23123\npaired\nnf-core/scrnaseq 1.8.2\n*\nJARH\nHomo sapiens\nexternal\nNA\nscRNAseq analysis of human skin development\nNA\n\n\nSCR_humanBrain_20220610\nRNAseq\nsingle cell RNAseq\nMartin Proks\nNextSeq 2000\nNA\nNA\nNA\nhumanBrain\n20220610\n1234\npaired\ncustom\n*\nMP\nHomo sapiens\nexternal\nNA\nscRNAseq analysis of human brain development\nNA\n\n\n\n\n\n\n\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "NGS data",
      "NGS Assay and Project metadata"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html",
    "href": "develop/01_RDM_intro.html",
    "title": "1. Introduction to RDM",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nFundamentals of Research Data Management\nEffective Research Data Management Guidelines\nData Lifecycle Management and phases\nFAIR principles and Open Science",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#guidelines-and-benefits-of-effective-rdm",
    "href": "develop/01_RDM_intro.html#guidelines-and-benefits-of-effective-rdm",
    "title": "1. Introduction to RDM",
    "section": "Guidelines and benefits of effective RDM",
    "text": "Guidelines and benefits of effective RDM\nRDM ensures ethical and legal compliance with research requirements. Effective RDM can significantly benefit research and provide advantages for individual researchers:\n\nDetailed data management planning helps in identifying and addressing potential uses, alining expectations among collaborators, and clarifying data rights and ownership\nTransparent and Structured Data Management enhances the reliability and credibility of research findings\nData documentation and data sharing promotes discoverability and facilitates collaborations. Clear documentation of research also streamlines access to previous work, enhancing efficiency, building upon existing knowledge, maximizing research value, accelerating scientific discoveries, and improving visibility and impact\nRisk assessments and strategies for data storage and security can prevent data loss, breaches, or misuse and safeguard sensitive data\nLong-Term Preservation. Data accessibility well after the project‚Äôs completion contributes to data accessibility and continued research relevance\n\n\n\n\n\n\n\nConsequences of poor RDM\n\n\n\n\n\nSeveral surveys have shown that data scientists spend almost half of their time loading and cleaning data, becoming the most consuming, and what many would call tedious, tasks of their jobs (‚ÄúThe State of Data Science 2020 Moving from Hype Toward Maturity‚Äù 2020; ‚ÄúCleaning Big Data: Most Time-Consuming, Least Enjoyable Data Science Task, Survey Says‚Äù 2016).\nCan you consider why we dedicate such a significant amount of time? Perhaps these images look familiar to you!\n Caption: Top-left: Photo by Wonderlane on Unsplash; Top-right: From Stanford Center for Reproducible Neuroscience; Bottom: Messy folder structure, by J.A.HR\nIneffective data management practices can have significant consequences that affect your future self, colleagues, or collaborators who may have to deal with your data. The implications of poor data management include:\n\nDifficulty in Data Retrieval: Without proper organization and documentation, finding specific data files or understanding their content becomes challenging and time-consuming, leading to inefficiency.\nLoss of Data: Inadequate backup and storage strategies increase the risk of data loss (hardware failures, accidental deletions‚Ä¶), potentially erasing months or years of work.\nData Incompleteness and Errors: Insufficient documentation leads to ambiguity and errors in data interpretation undermining research credibility.\nDifficulty in Reproducibility: Poor data management hinders scientific progress by impeding the reproduction of research results.\nDelayed or Compromised Collaboration: disorganized data slows down collaborative research projects, hindering communication.\nData Security and Privacy Risks: Inadequate security measures measures expose sensitive information to breaches, risking privacy.\nWasted Time and Resources: Poor management diverts resources from research tasks, increasing labor costs (additional time on data management).\nFinancial Implications: Time-consuming data management tasks lead to increased labor costs and potential project delays. Data loss can also have negative implications.\nReputational Damage: Inaccurate or irreproducible research outcomes harm a researcher‚Äôs credibility in the scientific community.\n\nTo address these challenges, prioritizing and investing in effective RDM practices like organization, documentation, backup strategies, and data security and preservation protocols, can prevent setbacks, ensure data integrity, and enhance scientific research reliability.\n\n\n\n\n\n\n\n\n\nExercise 1: RDM practices\n\n\n\n\n\n\n\nThink about situations that you believe are consequences of poor data management that may have occurred in your research environment, or discuss if you have encountered any of the following.\n\nA researcher struggles over time with disorganized data, hindering efficient locating of files and causing delays in analysis.\nInadequate documentation of data collection leads to misinterpretations and errors in analysis by other researchers or colleagues.\nPoorly organized data requires extensive cleaning, wasting valuable research time.\nLack of proper documentation and data availability in a groundbreaking study raises doubts about the validity of its findings (from the lack of reproducibility).\n\nHow would you approach these issues differently or what steps would you take to address them?\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nImplementation of a clear and consistent folder structure with descriptive file names. Additionally, using version control systems, such as Git, for code and analysis files can help track changes and facilitate easy retrieval of previous versions of analyses and results.\nProper data documentation, including detailed metadata, could have been maintained throughout the data collection process, providing necessary context and reducing the risk of incomplete or ambiguous data.\nFollowing FAIR principles (Findable, Accessible, Interoperable, Reusable) by making their data, along with detailed methods and documentation, openly accessible in a reputable data repository.\nImplementation of management strategies from the outset of the research project saves time and resources later on, ensuring that data is well-organized and properly documented.",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#research-data-cycle",
    "href": "develop/01_RDM_intro.html#research-data-cycle",
    "title": "1. Introduction to RDM",
    "section": "Research Data Cycle",
    "text": "Research Data Cycle\nThe Research Data Life Cycle is a structured framework depicting the stages of data from its creation or collection to its eventual archiving or disposal. Comprising inception, collection, processing, analysis, sharing, and preservation stages, the cycle parallels a living organism‚Äôs growth, demanding tailored management at each phase. Mastering this cycle is vital for researchers to maintain data integrity, accessibility, and long-term (re)usability, as it fosters transparency, reproducibility, and collaboration in scientific research.\nThe data life cycle is described in 6 phases:\n\nPlan: definition of the objectives, and data requirements, and develop a data management plan outlining data collection, storage, sharing, and ethical/legal considerations.\nCollect and Document: data is gathered according to the plan, and important details such as source, collection methods, and modifications are documented to ensure quality and facilitate future use.\nProcess and Analyse: data is processed and analyzed using various methods and tools to extract meaningful insights. This involves transforming, cleaning, and formatting data for analysis.\nStore and Secure: data is stored securely to prevent loss, unauthorized access, or corruption. Researchers select appropriate storage solutions and implement security measures to protect sensitive information.\nPublish and Share: sharing data openly, following Open Science and FAIR principles, to foster collaboration, increase research visibility, and enable data reuse by others.\nPreserve: Valuable data is preserved in trusted repositories or archives to ensure long-term accessibility and usability for the scientific community.\n\n\n\n\nResearch Data Life Cycle, University of Copenhagen RDM guidelines\n\n\n\n\n\n\n\n\nRead more on your institution‚Äôs website\n\n\n\n\n\n\n\n\nUniversity of Copenhagen and pdf link\nAarhus University\nUniversity of Southern Denmark, SDU\nAalborg University and FAIR link\nUniversity of Southern Denmark, SDU\n\n\n\n\n\n\nTo delve deeper into this topic, click below and explore each phase of the data life cycle. You will find tips and links to future material.\n\n\n Phases of the data life cycle in detail\n\n\n\n1. Plan\nThe management of research data must be thoroughly considered before physical materials and digital data are collected, observed, generated, created, or reused. This includes developing and documenting data management plans (DMP) in electronic format.DMPs should be updated when significant changes occur and stored alongside the corresponding research data. It‚Äôs essential to discuss DMPs with project collaborators, research managers, and supervisors to establish responsibilities for data management activities during and after research projects.\n\n\n\n\n\n\nTip\n\n\n\nCheck out next lesson to learn more about creating effective DMPs.\n\n\n\n\n2. Collect and Document\nResearch data collection and processing should be in line with the best practices within the respective research discipline. Research projects should be documented in a way that enables reproducibility by others. This entails providing clear and accurate descriptions of project methodology, software, and code utilized. Additionally, workflows for data preprocessing and file structuring should be outlined.\nResearch data should be described in metadata to enable effective searching, identification, and interpretation of the data, with metadata linked to the research data for as long as they exist.\n\n\n\n\n\n\nTip\n\n\n\n\nWe will cover strategies for organizing your files and folder in lesson 3.\nWe will discuss different types of metadata in lesson 4\n\n\n\n\n\n3. Process and analyze\nDuring this phase, researchers employ computational methods and bioinformatics tools to extract meaningful information from the data. Good coding practices ensure well-documented and reproducible analyses. For example, code notebooks and version control tools, such as Git, are essential for transparency and sharing results with the scientific community.\nTo streamline and standardize the data analysis process, researchers often implement workflows and pipelines, automating multiple analysis steps to enhance efficiency and consistency while promoting reproducibility.\n\n\n\n\n\n\nTip\n\n\n\n\nCollaborative efforts by the nf-core community provide curated pipelines across different areas of bioinformatics and genomics.\nLearn more about version control in lesson 5\nIf you want to implement your own pipelines, we have the course for you [IN DEVELOPMENT].\n\n\n\n\n\n4. Store and Secure\nResearch data must be classified based on sensitivity and the potential impact to the research institution from unauthorized disclosure, alteration, or destruction. Risks to data security and data loss should be assessed accordingly. This includes evaluating:\n\nPhysical and digital access to research data\nRisks associated with data management procedures\nBackup requirements and backup procedures\nExternal and internal threats to data confidentiality, integrity and accessibility\nFinancial, regulatory, and technical consequences of working with data, data storage, and data preservation\n\n\n\n\n\n\n\nWarning\n\n\n\nThis step is very specific to the setup used in your environment so we cannot include it in a comprehensive guideline on this matter.\n\nEnroll in the next GDPR course offered by the Center for Health Data Science to learn more about data protection and GDPR compliance.\n\n\n\n\n\n5. Share and publish\nLegislation or agreements may restrict research data sharing and require obtaining relevant approvals and establishing agreements allowing sharing. By default, research data should be made openly available post-project, especially for data underlying research publications. This approach balances openness with considerations like intellectual property, personal data protection, and national interests in accordance with the principle of ‚Äòas open as possible, as closed as necessary‚Äô. When data cannot be openly shared, sharing associated metadata is encouraged.\nAdherence to FAIR principles (findable, accessible, interoperable, and reusable) is crucial, which includes:\n\nProviding open access to data (Open Data) by depositing data in a data repository, or by providing access to information on whether, when, how, and to what extent data can be accessed if data sets cannot be made openly available.\nUsing persistent identifiers (PID) and metadata (such as descriptive keywords) that help locate the data set.\nCommunicating terms for data reuse, for example by attaching a data license.\nOffering the necessary information to understand the process of data creation, purpose, and structure.\n\n\n\n\n\n\n\nTip\n\n\n\n\nMore on FAIR and OS principles in the next section\n\n\n\n\n\n6. Preserve\nArrangements for long-term preservation (data and metadata) must adhere to legislation and agreements. This should include:\n\nInformation on research data: At least the data sets supporting published research must be preserved to address objections or criticisms.\nPreservation duration: Retain data supporting research publications for at least five years post-project or publication.\nChoose preservation format and location: Determine format, location, and associated metadata.\nDelete/destroy data if excluded by legislation or agreements, or if preservation isn‚Äôt necessary or possible (for example, when research data can easily be reproduced or is too costly to store or when material quality will deteriorate over time).\nAssign responsibility: Appoint individuals or roles to safeguard data integrity post-project.\nDetermine access rights: Establish rights for accessing and using preserved data sets.\n\nCheck with your institution their requirements for data preservation, such as keeping copies accessible to research managers and peers at the institution‚Äôs premises.\n\n\n\n\n\n\nExample - University of Copenhagen\n\n\n\n\n\n\n\nFor example, the UCPH mandates that a copy of data sets and associated metadata must remain at UCPH after the project ends and/or when employment with the University ceases, in a way in which they are accessible to research managers and understandable for research managers and peers, unless legislation or agreements determine otherwise.\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nWe will check about which repositories you can use to preserve your NGS data in the 10th lesson\n\n\n\n\nTo guarantee effective RDM, researchers should follow the FAIR principles.",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#fair-and-open-science",
    "href": "develop/01_RDM_intro.html#fair-and-open-science",
    "title": "1. Introduction to RDM",
    "section": "FAIR and Open Science",
    "text": "FAIR and Open Science\nOpen Science and FAIR principles have become essential frameworks for promoting transparency, accessibility, and reusability in scientific research. While Open Science advocates for unrestricted access to research outputs, data, and methodologies, FAIR principles emphasize making data Findable, Accessible, Interoperable, and Reusable. Together, they foster collaboration, transcend disciplinary boundaries, and support long-term data preservation. However, they were not directly relevant to software until recently. Governments and funding agencies worldwide increasingly recognize their value and are actively promoting their adoption in academia. In this section, you will learn how to apply these principles to your research.\n\nOpen Science\nOpen Science facilitates wider accessibility of research outputs, fosters collaboration, and promotes transparency and reproducibility, thus enhancing responsible research conduct. Economically, promoting Open Access and data sharing maximizes the return on research investments and fuels innovation and entrepreneurship. This approach enables businesses and startups to leverage research findings, while researchers can integrate large datasets for new discoveries.\n\n\n\nOpen Science, Australian Citizen Science Association\n\n\n\n\n\n\n\n\nExamples of Open Science Initiatives\n\n\n\n\n\n\n\n\nNational Institutes of Health (NIH): the USA encourages Open Science practices, including data sharing, through policies like the NIH Data Sharing Policy.\nWellcome Trust: mandates open access globally to research outputs funded by the foundation.\nEuropean Molecular Biology Organization (EMBO): supports Open Access and provides guidelines for data sharing.\nBill & Melinda Gates Foundation: advocates for Open Access and data sharing to maximize the impact of its research.\nEuropean Research Council (ERC): promotes Open Access to research publications and adheres to the FAIR principles for data management.\n\n\n\n\n\n\n\n\n\n\n\n\nBenefits of Open Science for Researchers\n\n\n\n\nIncreased Visibility and Impact: more people can access and engage with your findings.\nFacilitated Collaboration: leading to the development of innovative ideas and impactful projects.\nEnhanced Credibility: sharing data and methods openly allows for validation of research findings by others.\nAccelerated Research Progress:: by enabling researchers to build upon each other‚Äôs work and leverage shared data.\nStimulation of New Research: shared data can inspire novel research questions and discoveries.\nAttract Funding Opportunities: adhering to Open Science principles may make you eligible for additional funding opportunities.\nTransparency and Accountability: promoting responsible conduct in research.\nPLong-Term Data Preservation: by archiving research data in repositories.\n\n\n\n\n\nFAIR principles\nThe FAIR principles complementing Open Science, aim to improve research data management, sharing, and usability. FAIR stands for Findable, Accessible, Interoperable, and Reusable, enhancing the value, impact, and sustainability of research data. Adhering to FAIR principles benefits individual researchers and fosters collaboration, data-driven discoveries, knowledge advancement, and long-term preservation. However, achieving FAIR compliance is nuanced, with some aspects being more complex, especially concerning metadata standards and controlled vocabularies.\nWe strongly endorse these recommendations for those developing software or performing data analyses: https://fair-software.nl/endorse.\n\n\nBreaking down the FAIR Principles\n\n\n\nFindable\n\nResearch data should be easily identifiable, achieved through the following key components:\n\nAssign Persistent and Unique Identifiers: such as Digital Object Identifiers (DOIs) or Uniform Resource Identifiers (URIs).\nCreate Rich and Standard Metadata: describing the content, context, and characteristics of the data (such as origin, format, version, licensing, and the conditions for reuse).\nUse a Data Repository or Catalog: following FAIR principles to deposit your data, enhancing data discoverability and access from a centralized and reliable source.\n\nClear and comprehensive metadata facilitates data discovery by both researchers and machines through various search engines and data repositories.\n\n\nAccessible\n\nResearch data should be accessible with minimal restrictions on access and downloading, to facilitate collaboration, verification of findings, and ensuring transparency. Key elements to follow:\n\nOpen Access: Ensure data is freely accessible without unnecessary barriers. Choose suitable licenses for broad data reuse (such as MIT, and Apache-2.0).\nAuthentication and Authorization: Implement secure mechanisms for access control, especially for sensitive data\nMetadata: Deposit metadata even when data access is restricted, providing valuable information about the dataset (version control systems).\n\n\n\n\n\n\n\nImportant note: As open as possible, as closed as necessary\n\n\n\n\n\nEnsure data accessibility aligns with privacy regulations like GDPR, and when necessary, limit access to sensitive data. When dealing with sensitive data, share query details and data sources to maintain transparency.\n\n\n\n\n\n\nExample\n\n\n\n\n\n\n\nYou are working with sensitive data from a national authority regarding personal information. You won‚Äôt be able to publish or deposit the data but you can describe what was the query you used to obtain the data and its source (e.g., sample population: people between 20-30 years old, smoker).\n\n\n\n\n\n\n\n\n\n\nInteroperable\n\nInteroperability involves structuring and formatting data to seamlessly integrate with other datasets. Utilizing standard data formats, widely accepted vocabularies and ontologies enables integration and comparison across various sources. Key components to follow:\n\nStandard Data Formats: facilitating data exchange and interoperability with various software tools and platforms.\nVocabularies and Ontologies: commonly used by the scientific community ensuring data can be understood and combined with other datasets more effectively.\nLinked Data: to related data and resources, to enrich contextualization of datasets, facilitating integration and discovery of interconnected information.\n\n\n\nReusable\n\nData should be thoroughly documented and prepared, with detailed descriptions of data collection, processing, and methodology provided for replication by other researchers. Clear statements on licensing and ethical considerations are essential for enabling data reuse. Key components to follow:\n\nDocumentation and Provenance: Comprehensive documentation on data collection, processing, and analysis. Provenance information elucidates data origin and processing history.\nEthical and Legal Considerations: related to data collection and use. Additionally, adherence to legal requirements ensures responsible and ethical data reuse.\nData Licensing: Clearly stated licensing terms facilitate data reuse, specifying usage, modification, and redistribution while respecting intellectual property rights and legal constraints.\n\n\n\n\n\n\n\n\n\nTest your knowledge: use cases\n\n\n\nBefore moving on to the next lesson, take a moment to explore this practical example if you‚Äôre working with NGS data. Here, you‚Äôll find exercises and examples to reinforce the knowledge you‚Äôve acquired in this lesson.\n\n\n\nKey online links\n\nHow to FAIR DK\nFAIR principles\nFAIR software\nOpen AIRE\nDeiC - RDMElearn",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/01_RDM_intro.html#wrap-up",
    "href": "develop/01_RDM_intro.html#wrap-up",
    "title": "1. Introduction to RDM",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we‚Äôve covered the definition of RDM, the advantages of effective RDM practices the phases of the research data life cycle, and the FAIR principles and Open Science. While much of the guidelines are in the context of omics data, it‚Äôs worth noting its applicability to other fields and institutions. Nonetheless, we recommend exploring these guidelines further at your institution (links provided above).\nIn the next lessons, we will explore different resources, tools, and guidelines that can be applied to all kinds of data and how to apply them specifically to biological (with a focus on NGS) data.\n\nSources\n\nRDMkit: ELIXIR (2021) Research Data Management Kit. A deliverable from the EU-funded ELIXIR-CONVERGE project (grant agreement 871075).",
    "crumbs": [
      "Course material",
      "Basics",
      "1. Introduction to RDM"
    ]
  },
  {
    "objectID": "develop/02_DMP.html",
    "href": "develop/02_DMP.html",
    "title": "2. Data Management Plan",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nLearn what is a DMP\nLearn about the different DMP templates\nHow to write a DMP focused on NGS data\nThe process of data management involves implementing tailored best practices for your data but how do you ensure comprehensive coverage of the decisions and that data is well-managed throughout its life cycle. To achieve this, a Data Management Plan (DMP) is essential.\nA DMP serves as a comprehensive document detailing strategies for handling project data, code, and documentation across its life cycle. It includes plans for data collection, documentation, organization, and preservation.",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/02_DMP.html#wrap-up",
    "href": "develop/02_DMP.html#wrap-up",
    "title": "2. Data Management Plan",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we covered Data Management Plans (DMPs) and introduced tools to aid in their creation, along with a template for your projects. DMPs serve as valuable aids in project planning, data preservation, and facilitating future use by yourself, collaborators, or the wider scientific community. Next, we‚Äôll delve into organizing data efficiently and provide helpful tools for the task.\n\nSources\n\nNBISweden workshop on RDM practices\nStanford University Library Data Management Services website\nRDM Guide, Elixir Belgium",
    "crumbs": [
      "Course material",
      "Basics",
      "2. Data Management Plan"
    ]
  },
  {
    "objectID": "develop/examples/NGS_management.html",
    "href": "develop/examples/NGS_management.html",
    "title": "NGS data strategies",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nNext Generation Sequencing data types and metadata\nBest practices for software and code management\nPipelines and workflows\n\n\n\nIn the data life cycle for Next Generation Sequencing (NGS) technology data, processing and analyzing are critical phases that involves transforming raw sequencing data into meaningful biological insights. Researchers apply computational methods and bioinformatics tools to extract valuable information from the vast amount of sequencing data generated in NGS experiments. We‚Äôll first explore the primary data types generated pre- and post-processing and the importance of detailed documentation.We will then focus on good practices used when performing data analysis and software development.\n\n\n\n\n\n\nNext Generation Sequencing\n\n\n\n\n\n\n\nNext Generation Sequencing (NGS), or high-throughput sequencing, has revolutionized genomics research. It encompasses advanced techniques for rapid and cost-effective analysis of DNA or RNA molecules. Unlike traditional methods, NGS can analyze millions of DNA fragments simultaneously, enhancing the speed, efficiency, and scale of sequencing and becoming integral to modern genomics and biomedical studies.As NGS technologies continue to advance and become more accessible, they will remain at the front of cutting-edge genomics research, driving innovations that contribute to our understanding of complex genetic interactions and their implications for human health and biology.\nApplications\nIt is widely utilized in various applications, including genomic sequencing, transcriptome analysis (RNA-Seq), epigenetic profiling (ChIP-Seq), metagenomics, and targeted sequencing. In addition, it plays a crucial role in fields such as oncology, infectious disease research, and personalized medicine.\nData production\nNGS workflows involve key steps, from sample preparation to data analysis. Samples undergo extraction and fragmentation, followed by the addition of unique identifiers, known as library preparation, for multiplexed sequencing. Then,fragments are amplified and sequences in parallel sequencing using state-of-the-art NGS platforms. Subsequent data analysis processes reconstruct the original sequence and identify genetic variations, structural changes, or functional elements. The unique identifiers are specific adapter sequences that allow future identification of individual samples within a multiplexed sequencing run.\n\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\n\nDo you ensure that all the data you collect or generate is accompanied by metadata? Have you ever encountered missing information when reading a provided file?\nDo you utilize specific databases or repositories for storing and accessing your research data?\nWhat are the typical data formats you encounter during data processing? As outputs of your analysis, what are the common data formats you encounter for visualization or further analysis?\nDo you document and track the workflows you use for data processing and analysis, including the software employed? How do you ensure reproducibility?\n\n\n\n\n\n\n\n\n\n\nThoroughly document your datasets and the experimental setup to ensure reproducibility. Adhering to standards will ensure interoperability. Data types‚Äô examples:\n\nElectronic Laboratory Notebook (ELN): digital description of the experimental design, and measurement devices. ELNs offer features like data entry, text editing, file attachments, collaboration tools, and search capabilities.\nLaboratory protocols: methodologies to prepare and manage samples.\nSamples: refers to the biological material (extraction of DNA, RNA, or proteins). Specification of sample identifier, sample type, source organism etc.\nSequencing: details on the platform (e.g., Illumina, Oxford Nanopore), library preparation method, coverage, quality control metrics (e.g., Phred score)‚Ä¶\nRaw sequencing data: sequences and quality scores (e.g., FASTQ files)\n\n\n\n\n\n\n\nNote\n\n\n\nA metadata file is crucial during data analysis as it contains information about the experimental conditions (such as sequencing details, treatment, sample type, time points, tissue‚Ä¶).\n\n\n\n\n\nExamples of data types generated during processing:\n\nQuality control metrics: to filter out potential artifacts and ensure the reliability of downstream analyses (e.g., bioinformatics tool like FastQC or MultiQC for results‚Äô aggregation)\nData alignments: in genomics to determine the location of the read in the genome and in transcriptomics to identify gene expression levels.\nDNA analyses results: such as variant calling, genome annotation, functional genomics, phylogenetics, metagenomics etc. Results are usually presented in tabular format.\nRNA Expression Analyses results: from differential gene expression, gene ontology (GO) enrichment, alternative splicing, pathway analysis etc. Results are usually presented in tabular format.\nEpigenetic profiling outputs: to assess gene regulation and chromatin structure (e.g., ChIP-Seq). Usually presented in BED format.\n\nThe interpretation of NGS data relies heavily on the results of data analysis, which are pivotal for understanding the biological significance of the findings and formulating hypotheses for further exploration. Clear and effective visualization methods are crucial for communicating and interpreting the vast amount of information generated by NGS experiments.\n\n\n\n\n\n\nOther types of data: databases and visualizations\n\n\n\n\n\n\n\n\nKnowledge databases\nA knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\nVisualizations\n\n\nHeatmaps: frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across samples/conditions.\nVolcano Plots: commonly used in differential gene expression analysis\nGenome Browser Snapshots: display alignments and genomic features in genomic regions (e.g., gene annotations, ChIP-Seq peaks)\nNetwork Visualizations:utilized to explore gene regulatory networks or protein-protein interaction\nGenomic Annotations: to annotate genetic variations (functional impact on genes, genomic regions, or regulatory element)\n\n\n\n\n\n\n\n\n\nBest practices for software and code management (don‚Äôt forget to read about FAIR software):\n\nCommenting your code: to enhance readability and comprehension\nMake your source code accessible using repository (GitHub, GitLab, Bitbucket, SourceForge, etc.) that provides version control (VC) solutions. This step is one of the most important ones as version control systems (Git or SVN) track changes in your code over time and enable collaboration and easy version management. Most danish institutions provide courses on Git/GitHub, check yours! We also highly recommend reading this paper (Perez-Riverol et al. 2016).\nREADME file: with the comprehensive information about the project and include installation instructions, usage examples or tutorials, licensing details, citation information, etc.\nRegister your code in a research software registry and include a clear and accessible software usage license: enabling other researcher to discover and reuse software packages (alongside a metadata). More recommendations here.\nUse domain-relevant community standards to ensure consistency and interoperability (e.g., CodeMeta).\n\n\n\n\n\n\n\nGit and Github courses and other resources\n\n\n\n\n\n\n\n\nUniversity of Copenhagen\nAarhus University\nAalborg University\nDTU Git guidelines Find more resources in Berkeley Library website\n\n\n\n\n\n\n\n\n\nYou might use standard workflows or generate new ones during data processing and data analyses steps.\n\nCode notebooks: tools for data documentation (e.g.¬†Jupyter Notebook, Rmarkdown) enabling the combination of code with descriptive text and visualizations.\n\nIntegrated development environments (knitr or MLflow).\nPipeline frameworks or workflow management systems: designed to streamline and automate various steps involved in data analysis (data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages. There are two very popular systems, Nextflow and Snakemake.\n\nA great example of community curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.\n\n\n\n\n\n\nCourse on pipelines and workflows\n\n\n\nTake our course on Reproducible Research Practices LINK\n\n\nClick below to access a list of the most common file formats used when working with NGS data.\n\n\nData types summary\n\n\nSelect appropriate file formats that balance data accessibility, storage efficiency, and compatibility with downstream analysis tools. Standardized file formats facilitates data sharing and collaboration among researchers in the scientific community.\n\nBAM/SAM: stores the alignment information (binary and text-based respectively)\nFASTA: store nucleotide or amino acid sequence, commonly used for reference sequences or assembled contigs.\nGene Transfer Format (GTF) and General Feature Format (GFF): annotates genomic features such as genes, exons, and transcripts.\nAlignment indexes: data structures for efficient and rapid mapping of sequencing reads to a reference.\nVariant Call Format (VCF): stores genetic variation such as single nucleotide variants (SNVs), insertions, deletions, and structural variants (and their position, quality score etc.)\nCount matrix: quantifies the abundance of RNA transcripts or genomic features across samples\nBED/BEDGraph: represent genomic intervals or coverage information (e.g., peak calling identifies regions of enriched signal intensity)\nWIG/BigWig: store genome-wide data\n\nGeneral formats\n\nTabular formats: File formats like CSV, TSV, and XLSX used to store data in rows and columns for easy data analysis and sharing\nImage formats: File formats such as PNG and SVG used to store graphical visualizations, making them easily viewable and shareable\nBinary formats: File formats like NPZ and H5 used to store large datasets, ensuring efficient data access and storage\nJSON: A lightweight data-interchange format for storing hierarchical data structures, commonly used in bioinformatics tools\nHTML: A format used to create interactive reports that include both visualizations and textual descriptions of analysis results\nCode notebooks: Interactive documents combining code, visualizations, and explanatory text, aiding in data analysis reproducibility and documentation\nScripts: Text files containing sets of commands or code instructions for automating data processing and analysis tasks\n\nExplore more data types at the UCSC webpage. Check out this tutorial for more detailed explanations.\n\n\n\n\n\n\nIn this lesson we have taken a look a the vast and diverse landscape of bioinformatics data.",
    "crumbs": [
      "Use cases",
      "NGS data",
      "NGS data strategies"
    ]
  },
  {
    "objectID": "develop/examples/NGS_management.html#practical-tips-for-computational-research",
    "href": "develop/examples/NGS_management.html#practical-tips-for-computational-research",
    "title": "NGS data strategies",
    "section": "",
    "text": "Thoroughly document your datasets and the experimental setup to ensure reproducibility. Adhering to standards will ensure interoperability. Data types‚Äô examples:\n\nElectronic Laboratory Notebook (ELN): digital description of the experimental design, and measurement devices. ELNs offer features like data entry, text editing, file attachments, collaboration tools, and search capabilities.\nLaboratory protocols: methodologies to prepare and manage samples.\nSamples: refers to the biological material (extraction of DNA, RNA, or proteins). Specification of sample identifier, sample type, source organism etc.\nSequencing: details on the platform (e.g., Illumina, Oxford Nanopore), library preparation method, coverage, quality control metrics (e.g., Phred score)‚Ä¶\nRaw sequencing data: sequences and quality scores (e.g., FASTQ files)\n\n\n\n\n\n\n\nNote\n\n\n\nA metadata file is crucial during data analysis as it contains information about the experimental conditions (such as sequencing details, treatment, sample type, time points, tissue‚Ä¶).\n\n\n\n\n\nExamples of data types generated during processing:\n\nQuality control metrics: to filter out potential artifacts and ensure the reliability of downstream analyses (e.g., bioinformatics tool like FastQC or MultiQC for results‚Äô aggregation)\nData alignments: in genomics to determine the location of the read in the genome and in transcriptomics to identify gene expression levels.\nDNA analyses results: such as variant calling, genome annotation, functional genomics, phylogenetics, metagenomics etc. Results are usually presented in tabular format.\nRNA Expression Analyses results: from differential gene expression, gene ontology (GO) enrichment, alternative splicing, pathway analysis etc. Results are usually presented in tabular format.\nEpigenetic profiling outputs: to assess gene regulation and chromatin structure (e.g., ChIP-Seq). Usually presented in BED format.\n\nThe interpretation of NGS data relies heavily on the results of data analysis, which are pivotal for understanding the biological significance of the findings and formulating hypotheses for further exploration. Clear and effective visualization methods are crucial for communicating and interpreting the vast amount of information generated by NGS experiments.\n\n\n\n\n\n\nOther types of data: databases and visualizations\n\n\n\n\n\n\n\n\nKnowledge databases\nA knowledge database is a structured repository of biological information that categorizes and annotates genes, proteins, and their functions, facilitating comprehensive understanding and analysis of biological systems. Here are five examples of knowledge databases:\n\n\nGene Ontology (GO): A comprehensive resource that classifies gene functions into defined terms, allowing for standardized annotation and comparison of genes across different organisms.\nDisease Ontology: A database that provides structured, standardized terminology for various diseases and their relationships, aiding in the systematic analysis of disease-related data.\nKEGG Pathways: A collection of manually curated pathway maps representing molecular interactions and reaction networks within cells, enabling the interpretation of high-throughput data in the context of biological systems.\nReactome: An open-access database that offers curated descriptions of biological processes, including pathways, reactions, and molecular events, facilitating the interpretation of large-scale biological data.\nUniProt: An extensive protein knowledgebase that provides detailed information about proteins, including their sequences, functions, and related annotations, supporting a wide range of biological research endeavors.\n\n\nVisualizations\n\n\nHeatmaps: frequently used to visualize gene expression patterns, epigenetic modifications, or microbial abundances across samples/conditions.\nVolcano Plots: commonly used in differential gene expression analysis\nGenome Browser Snapshots: display alignments and genomic features in genomic regions (e.g., gene annotations, ChIP-Seq peaks)\nNetwork Visualizations:utilized to explore gene regulatory networks or protein-protein interaction\nGenomic Annotations: to annotate genetic variations (functional impact on genes, genomic regions, or regulatory element)\n\n\n\n\n\n\n\n\n\nBest practices for software and code management (don‚Äôt forget to read about FAIR software):\n\nCommenting your code: to enhance readability and comprehension\nMake your source code accessible using repository (GitHub, GitLab, Bitbucket, SourceForge, etc.) that provides version control (VC) solutions. This step is one of the most important ones as version control systems (Git or SVN) track changes in your code over time and enable collaboration and easy version management. Most danish institutions provide courses on Git/GitHub, check yours! We also highly recommend reading this paper (Perez-Riverol et al. 2016).\nREADME file: with the comprehensive information about the project and include installation instructions, usage examples or tutorials, licensing details, citation information, etc.\nRegister your code in a research software registry and include a clear and accessible software usage license: enabling other researcher to discover and reuse software packages (alongside a metadata). More recommendations here.\nUse domain-relevant community standards to ensure consistency and interoperability (e.g., CodeMeta).\n\n\n\n\n\n\n\nGit and Github courses and other resources\n\n\n\n\n\n\n\n\nUniversity of Copenhagen\nAarhus University\nAalborg University\nDTU Git guidelines Find more resources in Berkeley Library website\n\n\n\n\n\n\n\n\n\nYou might use standard workflows or generate new ones during data processing and data analyses steps.\n\nCode notebooks: tools for data documentation (e.g.¬†Jupyter Notebook, Rmarkdown) enabling the combination of code with descriptive text and visualizations.\n\nIntegrated development environments (knitr or MLflow).\nPipeline frameworks or workflow management systems: designed to streamline and automate various steps involved in data analysis (data extraction, transformation, validation, visualization, and more). Additionally, they contribute to ensuring interoperability by facilitating seamless integration and interaction between different components or stages. There are two very popular systems, Nextflow and Snakemake.\n\nA great example of community curated workflows is the nf-core community. Nf-core is a collaborative and open-source initiative comprising bioinformaticians and researchers dedicated to developing and maintaining a collection of curated and reproducible Nextflow-based pipelines for NGS data analysis, ensuring standardized and efficient data processing workflows.\n\n\n\n\n\n\nCourse on pipelines and workflows\n\n\n\nTake our course on Reproducible Research Practices LINK\n\n\nClick below to access a list of the most common file formats used when working with NGS data.\n\n\nData types summary\n\n\nSelect appropriate file formats that balance data accessibility, storage efficiency, and compatibility with downstream analysis tools. Standardized file formats facilitates data sharing and collaboration among researchers in the scientific community.\n\nBAM/SAM: stores the alignment information (binary and text-based respectively)\nFASTA: store nucleotide or amino acid sequence, commonly used for reference sequences or assembled contigs.\nGene Transfer Format (GTF) and General Feature Format (GFF): annotates genomic features such as genes, exons, and transcripts.\nAlignment indexes: data structures for efficient and rapid mapping of sequencing reads to a reference.\nVariant Call Format (VCF): stores genetic variation such as single nucleotide variants (SNVs), insertions, deletions, and structural variants (and their position, quality score etc.)\nCount matrix: quantifies the abundance of RNA transcripts or genomic features across samples\nBED/BEDGraph: represent genomic intervals or coverage information (e.g., peak calling identifies regions of enriched signal intensity)\nWIG/BigWig: store genome-wide data\n\nGeneral formats\n\nTabular formats: File formats like CSV, TSV, and XLSX used to store data in rows and columns for easy data analysis and sharing\nImage formats: File formats such as PNG and SVG used to store graphical visualizations, making them easily viewable and shareable\nBinary formats: File formats like NPZ and H5 used to store large datasets, ensuring efficient data access and storage\nJSON: A lightweight data-interchange format for storing hierarchical data structures, commonly used in bioinformatics tools\nHTML: A format used to create interactive reports that include both visualizations and textual descriptions of analysis results\nCode notebooks: Interactive documents combining code, visualizations, and explanatory text, aiding in data analysis reproducibility and documentation\nScripts: Text files containing sets of commands or code instructions for automating data processing and analysis tasks\n\nExplore more data types at the UCSC webpage. Check out this tutorial for more detailed explanations.",
    "crumbs": [
      "Use cases",
      "NGS data",
      "NGS data strategies"
    ]
  },
  {
    "objectID": "develop/examples/NGS_management.html#wrap-up",
    "href": "develop/examples/NGS_management.html#wrap-up",
    "title": "NGS data strategies",
    "section": "",
    "text": "In this lesson we have taken a look a the vast and diverse landscape of bioinformatics data.",
    "crumbs": [
      "Use cases",
      "NGS data",
      "NGS data strategies"
    ]
  },
  {
    "objectID": "develop/examples/NGS_OS_FAIR.html",
    "href": "develop/examples/NGS_OS_FAIR.html",
    "title": "Applied Open Science and FAIR principles to NGS",
    "section": "",
    "text": "Section Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n1. Apply Open Science and FAIR principles to your data\n\n\nYou should consider revisiting these exercises and examples after completing lesson 1 in the course material.\n\n\n\n\n\n\nExercise 1: FAIR principles in Research practice\n\n\n\n\n\n\n\nThink about your current research project or a hypothetical one. How could you apply Open Science and FAIR principles to improve the transparency, accessibility, and reusability of your research data and/or data analyses? Consider aspects such as data sharing, documentation, metadata standards, and licensing. Write down three specific actions you could take to implement Open Science and FAIR principles into your research workflow.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nExperiments / input / raw data\n\nStandardized file formats and data structure\nStandardized metadata of raw files and results (describing origin, format, etc.)\nDepositing raw data in public repositories or data archives\n\nPipelines / workflows\n\nUse data repositories and document your workflows (software tools, parameters/settings, versions‚Ä¶)\nDocumenting all preprocessing scripts and employed pipelines\nDevelopment environments\nUsing version control systems (git, GitHub, GitLab)\n\nSoftware / code\n\nPipeline scripts and software code openly accessible (commented code)\nUse data repositories (and version control)\nProvide a README file (installation and usage instruction, license and citation information etc.)\nOpen-source licensing\nFollow these recommendations: FAIR software\n\n\n\n\n\n\n\n\n\n\n\nNote that not all data needs to be archived and deposited. NGS data processing generates vast amounts of data that might not need to be share publicly, as long as you describe how you produced the data. For example, in an usual bulk RNAseq experiment, FASTQ reads are cleaned and subsequently aligned to a reference genome, creating a subset of cleaned FASTQ files and BAM files. After transcript/gene quantification, you can obtain a final count matrix that can be used for data analysis, such as Differential Expression Analysis. If you provide enough documentation on how these files were processed (which softwares, which versions and which options), you won‚Äôt need to deposit neither the cleaned nor aligned reads, only the original FASTQ files and the final result of your preprocessing. This will save quite the computational resources and metadata needed to preserve the intermediary data. Providing documentation on how the data was generated is much simpler if you are using community curated pipelines such as the ones created by the nf-core community.\n\nOpen Science\nUnless your data is of sensitive nature (human individual samples, patient data, or anything protected), you should always deposit your data with as little restrictions as possible. This includes publishing your manuscript in Open Access journals as well! For your generated NGS data, our suggestion is that you use a license such as Creative Commons licence like CC-BY license, which only requires users to attribute the source of the data, but this also depends on the repository that you use for your data. We will see more about it in the lesson 10\n\n\n\nCC licenses\n\n\n\n\nFAIR principles\nNext we will see how we can apply each of the FAIR principles to your NGS data.\n\nFindable\nTo make your NGS data easy to find, you should deposit it in a domain specific repository, such as the Gene Expression Omnibus or Annotare. We will see more about them in lesson 10. Both of these repositories will help you give your data a unique identifier, and provide information (metadata) on how the data was generated. The metadata you include in your submission should contain, at least, the minimum necessary information to understand what kind of data it is submitted, and how it was generated. This includes:\n\nSample metadata in tabular format, containing information about the samples used in the experiment as well as variables of interest for the analysis.\nExperiment metadata, including data provenance, that is, how the samples were obtain, from which organism, following what protocols, kits, sequencing libraries, sequencing method and data preprocessing workflows/pipelines. This is usually submitted as part of a submission form and it depends on the repository.\nKeywords, such as type of NGS data, conditions or diseases studied in the experiment, organisms used, genes studied, etc.\n\n\n\nAccessible\nBoth GEO and Annotare repositories promote the use of unrestricted access to the data. In the case of Annotare, deposited data is under CC0 license, while GEO states deposited data is public domain. Depositing your data will require you to have an account but it does not require authentication from the user to access and download the data.\n\n\n\nAnnotare main page\n\n\n\n\n\n\n\n\nOn sensitive data\n\n\n\nIf you would like to deposit sensitive data that needs controlled access, it is possible to do so through the European Genome-phenome Archive (EGA).\n\n\nIn addition, if you have deposited your data with rich metadata, as explained in the previous step, it will be easier for users to query your data by date, author, organism, type of NGS data, etc etc.\n\n\nInteroperable\nBy using standard bioinformatics formats, such as fastq files for raw NGS data, count matrices in tabular format, BED files for peak calling results, etc., you are already complying to this section. In addition, GEO and Annotare repositories are complaint to NGS data standards, such as MIAME/MINSEQE/MINSCE guidelines.\n\n\n\nMINSEQ\n\n\nNonetheless, this is the easy part! Adhering to controlled vocabularies seems to be the most difficult part of the FAIR principles. Here are some cases:\n\nUsing organism names instead of their taxonomy. For example: mouse instead of Mus musculus, or human, instead of Homo sapiens. Even better, we should use a taxonomy ID, such as the NCBI taxonomy ID for human NCBITaxon_9606, which will unequivocally refer to human.\nUsing gene names or symbols instead of gene IDs. For example: the gene POU5F1 has many synonyms, like OCT4, OCT, OTF4. In order to be explicit, it is better to reference the gene ID, like an ENSEMBL gene ID ENSG00000204531.\nUsing disease names instead of disease IDs. Again, this will reference specifically the disease you mention.\n\nThere are many more stances where you can use controlled vocabularies for other variables of interest, like cell type, tissue, cell cycle, etc. We will see in the metadata lesson where you can find controlled vocabularies for different variables of interest in NGS data.\n\n\nReusable\nIn order for your NGS data to be reusable, you will have to provide a thorough documentation on how it was generated, as well as the terms (that is a license) on how the data can be used/retrieved. We have talked already about collecting metadata on how the samples were generated (laboratory protocols, sequencing library, kits, technology, etc) and processed (workflows or pipelines along with the software used, which versions adn options). We also talked on what type of standard file formats you should use, such as fastq files for raw data and tabular formats for sample metadata. Finally, we have discussed in the Open Science section that you should try to license your data as freely as possible, like a CC0 license or CC-BY license. If your data is of sensitive nature and has restricted access or conditions, you should instead provide information on how other people can access the data, as well as any agreements or ethical approvals necessary for its reuse.\n\n\n\ncc-by license\n\n\n\n\n\nSources\n\nElixir Belgium: https://rdm.elixir-belgium.org/omics_data\n\n\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "NGS data",
      "Applied Open Science and FAIR principles to NGS"
    ]
  },
  {
    "objectID": "develop/04_metadata.html",
    "href": "develop/04_metadata.html",
    "title": "4. Metadata for NGS data",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nThe role of metadata in effective management\nBest practices to create metadata and README files\nSources for controlled vocabularies\n(Optional) Create a database and a catalogue browser\nIn bioinformatics data management, documentation plays a critical role in ensuring clarity and reproducibility. Documentation and metadata are essential components in ensuring your data adheres to FAIR principles.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#documentation-and-metadata",
    "href": "develop/04_metadata.html#documentation-and-metadata",
    "title": "4. Metadata for NGS data",
    "section": "Documentation and metadata",
    "text": "Documentation and metadata\nEssential documentation comes in different forms and flavors, serving various purposes in research. Examples include protocols outlining experimental procedures, detailed lab journals recording experimental conditions and observations, codebooks explaining concepts, variables, and abbreviations used in the analysis, information about the structure and content of a dataset, software installation and usage manual, code explanation within files or methodological information outlining data processing steps.\n From ontotext.com\nMetadata provides essential context and structure to (primary) data, enabling researchers to understand its significance and facilitate efficient data management. Some common elements found in metadata for bioinformatics data include:\n\nSample information and collection details\nExperimental conditions\nData processing steps applied to the raw data\nAnnotation and Ontology terms\nFile metadata (file type, file format, etc.)\nEthical and Legal compliance\n\nMetadata serves as a crucial guide in navigating the complex landscape of data, akin to a cheat sheet for piecing together the puzzle of information. Much like identifying puzzle pieces, metadata provides essential details about data origin, structure, and context, such as sample collection details, experimental procedures, and equipment used. Metadata enables data exploration, interpretation, and future accessibility, promoting effective management and facilitating data usability and reuse.\n\n\n\n\n\n\nBenefits of collecting proper metadata\n\n\n\n\nData Context and Interpretation: Aiding in understanding experimental conditions, sample origins, and processing methods, crucial for accurate results interpretation.\nData Discovery and Access: Metadata enables easy locating and accessing of specific datasets by quickly identifying relevant data through sample identifiers, experimental parameters, and timestamps.\nReproducibility and Collaboration: Metadata facilitates experiment replication and validation by enabling colleagues to reproduce analyses, compare results, and collaborate effectively, enhancing the integrity of scientific findings.\nQuality Control and Validation: Metadata supports data quality assessment by tracking the origin and handling of NGS data, allowing identification of errors or biases to validate analysis accuracy and reliability.\nLong-Term Data Preservation: metadata ensures preservation over time, facilitating future understanding and utilization of archived datasets for continued scientific impact as research progresses.\n\n\n\n\nStreamlining Metadata Collection\nData and project directories should both include a metadata and a README file.\n\n\n\n\n\n\nPractical tips\n\n\n\n\nImplement a logical structure with clear and descriptive file names.\nUse of controlled vocabularies and ontologies to ensure consistency and efficient data management and interpretation.\nUse a repository and a versioning system\nMake it Machine-readable, -actionable and/or -interpretable.\nDevelop standards further within your research environment FAIRsharing standards.\nInclude all information for others to comprehend and effectively utilize the data.\n\n\n\n\n\nREADME.md\nThe README.md file, written in markdown format, provides a detailed description of the folder‚Äôs content. It includes information such as the purpose of the data, collection methods, and relevant details. The content might differ based on the purpose of the data.\n\n\n\n\n\n\nExercise 1: Identify README.md key components.\n\n\n\n\n\n\n\nSelect one of the examples below and reflect on how effectively the README communicates important information about the project. Please note that some of the links lead to README files describing databases, while others pertain to software and tools.\n\n1000 Genomes Project. You will find several readme files here.\n\nHomo Sapiens, fasta GRCh38\nIPD-IMGT/HLA Database\nDocker\nPython pandas\n\n\n\n\n\n\nStructure for bioinformatics projects.\n\nDescription of the project\nObjectives and aims\nDatasets and software requirements\nInstruction for data interpretation\nSummary of results\nContributions\nAdditional comments or notes\n\n\n\nmetadata.yml\nMetadata can be written in many file formats (commonly used: YAML, TXT, JSON and CSV). We recommend YAML format, which is a text document that contains data formatted using a human-readable data format for data serialization. The content will be specific to the type of project.\nmetadata:\n  project: \"Title\"\n  author: \"Name\"\n  date: \"YYYYMMDD\"\n  description: \"Project short description\"\n  version: \"1.0\"\n  analysis:\n    tool: \"software\"\n    version: \"1.1.1\"\nSome general metadata fields used across different disciplines:\n\nProject Title:A concise and informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. Include ORCID for identification.\nDate Created: The date when the dataset was originally generated or compiled, in YYYY-MM-DD format.\nDate Modified: The date when the dataset was last updated or modified (YYYY-MM-DD).\nObject ID: The project or assay ID for tracking and reference purposes.\nDescription: A short narrative explaining the content, purpose, and context of the project.\nKeywords: Descriptive terms or phrases capturing the main topics and attributes.\nEthical and Legal Considerations: Information about ethical approvals, consent, and any legal restrictions.\nVersion: The version number or identifier, useful for tracking changes.\nRelated Publications: Links or references to scientific publications associated with the folder. Always add the DOI.\nFunding Source: Details about the funding agency or source that supported the research or data generation.\nLicense: The type of license or terms of use associated with the dataset/project.\nContact Information: Contact details for individuals who can provide further information about the dataset/project.\n\n\n\n\n\n\n\nTip\n\n\n\nThere is an exercise in the practical material to streamline the creation of metadata files using Cookiecutter, a template-based scaffolding tool.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nCreate a metadata file with the following description fields: name, date, description, version, authors, keywords, license. Fill it up at the start of the project, when you generate the file structure.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#controlled-vocabularies-and-ontologies",
    "href": "develop/04_metadata.html#controlled-vocabularies-and-ontologies",
    "title": "4. Metadata for NGS data",
    "section": "Controlled vocabularies and ontologies",
    "text": "Controlled vocabularies and ontologies\nResearchers encountering inconsistent and non-standardized terms (e.g., gene names, disease names, cell types, protein domains, etc.) across datasets may face challenges in data integration. Thus, requiring additional curation time to enable meaningful comparisons. Standardized vocabularies streamline integration, improving consistency and comparability in analysis. Leveraging widely accepted ontologies in the documentation ensures consistent capture of experiment details in metadata fields, aiding data interpretation.\n\n\n\n\n\n\nExamples of ontology services\n\n\n\n\nUberon anatomy ontology\nGene ontology\nEnsembl gene IDs.\nMedical Subject Headings (MeSH)\nChemical Entities of Biological Interest\nMicroarray Gene Expression Society Ontology (MGED)\n\n\n\n\n\n\n\n\n\nOntology definition\n\n\n\n\n\n\n\nAn ontology is a structured framework representing concepts, attributes, and relationships within a specific domain, aiding knowledge organization and integration. Employing standardized vocabularies, it facilitates effective communication and reasoning between humans and computers. Ontologies are crucial for knowledge representation, data integration, and semantic interoperability, enhancing understanding and collaboration across complex domains.\n\n\n\n\n\nStandardization improves data discoverability and interoperability, enabling robust analysis, accelerating knowledge sharing, and facilitating cross-study comparisons. Ontologies act as universal translators, fostering harmonious data interpretation and collaboration across scientific disciplines.\nYou can find three examples of metadata tailored for different purposes NGS data examples: sample metadata, project metadata, and experimental metadata. We suggest exploring controlled vocabularies and metadata standards within your field and seeking additional specialized sources. You will find a few sources at the end of the page.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#database-and-data-catalogs",
    "href": "develop/04_metadata.html#database-and-data-catalogs",
    "title": "4. Metadata for NGS data",
    "section": "Database and data catalogs",
    "text": "Database and data catalogs\nMetadata can be used to create data catalogs, particularly beneficial for efficient organization of experimental or sequencing data generated by researchers. While databases can range from simple tabular formats like Excel to sophisticated DataBase Management Systems (DBMS) like SQLite, the choice depends on factors such as complexity and volume of data. Leveraging a DBMS offers advantages like efficient data storage, enhanced security, and rapid data querying capabilities.\n\nTables as databases\nA browsable table can be created by recursively navigating through a project‚Äôs folder hierarchy using a script and generating a TSV file (tab-separated values) named, for example, database_YYYYMMDD.tsv. This table acts as a centralized repository for all project data, simplifying access and organization. Consistency in metadata structure across projects is vital for efficient data management and integration, as it aids in tracking all conducted assays. Adhering to a uniform metadata format enables the seamless inclusion of essential information from YAML files into the browsable table.\n\n\n\n\n\n\nExercise 2: Generate database tables from metadata\n\n\n\n\n\n\n\nWrite a script (R or python) that recursively fetch metadata.yml files in a given path. It is important that each subdirectory contains its corresponding metadata.yml.\nRequirements:\n\nData folder structure: containing all project folders\nYAML metadata files associated with each project\n\nClick on the hint to reveal the solution and a code example for the exercise, which may serve as inspiration.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nquiet &lt;- function(x) { suppressMessages(suppressWarnings(x)) }\nquiet(library(yaml))\nquiet(library(dplyr))\nquiet(library(lubridate))\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n  file_list &lt;- list.files(path = folder_path, \n    pattern = \"metadata\\\\.yml$\", \n    recursive = TRUE, full.names = TRUE)\n  metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n  return(metadata_list)\n}\n\n# Specify the folder path\nfolder_path &lt;- \"/path/to/your/folder\"\n\n# Fetch metadata from the specified folder\nmetadata &lt;- get_metadata(folder_path)\n\n# Convert metadata to a data frame\nmetadata_df &lt;- data.frame(matrix(unlist(metadata), \nncol = length(metadata), byrow = TRUE))\ncolnames(metadata_df) &lt;- names(metadata[[1]])\n\n# Save the data frame as a TSV file\noutput_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\nwrite.table(metadata_df, \n  file = output_file, \n  sep = \"\\t\", \n  quote = FALSE, \n  row.names = FALSE)\n\n# Print confirmation message\nprint(\"Database saved as\", output_file, \"\\n\")\n\n\n\n\n\n\n\n\n\n\n\n\nSQLite database\nAn alternative to the tabular format is SQLite, a lightweight and self-contained relational database management system known for its simplicity and efficient. SQLite operates without the need for a separate server, making it ideal for scenarios requiring minimal resource usage. It excels in tasks involving structured data storage and retrieval, making it suitable for managing experiment metadata. Similar to the previous example, you can use a script that records all the information from the YAML file in a SQLite database.\n\n\n\n\n\n\nAdvantages of using SQLite database\n\n\n\n\nEfficient Querying: SQLite databases optimize querying and data retrieval, enabling fast and efficient extraction of specific information.\nStructured Organization: Databases provide structured and organized data storage, ensuring easy access and maintenance.\nData Integrity: SQLite databases enforce data integrity through constraints and validations, minimizing errors and inconsistencies.\nConcurrency and Multi-User Support: SQLite supports concurrent read access from multiple users, ensuring accessibility without compromising data integrity.\nScalability: It can handle growing volumes of data without significant performance degradation.\nModularity and Portability: Databases are self-contained and modular, simplifying data distribution and portability.\nSecurity and Access Control: SQLite offer security features like password protection and encryption, with granular control over user access.\nIndexing: Support for indexing accelerates data retrieval based on specific columns, particularly beneficial for large datasets.\nData Relationships: Databases allow for the establishment of relationships between tables, facilitating storage of interconnected data, such as project, assay, and sample information.\n\n\n\n\n\n\n\n\n\nExercise 3: Generate a SQLite database from metadata\n\n\n\n\n\n\n\nClick on the hint to reveal the solution and a code example for the exercise, which may serve as inspiration.\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nquiet &lt;- function(x) { suppressMessages(suppressWarnings(x)) }\nquiet(library(yaml))\nquiet(library(dplyr))\nquiet(library(lubridate))\nquiet(library(DBI))\n\n# Generate the metadata_df using the script from the example above (recursively fetching metadata.yml files)\n\n# Create an SQLite database and insert data\ndb_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".sqlite\")\ncon &lt;- dbConnect(SQLite(), db_file)\n\ndbWriteTable(con, \"metadata\", metadata_df, row.names = FALSE)\n\n# Print confirmation message\ncat(\"Database saved as\", db_file, \"\\n\")\n\n# Close the database connection\ndbDisconnect(con)\n\n\n\n\n\n\n\n\n\n\n\n\nCatalog browser\nYou can design a user-friendly catalog browser for your database using tools like Rshiny or Panel. These frameworks provide interfaces for dynamic search, filtering, and visualization, facilitating efficient exploration of database contents. Creating such a tool with Rshiny from both a tsv file and a SQLite database will be demonstrated below.\nHere‚Äôs an example of a SQLite database catalog created by the Brickman Lab at the Center for Stem Cell Medicine. It‚Äôs simple yet effective! Clicking on a data row opens the metadata.yml file, allowing access to detailed metadata for that assay.\n\n\nVideo\ntype:video\n\n\n\n\n\n\n\n\nExercise 4: Create your first catalog browser using Rshiny\n\n\n\n\n\n\n\nClick on the hint to reveal the solution and a code example for the exercise, which may serve as inspiration.\n\nSolution A. From a TSV\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nR script\n\nquiet &lt;- function(x) { suppressMessages(suppressWarnings(x)) }\nquiet(library(shiny))\nquiet(library(DT))\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"TSV File Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"file\", \"Choose a TSV file\", accept = c(\".tsv\"))\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output) {\n  \n  data &lt;- reactive({\n    req(input$file)\n    read.delim(input$file$datapath, sep = \"\\t\")\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\n\n\n\nSolution B. From a SQLite database\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nR script\nquiet &lt;- function(x) { suppressMessages(suppressWarnings(x)) }\nquiet(library(shiny))\nquiet(library(DT))\nquiet(library(DBI))\n\n# UI\nui &lt;- fluidPage(\n  titlePanel(\"SQLite Database Viewer\"),\n  \n  sidebarLayout(\n    sidebarPanel(\n      fileInput(\"db_file\", \"Choose an SQLite Database\", accept = c(\".sqlite\")),\n      textInput(\"table_name\", \"Enter Table Name:\", value = \"\"),\n      actionButton(\"load_button\", \"Load Table\")\n    ),\n    \n    mainPanel(\n      DTOutput(\"table\")\n    )\n  )\n)\n\n# Server\nserver &lt;- function(input, output, session) {\n  \n  con &lt;- reactive({\n    if (!is.null(input$db_file)) {\n      dbConnect(SQLite(), input$db_file$datapath)\n    }\n  })\n  \n  data &lt;- reactive({\n    req(input$load_button &gt; 0, input$table_name, con())\n    query &lt;- glue::glue_sql(\"SELECT * FROM {dbQuoteIdentifier(con(), input$table_name)}\")\n    dbGetQuery(con(), query)\n  })\n  \n  output$table &lt;- renderDT({\n    datatable(data())\n  })\n  \n  observeEvent(input$load_button, {\n    output$table &lt;- renderDT({\n      datatable(data())\n    })\n  })\n  \n  # Disconnect from the database when app closes\n  observe({\n    on.exit(dbDisconnect(con()), add = TRUE)\n  })\n}\n\n# Run the app\nshinyApp(ui, server)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 5: Add complex features to your catalog browser\n\n\n\n\n\n\n\nOnce you‚Äôve finished the previous exercise, consider implementing these additional ideas to maximize the utility of your catalog browser.\n\nAdd a tab to create a project directory interactively (and filling up the metadata fields)\nModify existing entries\nVisualize results using Cirrocumulus",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/04_metadata.html#wrap-up",
    "href": "develop/04_metadata.html#wrap-up",
    "title": "4. Metadata for NGS data",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we‚Äôve covered the importance of attaching metadata to your data for future reusability and comprehension. We briefly introduced various controlled vocabularies and provided several sources for inspirations. Implementing ontologies is optional, as their usage complexity varies.\nOptionally, if you‚Äôve gone through the lesson, you‚Äôve learned how to use the metadata YAML files to create a database and a catalog browser using Shiny apps. This makes it easy to manage all assays together.\n\nSources\n\nRMDKit: https://rdmkit.elixir-europe.org/data_brokering#collecting-and-processing-the-metadata-and-data\nFAIRsharing.org: provide a searchable database of metadata standards for a wide variety of disciplines\n\nOther sources: - Johns Hopkins Sheridan libraries, RDM. They provide a list of medical metadata standards resources.\n- KU Leuven Guidance: https://www.kuleuven.be/rdm/en/guidance/documentation-metadata - Transcriptomics metadata standards and fields - Bionty: Biological ontologies for data scientists. - NIH standarizing data collection - Observational Health Data Sciences and Informatics (OHDSI) OMOP Common Data Model\n\n\nTools and software\n\nRightfield: open source tool facilitates the integration of ontology terms into Excel spreadsheet.\nOwlready2: Python package, enables the loading of ontologies as Python objects. This versatile tool allows users to manipulate and store ontology classes, instances, and properties as needed.",
    "crumbs": [
      "Course material",
      "Key practices",
      "4. Metadata for NGS data"
    ]
  },
  {
    "objectID": "develop/03_DOD.html",
    "href": "develop/03_DOD.html",
    "title": "3. Best Practices for Data Storage",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nOrganize NGS data and external resources efficiently\nApply naming conventions for files and folders\nDefine rules for naming results and figures accurately\nSo far, we have covered how to adhere to FAIR and Open Science standards, which primarily focus on data sharing post-project completion. However, effective data management is essential while actively working on the project. Organizing data folders, raw and processed data, analysis scripts and pipelines, and results ensure long-term project success. Without a clear structure, future access and understanding of data become challenging, even more so for collaborators, leading to potential chaos down the line.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#folder-organization",
    "href": "develop/03_DOD.html#folder-organization",
    "title": "3. Best Practices for Data Storage",
    "section": "Folder organization",
    "text": "Folder organization\nHere we suggest the use of three main folders:\n\nShared project data folders:\n\n\nThis shared directory is designated for storing unprocessed sequencing data files, with each subfolder representing a separate project.\nEach project folder contains raw data, corresponding metadata, and optionally pre-processed data like quality control reports and processed data.\n\nInclude the pipeline or workflow used for data processing, along with a metadata file.\n\nData in these folders should be locked and set to read-only to prevent unauthorized (‚Äúunwanted‚Äù) modifications.\n\n\nIndividual project folders:\n\n\nThis directory typically belongs to the researcher conducting bioinformatics analyses and encompasses all essential files for a specific research project (data, scripts, software, workflows, results, etc.).\nA project may utilize data from various assays or results obtained from other projects. It‚Äôs important to avoid duplicating datasets; instead, link them from the original source to maintain data integrity and avoid redundancy.\n\n\nResources and databases folders:\n\n\nThis (commonly) shared directory contains common repositories or curated databases that facilitate research (genomics, clinical data, imaging data, and more!). For instance, in genomics, it includes genome references (fasta files), annotations (gtf files) for different species, and indexes for various alignment algorithms.\nEach folder corresponds to a unique reference or database version, allowing for multiple references from the same organism or different species.\n\nEnsure each contains the version of the reference and a metadata file.\nMore subfolders can be created for different data formats.\n\n\n\n\n\n\n\n\nVerify the integrity of downloaded files!\n\n\n\nEnsure that the person downloading the files employs checksums or cryptographic hash functions to verify the integrity and ascertain that files are neither corrupted nor tampered with.\n\nMD5 Checksum: Files with names ending in ‚Äú.md5‚Äù contain MD5 checksums. For instance, ‚Äúfilename.txt.md5‚Äù holds the MD5 checksum of ‚Äúfilename.txt‚Äù.‚Äù\n\n\n\n\n\n\n\n\n\nDatabase\n\n\n\n\n\n\n\nA database is a structured repository for storing, managing, and retrieving information, forming the cornerstone of efficient data organization.\n\n\n\n\n\n\n\n\n\n\n\nCreate shortcuts to public datasets and assays!\n\n\n\nThe use of symbolic links, also referred to as softlinks, is a key practice in large labs where data might used for different purposes and by multiple people.\n\nThey act as pointers, containing the path to the location of the target files/directories.\nThey avoid duplication and they are flexible and lightweight (do not occupy much disk space).\nThey simplify directory structures.\n\nExtra use case: create symbolic links to executable files and libraries!\n\n\n\n\n\n\n\n\n\nExercise: create a softlink link\n\n\n\n\n\n\n\nOpen your terminal and create a softlink using the following command. The first path is the target (directory or file) and the second one is where the symbolic link will be created.\nln -s path/to/dataset/&lt;ASSAY_ID&gt; /path/to/user/&lt;PROJECT_ID&gt;/data/\nNow, access the target file/directory through the symbolic link:\nls /path/to/user/&lt;PROJECT_ID&gt;/data/\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nFollow this example if need extra guidance (change paths!):\n\nCreate the target/original file\n\necho \"This is the content of the original file.\" &gt; /home/users/Documents/original_file.txt\n\nCreate the symbolic link\n\nln -s /home/users/Documents/original_file.txt /home/users/Desktop/original_file.txt\n\nVerify the symbolic link\n\nls -s /home/users/Desktop/original_file.txt\n\nAccess the file through the symbolic link:\n\ncat /home/users/Desktop/original_file.txt\nThe last command will display the contents of the original file.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#navigating-shared-project-data",
    "href": "develop/03_DOD.html#navigating-shared-project-data",
    "title": "3. Best Practices for Data Storage",
    "section": "1. Navigating Shared Project Data",
    "text": "1. Navigating Shared Project Data\nLet‚Äôs focus on the shared folders containing experimental datasets generated in-house.\n\nNaming Shared Folders Effectively\nCreate a folder for all your NGS experiments, for instance, named Assay. Each subfolder, denoted by a unique Assay-ID, should be named clearly and comprehensibly. Assay-ID comprises raw files, processed files, and the pipeline used to generate them. Raw files should remain unchanged, while modifications to processed files should be restricted post-preprocessing (e.g., after quality control) to prevent unintended alterations. Check the exercise for efficient naming of Assay-ID:\n\n\n\n\n\n\nExercise: name your Assay-ID\n\n\n\n\n\n\n\n\nHow would you ensure its name is unique and understood at a glance?\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\nUse an acronym (1) that describes the type of NGS assay (RNAseq, ChIPseq, ATACseq) a keyword (2) that represents a unique element to that assay, and the date (3).\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOther Assay ID code names\n\n\n\n\n\n\nCHIP: ChIP-seq\nRNA: RNA-seq\nATAC: ATAC-seq\nSCR: scRNA-seq\nPROT: Mass Spectrometry Assay\nCAT: Cut&Tag\nCAR: Cut&Run\nRIME: Rapid Immunoprecipitation Mass spectrometry of Endogenous proteins\n‚Ä¶\n\n\n\n\nKeep in mind that these folders might be (re)used in different individual projects over many years.\n\n\nOptimizing Folder Structures\nThe provided folder structure is designed to be intuitive for NGS data. The description and metadata files aid in understanding the project‚Äôs origin and structure, crucial for archiving and manuscript preparation. There is a section dedicated to databases in lesson 4. Let‚Äôs explore the example and its folder contents:\n&lt;data_type&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ README.md \n‚îú‚îÄ‚îÄ CHECKSUMS\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n    ‚îú‚îÄ‚îÄ fastqc/\n    ‚îú‚îÄ‚îÄ multiqc/\n    ‚îú‚îÄ‚îÄ final_fastq/\n‚îî‚îÄ‚îÄ raw\n    ‚îú‚îÄ‚îÄ .fastq.gz \n    ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: This file contains general information about the project or experiment, usually in markdown or plain text format. It includes details such as such as the origin of the raw NGS data (including sample information, laboratory protocols used, and the assay‚Äôs objectives). Sometimes, it also outlines the basic directory structure and file naming conventions.\nmetadata.yml: This serves as the metadata file for the project (see this lesson).\npipeline.md: This document describes the pipeline employed to process the raw data, along with the specific commands used to execute the pipeline. The specific format can vary depending on the workflow system employed (e.g., bash, Snakemake, Nextflow, Jupyter Notebooks, etc.) (see this lesson). Employing a standardized pipeline ensures a consistent file organization system (and the corresponding documentation)\nprocessed_data: folder with results of the preprocessing pipeline. The contents may vary depending on the pipeline utilized. For example,\n\nfastqc: quality Control results of the raw fastq files.\nmultiqc: aggregated quality control results across all samples\nfinal_fastq: cleaned and processed files\n\nraw_data: folder with the raw data.\n\n.fastq.gz or other file formats (depending on the field or the experiment)\nsamplesheet.csv: metadata information for the samples. It may contain additional columns that will facilitate downstream analysis. This file is key if are planning to use nf-core pipelines.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#navigating-project-folder",
    "href": "develop/03_DOD.html#navigating-project-folder",
    "title": "3. Best Practices for Data Storage",
    "section": "2. Navigating Project folder",
    "text": "2. Navigating Project folder\nIn the Projects folder, usually private to the individual performing the data analysis, each project has its own subfolder containing project information, data analysis scripts and pipelines, and results. It‚Äôs advisable to maintain folders for individual projects, separate from shared data folders, as project-specific files typically aren‚Äôt reused across multiple projects, and more than one dataset might be needed to answer a specific scientific question.\n\nNaming Project Folders Effectively\nThe Project folder should have a unique, easily readable, distinguishable, and instantly understandable name. For instance, consider naming it using the main author‚Äôs initials, a descriptive keyword, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n\n\nOptimizing Folder Structures\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data (symbolic link)\n‚îÇ  ‚îî‚îÄ‚îÄ raw\n‚îÇ  ‚îî‚îÄ‚îÄ processed\n‚îÇ  ‚îî‚îÄ‚îÄ external (third party resources)\n‚îú‚îÄ‚îÄ docs\n‚îÇ  ‚îî‚îÄ‚îÄ project_template.docx\n‚îú‚îÄ‚îÄ notebooks or pipelines/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ logs\n‚îú‚îÄ‚îÄ tmp\n‚îú‚îÄ‚îÄ environment\n‚îÇ  ‚îî‚îÄ‚îÄ requirements.txt or environment.yml\n‚îú‚îÄ‚îÄ scripts/src\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;file_name&gt;.html\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ tables/\n‚îÇ  ‚îî‚îÄ‚îÄ figures/\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: contains symlinks or shortcuts to where the data is (raw, processed, external, etc.), avoiding duplication and modification of original files.\ndocs: a folder containing Word documents, slides, or PDFs related to the project. It also contains your Data Management Plan.\nnotebooks or pipelines: a folder containing notebooks (Jupyter, R markdown, Quarto notebooks) or workflows (Snakemake or Nextflow) with the actual data analysis. Tip: Label them numerically indicating the sequential order.\nREADME.md: detailed description of the project in markdown format.\nlogs: log files.\ntmp: store temporary or intermediate files.\nenvironment: files for reproducing the analysis environment to reproduce the results, such as a Dockerfile, conda yaml file, or a text file (See 6th lesson for more tips on making your pipelines reproducible). It includes software, libraries/packages, and dependencies (and their versions!).\nscripts: a folder containing helper scripts to run data analysis or source code\nreports: Generated analysis as HTML, PDF, LaTeX, etc. Great for sharing with colleagues and creating formal reports of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. Tip: save the figures under a subfolder named after the notebook/pipeline that created them (you will appreciate this organization when you need to rerun analysis and know which script created each figure!).\n\nresults: results from the data analysis, such as tables and figures, etc. Tip: Create a subfolder named after the notebook or pipeline for storing the results generated by that specific notebook or pipeline.\nmetadata.yml: metadata file describing the dataset, samples, etc. (see this lesson).\n\n\n\n\n\n\n\nExercise: Write your personal data structure\n\n\n\n\n\n\n\n\nCreate your own data structure for one of the projects you are currently working on. Consider how it is similar to the example provided and how it differs. Make sure the data structure is easily understandable and navigable.\nWhat improvements or modifications could be made to enhance clarity and efficiency?",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#template-engine",
    "href": "develop/03_DOD.html#template-engine",
    "title": "3. Best Practices for Data Storage",
    "section": "Template engine",
    "text": "Template engine\nSetting up folder structures manually for each new project can be time-consuming. Thankfully, tools like Cookiecutter offer a solution by allowing users to create project templates easily. These templates can ensure consistency across projects and save time. Additionally, using cruft alongside Cookiecutter can assist in maintaining older templates when updates are made (by synchronizing them with the latest version).\n\n\n\n\n\n\nCookiecutter templates\n\n\n\n\nCookiecutter template for Data science projects\nBrickmanlab template for NGS data: similar to the folder structures in the examples above. You can download and modify it to suit your needs.\n\n\n\n\nQuick tutorial on cookiecutter\n\n\n\n\n\n\nSandbox Tutorial\n\n\n\nLearn how to create your own template here.\nWe offer workshops on practical RDM for NGS data. Keep an eye on the upcoming events on the Sandbox website.",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#resources-and-databases-folder",
    "href": "develop/03_DOD.html#resources-and-databases-folder",
    "title": "3. Best Practices for Data Storage",
    "section": "3. Resources and databases folder",
    "text": "3. Resources and databases folder\nHealth databases are utilized for storing, organizing, and providing access to diverse health-related data, including genomic data, clinical records, imaging data, and more. These resources are regularly updated and released under different versions from various sources. To ensure data reproducibility, it‚Äôs crucial to manage and specify the versions and sources of data within these databases.\n\n\n\n\n\n\nExample NGS: genomic resources\n\n\n\n\n\nFor example, preprocessing NGS data involves utilizing various genomic resources for tasks like aligning and annotating fastq files. Essential resources include reference genomes in FASTA format (e.g., human and mouse), indexed fasta files for alignment tools like STAR and Bowtie, and GTF or GFF files for quantifying reads into genomic regions. One of the latest human reference genome is GRCh38, however many past studies are based on GRCh37.\nHow can you keep track of your resources? Name the folder using the version, or use a reference genome manager such as refgenie.\n\nRefgenie\nIt manages the storage, access, and transfer of reference genome resources. It provides command-line and Python interfaces to download pre-built reference genome ‚Äúassets‚Äù, like indexes used by bioinformatics tools. It can also build assets for custom genome assemblies. Refgenie provides programmatic access to a standard genome folder structure, so software can swap from one genome to another. Check this tutorial to get started.\n\n\n\n\n\nManual Download\nBest practices for downloading data from the source while ensuring the preservation of information about the version and other metadata include:\n\nOrganizing data structure: Create a data structure that allows storing all versions in the same parent directory, and ensure that all lab members follow these practices.\nDocumentation and metadata preservation: Before downloading, carefully review the documentation provided by the database. Download files containing the data version and any associated metadata.\nREADME.md: Record the version of the data in the README.md file.\nChecksums: Check for and use checksums provided by the database to verify the integrity of the downloaded data, ensuring that it hasn‚Äôt been corrupted during transfer. Do the exercise below.\nVerify File size: Check the file size provided by the source. It is not as secure as checksum verification but discrepancies could indicate corruption.\nAutomated Processes: whenever possible, automate the download process to reduce the likelihood of errors and ensure consistency (e.g.¬†use bash script or pipeline).\n\n\n\n\n\n\n\nOptional: Exercise on CHECKSUMS\n\n\n\n\n\nWe recommend the use of md5sum to verify data integrity, especially if you are downloading large datasets. In this example, we use data from the HLA FTP Directory.\n\nInstall md5sum (from coreutils package)\n\n#!/bin/bash\n# On Ubuntu/Debian\napt-get install coreutils\n# On macOS\nbrew install coreutils\n\nCreate a bash script to download the target files (named ‚Äúdw_resources.sh‚Äù in the data structure).\n\n#!/bin/bash\n# Important: go through the README before downloading! Check if a checksums file is included. \n\n# 1. Create or change the directory to the resources dir. \n\n# Check for checksums (e.g.: md5checksum.txt), download, and modify it so that it only contains the checksums of the target files. The file will look like this:\n1a3d12e4e6cc089388d88e3509e41cb3  hla_gen.fasta\n# Finally, save it: \nmd5file=\"md5checksum.txt\"\n\n# Define the URL of the files to download\nurl=\"ftp://ftp.ebi.ac.uk/pub/databases/ipd/imgt/hla/hla_gen.fasta\"\n# \nfilename=$(basename \"$url\")\n\n# (Optional) Define a different filename to save the downloaded file (`wget -O $out_filename`)\n# out_filename = \"imgt_hla_gen.fasta\"\n\n# Download the file\nwget $url && \\\nmd5sum --status --check $md5file\n\nFolder structure\n\ngenomic_resources/\n‚îú‚îÄ‚îÄ specie1/\n‚îÇ  ‚îî‚îÄ‚îÄ version/\n‚îÇ     ‚îú‚îÄ‚îÄ files.txt\n‚îÇ     ‚îî‚îÄ‚îÄ indexes/\n‚îî‚îÄ‚îÄ dw_resources.sh\n\nCreate a md5sum file and share it with collaborators before sharing the data. This allows others to check the integrity of the files.\n\nmd5sum &lt;data&gt;\n\n\n\n\n\n\nExercise\n\n\n\n\n\n\n\nDownload a file using md5sums. Choose a file from your favorite dataset or select one from the HLA database (for quick testing, consider using a text file such as Nomenclature_2009.txt).",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#naming-conventions",
    "href": "develop/03_DOD.html#naming-conventions",
    "title": "3. Best Practices for Data Storage",
    "section": "Naming conventions",
    "text": "Naming conventions\nConsistent naming conventions play a crucial role in scientific research by enhancing organization and data retrieval. By adopting standardized naming conventions, researchers ensure that files, experiments, or datasets are labeled logically, facilitating easy location and comparison of similar data. For instance, in fields like genomics, uniform naming conventions for files associated with particular experiments or samples allow for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. Overall, promotes efficiency, collaboration, and the integrity of scientific work.\n\n\n\n\n\n\nGeneral tips for file and folder naming\n\n\n\nRemember to keep the folder structure simple.\n\nKeep it short and meaningful (use understandable abbreviation only, e.g., Cor for correlations or LFC for Log Fold Change)\nConsider including one of these elements: project name, category, descriptor, content, author‚Ä¶\n\nAuthor-based: use initials\n\nUse alphanumeric characters: letters (A-Z) and numbers (0-9)\nAvoid special characters: ~!@#$%^&*()`‚Äú|\nDate-based format: use YYYYMMDD format (year/month/day format helps with sorting and listing files in chronological order)\nUse underscores and hyphens as delimiters and avoid spaces.\n\nNot all search tools may work well with spaces (messy to indicate paths)\nIf the length is a concern, use capital letters to delimit words camelCase.\n\nSequential numbering: Use a two-‚Äëdigit format for single-digit numbers (0‚Äì9) to ensure correct numerical sequence order (for example, 01 and not 1)\nVersion control: Indicate the version (‚ÄúV‚Äù) or revision (‚ÄúR‚Äù) as the last element, using the two-digit format (e.g., v01, v02)\nWrite down your naming convention pattern and document it in the README file\n\n\n\n\n\n\n\n\n\nDefine your file name conventions\n\n\n\n\n\n\n\nAvoid long and complicated names and ensure your file names are both informative and easy to manage:\n\nFor saving a new plot, a heatmap representing sample correlations\nWhen naming the file for the document containing the Research Data Management Course Objectives (Version 2, 2nd May 2024) from the University of Copenhagen\nConsider the most common file types you work with, such as visualizations, tables, etc., and create logical and clear file names\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\nheatmap_sampleCor_20240101.png\nKU_RDM-objectives_20240502_v02.doc or KU_RDMObj_20240502_v02.doc\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdditional file naming conventions\n\n\n\n\n\n\n\n\n\n\n\nname\ndescription\nnaming_convention\nfile format\nexample\n\n\n\n\n.fastq\nraw sequencing reads\nnan\nnan\nsampleID_run_read1.fastq\n\n\n.fastqc\nquality control from fastqc\nnan\nnan\nsampleID_run_read1.fastqc\n\n\n.bam\naligned reads\nnan\nnan\nsampleID_run_read1.bam\n\n\nGTF\nsequence annotation\nnan\nnan\none of https://www.gencodegenes.org/\n\n\nGFF\nsequence annotation\nnan\nnan\none of https://www.gencodegenes.org/\n\n\n.bed\ngenome locations\nnan\nnan\nnan\n\n\n.bigwig\ngenome coverage\nnan\nnan\nnan\n\n\n.fasta\nsequence data (nucleotide/aminoacid)\nnan\nnan\none of https://www.gencodegenes.org/\n\n\nMultiqc report\nQC aggregated report\n&lt;assayID\\&gt;_YYYYMMDD.multiqc\nmultiqc\nRNA_20200101.multiqc\n\n\nCount matrix\nfinal count matrix\n&lt;assayID\\&gt;_cm_aligner_YYYYMMDD.tsv\ntsv\nRNA_cm_salmon_20200101.tsv\n\n\nDEA\ndifferential expression analysis results\nDEA_&lt;condition1-condition2\\&gt;_LFC&lt;absolute_threshold\\&gt;_p&lt;pvalue decimals\\&gt;_YYYYMMDD.tsv\ntsv\nDEA_treat-untreat_LFC1_p01_20200101.tsv\n\n\nDBA\ndifferential binding analysis results\nDBA_&lt;condition1-condition2\\&gt;_LFC&lt;absolute_threshold\\&gt;_p&lt;pvalue decimals\\&gt;_YYYYMMDD.tsv\ntsv\nDBA_treat-untreat_LFC1_p01_20200101.tsv\n\n\nMAplot\nMA plot\nMAplot_&lt;condition1-condition2\\&gt;_YYYYMMDD.jpeg\njpeg\nMAplot_treat-untreat_20200101.jpeg\n\n\nHeatmap plot\nHeatmap plot of anything\nheatmap_&lt;type\\&gt;_YYYYMMDD.jpeg\njpeg\nheatmap_sampleCor_20200101.jpeg\n\n\nVolcano plot\nVolcano plot\nvolcano_&lt;condition1-condition2\\&gt;_YYYYMMDD.jpeg\njpeg\nvolcano_treat-untreat_20200101.jpeg\n\n\nVenn diagram\nVenn diagram\nvenn_&lt;type\\&gt;_YYYYMMDD.jpeg\njpeg\nvenn_consensus_20200101.jpeg\n\n\nEnrichment table\nEnrichment results\nnan\ntsv\nnan",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/03_DOD.html#wrap-up",
    "href": "develop/03_DOD.html#wrap-up",
    "title": "3. Best Practices for Data Storage",
    "section": "Wrap up",
    "text": "Wrap up\nIn this lesson, we have learned some practical tips and examples about how to organize your data and bring some order to chaos! Complete the practical tutorial on using cookiecutter as a template engine to be able to create your own templates and reuse them as much as you need.\n\nSources\n\nUK Data Service: https://ukdataservice.ac.uk/learning-hub/research-data-management/format-your-data/organising/\nOakland University: https://library.oakland.edu/services/research-data/file-org.html\nCessda guidelines: https://dmeg.cessda.eu/Data-Management-Expert-Guide/2.-Organise-Document/File-naming-and-folder-structure.\nRDMkit Elixir Europe: https://rdmkit.elixir-europe.org/data_organisation",
    "crumbs": [
      "Course material",
      "Key practices",
      "3. Best Practices for Data Storage"
    ]
  },
  {
    "objectID": "develop/practical_workshop.html",
    "href": "develop/practical_workshop.html",
    "title": "Practical material",
    "section": "",
    "text": "Course Overview\n\n\n\n‚è∞ Time Estimation: X minutes\nüí¨ Learning Objectives:\n\nOrganize and structure your data and data analysis with cookiecutter templates\nEstablish metadata fields and collect metadata when creating a cookiecutter folder\nEstablish naming conventions for your data\nMake a catalog of your data\nCreate GitHub repositories of your data analysis and display them as GitHub Pages\nArchive GitHub repositories on Zenodo\nThis is a practical version of the full RDM on NGS data workshop. The main key points of the exercises shown here is to help you organize and structure your NGS datasets and your data analyses. We will see how to keep track of your experiments metadata and how to safely version control and archive your data analyses using GitHub repositories and Zenodo. We hope that through these practical exercises and step-by-step guidance, you‚Äôll gain valuable skills in efficiently managing and sharing your research data, enhancing the reproducibility and impact of your work."
  },
  {
    "objectID": "develop/practical_workshop.html#organize-and-structure-your-ngs-data-and-data-analysis",
    "href": "develop/practical_workshop.html#organize-and-structure-your-ngs-data-and-data-analysis",
    "title": "Practical material",
    "section": "1. Organize and structure your NGS data and data analysis",
    "text": "1. Organize and structure your NGS data and data analysis\nApplying a consistent file structure and naming conventions to your files will help you to efficiently manage your data. We will divide your NGS data and data analyses into two different types of folders:\n\nAssay folders: These folders contain the raw and processed NGS datasets, as well as the pipeline/workflow used to generate the processed data, provenance of the raw data and quality control reports of the data. This data should be locked and read-only to prevent unwanted modifications.\nProject folders: These folders contain all the necessary files for a specific research project. A project may use several assays or results from other projects. The assay data should not be copied or duplicated, but linked from the original source.\n\nProjects and Assays are separated from each other because a project may use one or more assays to answer a scientific question, and assays may be reused several times in different projects. This could be, for example, all the data analysis related to a publication (a RNAseq and a ChIPseq experiment), or a comparison between a previous ATACseq experiment (which was used for a older project) with a new laboratory protocol.\nYou could also create Genomic resources folders such things such as genome references (fasta files) and annotations (gtf files) for different species, as well as indexes for different alignment algorithms. If you want to know more, feel free to check the relevant full lesson\nThis will help you to keep your data tidied up, specially if you are working on a big lab where assays may be used for different purposes and different people!\n\nAssay folder\nFor each NGS experiment there should be an Assay folder that will contain all experimental datasets, that is, an Assay (raw files and pipeline processed files). Raw files should not be modified at all, but you should probably lock modifications to the final results once you are done with preprocessing the data. This will help you prevent unwanted modifications to the data. Each Assay subfolder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name an NGS assay using an acronym for the type of NGS assay (RNAseq, ChIPseq, ATACseq), a keyword that represents a unique descriptive element of that assay, and the date. Like this:\n&lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example CHIP_Oct4_20230101 is a ChIPseq assay made on 1st January 2023 with the keyword Oct4, so it is easily identifiable by eye. Next, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\nCHIP_Oct4_20230101/\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ metadata.yml\n‚îú‚îÄ‚îÄ pipeline.md\n‚îú‚îÄ‚îÄ processed\n‚îî‚îÄ‚îÄ raw\n   ‚îú‚îÄ‚îÄ .fastq.gz\n   ‚îî‚îÄ‚îÄ samplesheet.csv\n\nREADME.md: Long description of the assay in markdown format. It should contain provenance of the raw NGS data (samples, laboratory protocols used, aim of the assay, etc)\nmetadata.yml: metadata file for the assay describing different keys and important information regarding that assay (see this lesson).\npipeline.md: description of the pipeline used to process raw data, as well as the commands used to run the pipeline.\nprocessed: folder with results of the preprocessing pipeline. Contents depend on the pipeline used.\nraw: folder with the raw data.\n\n.fastq.gz:In the case of NGS assays, there should be fastq files.\nsamplesheet.csv: file that contains metadata information for the samples. This file is used to run the nf-core pipelines. You can also add extra columns with info regarding the experimental variables and batches so it can be used for downstream analysis as well.\n\n\n\n\nProject folder\nOn the other hand, we have the other type of folder called Projects. In this folder you will save a subfolder for each project that you (or your lab) works on. Each Project subfolder will contain project information and all the data analysis notebooks and scripts used in that project.\nAs like for an Assay folder, the Project folder should be named in a way that it is unique, easily readable, distinguishable and understood at a glance. For example, you could name it after the main author initials, a keyword that represents a unique descriptive element of that assay, and the date:\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\nFor example, JARH_Oct4_20230101, is a project about the gene Oct4 owned by Jose Alejandro Romero Herrera, created on the 1st of January of 2023.\nNext, let‚Äôs take a look at a possible folder structure and what kind of files you can find there.\n&lt;author_initials&gt;_&lt;keyword&gt;_YYYYMMDD\n‚îú‚îÄ‚îÄ data\n‚îÇ  ‚îî‚îÄ‚îÄ &lt;Assay-ID&gt;_&lt;keyword&gt;_YYYYMMDD/\n‚îú‚îÄ‚îÄ documents\n‚îÇ  ‚îî‚îÄ‚îÄ Non-sensitive_NGS_research_project_template.docx\n‚îú‚îÄ‚îÄ notebooks\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.rmd\n‚îú‚îÄ‚îÄ README.md\n‚îú‚îÄ‚îÄ reports\n‚îÇ  ‚îú‚îÄ‚îÄ figures\n‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ  ‚îÇ   ‚îî‚îÄ‚îÄ heatmap_sampleCor_20230102.png\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis.html\n‚îú‚îÄ‚îÄ requirements.txt\n‚îú‚îÄ‚îÄ results\n‚îÇ  ‚îî‚îÄ‚îÄ 01_data_analysis/\n‚îÇ      ‚îî‚îÄ‚îÄ DEA_treat-control_LFC1_p01.tsv\n‚îú‚îÄ‚îÄ scripts\n‚îî‚îÄ‚îÄ metadata.yml\n\ndata: folder that contains symlinks or shortcuts to where the data is, avoiding copying and modification of original files.\ndocuments: folder containing word documents, slides or pdfs related to the project, such as explanations of the data or project, papers, etc. It also contains your Data Management Plan.\n\nNon-sensitive_NGS_research_project_template.docx. This is a pre-filled Data Management Plan based on the Horizon Europe guidelines.\n\nnotebooks: folder containing Jupyter, R markdown or Quarto notebooks with the actual data analysis.\nREADME.md: detailed description of the project in markdown format.\nreports: notebooks rendered as html/docx/pdf versions, ideal for sharing with colleagues and also as a formal report of the data analysis procedure.\n\nfigures: figures produced upon rendering notebooks. The figures will be saved under a subfolder named after the notebook that created them. This is for provenance purposes so we know which notebook created which figures.\n\nrequirements.txt: file explaining what software and libraries/packages and their versions are necessary to reproduce the code.\nresults: results from the data analysis, such as tables with differentially expressed genes, enrichment results, etc.\nscripts: folder containing helper scripts needed to run data analysis or reproduce the work of the folder\ndescription.yml: short description of the project.\nmetadata.yml: metadata file for the assay describing different keys (see this lesson).\n\n\n\nTemplate engine\nIt is very easy to create a folder template using cookiecutter. Cookiecutter is a command-line utility that creates projects from cookiecutters (that is, a template), e.g.¬†creating a Python package project from a Python package project template. Here you can find an example of a cookiecutter folder template directed to NGS data, where we have applied the structures explained in the previous sections. You are very welcome to adapt it or modify it to your needs!\n\nQuick tutorial on cookiecutter\nCreating a Cookiecutter template from scratch involves defining a folder structure, creating a cookiecutter.json file, and specifying the placeholders (keywords) that will be replaced during project generation. Let‚Äôs walk through the process step by step:\n\nStep 1: Create a Folder Template\nStart by creating a folder with the structure you want for your template. For example, let‚Äôs create a simple Python project template:\nmy_template/\n|-- {{cookiecutter.project_name}}\n|   |-- main.py\n|-- tests\n|   |-- test_{{cookiecutter.project_name}}.py\n|-- README.md\nIn this example, {cookiecutter.project_name} is a placeholder that will be replaced with the actual project name when the template is used.\n\n\nStep 2: Create cookiecutter.json\nIn the root of your template folder, create a file named cookiecutter.json. This file will define the variables (keywords) that users will be prompted to fill in. For our Python project template, it might look like this:\n{\n  \"project_name\": \"MyProject\",\n  \"author_name\": \"Your Name\",\n  \"description\": \"A short description of your project\"\n}\nThese are the questions users will be asked when generating a project based on your template. The values provided here will be used to replace the corresponding placeholders in the template files.\nIn addition to replacing placeholders in file and directory names, Cookiecutter can also automatically fill in information within the contents of text files. This can be useful for providing default configurations or templates for code files. Let‚Äôs extend our previous example to include a placeholder inside a text file:\nFirst, modify the my_template/main.py file to include a placeholder inside its contents:\n# main.py\n\ndef hello():\n    print(\"Hello, {{cookiecutter.project_name}}!\")\nNow, the {cookiecutter.project_name} placeholder is inside the main.py file. When you run Cookiecutter, it will automatically replace the placeholders not only in file and directory names but also within the contents of text files. After running Cookiecutter, your generated main.py file might look like this:\n# main.py\n\ndef hello():\n    print(\"Hello, MyProject!\")  # Assuming \"MyProject\" was entered as the project_name\n\n\nStep 3: Use Cookiecutter\nNow that your template is set up, you can use Cookiecutter to generate a project based on it. Open a terminal and run:\ncookiecutter path/to/your/template\nCookiecutter will prompt you to fill in the values for project_name, author_name, and description. After you provide these values, Cookiecutter will replace the placeholders in your template files with the entered values.\n\n\nStep 4: Explore the Generated Project\nOnce the generation process is complete, navigate to the directory where Cookiecutter created the new project. You will see a project structure with the placeholders replaced by the values you provided.\n\n\n\n\n\n\nExercise 1: Create your own template\n\n\n\n\n\n\n\nUsing cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs!\nRequierements:\nUsing cookiecutter, create your own templates for your folders. You do not need to copy exactly our suggestions, adjust your template to your own needs! In order to create your cookiecutter template, your will need to install python, cookiecutter, Git and a GitHub account. If you do not have Git and a GitHub account, we suggest you do one as soon as possible. We will take a deeper look at Git and GitHub in the version control lesson.\nWe have prepared already two simple cookiecutter templates in GitHub repositories.\nAssay\n\nFirst, fork our Assay folder template from the GitHub page into your own account/organization. \nThen, use git clone &lt;your URL to the template&gt; to put it in your computer.\nModify the contents of the repository so that it matches the Assay example above. You are welcome to do changes as you please!\nModify the cookiecutter.json file so that it will include the Assay name template\nGit add, commit and push your changes\nTest your folder by using cookiecutter &lt;URL to your GitHub repository for \"assay-template&gt;\n\nProject\n\nFirst, fork our Project folder template from the GitHub page into your own account/organization. \nThen, use git clone &lt;your URL to the template&gt; to put it in your computer.\nModify the contents of the repository so that it matches the Project example above. You are welcome to do changes as you please!\nModify the cookiecutter.json file so that it will include the Project name template\nGit add, commit and push your changes\nTest your folder by using cookiecutter &lt;URL to your GitHub repository for \"project-template&gt;"
  },
  {
    "objectID": "develop/practical_workshop.html#metadata",
    "href": "develop/practical_workshop.html#metadata",
    "title": "Practical material",
    "section": "2. Metadata",
    "text": "2. Metadata\nMetadata is the behind-the-scenes information that makes sense of data and gives context and structure. For NGS data, metadata includes information such as when and where the data was collected, what it represents, and how it was processed. Let‚Äôs check what kind of relevant metadata is available for NGS data and how to capture it in your Assay or Project folders. Both of these folders contain a metadata.yml file and a README.md file. In this section, we will check what kind of information you should collect in each of these files.\n\n\n\n\n\n\nMetadata and controlled vocabularies\n\n\n\nIn order for metadata to be most useful, you should try to use controlled vocabularies for all your fields. For example, tissue could be described with the UBERON ontologies, species using the NCBI taxonomy, diseases using the Mondo database, etc. Unfortunately, implementing a systematic way of using these vocabularies is rather complex and outside the scope of this workshop, but you are very welcome to try to implement them on your own!\n\n\n\nREADME.md file\nThe README.md file is a markdown file that allows you to write a long description of the data placed in a folder. Since it is a markdown file, you are able to write in rich text format (bold, italic, include links, etc) what is inside the folder, why it was created/collected, how and when. If it is an Assay folder, you could include the laboratory protocol used to generate the samples, images explaining the experiment design, a summary of the results of the experiment and any sort of comments that would help to understand the context of the experiment. On the other hand, a ‚ÄòProject‚Äô README file may contain a description of the project, what are its aims, why is it important, what ‚ÄòAssays‚Äô is it using, how to interpret the code notebooks, a summary of the results and, again, any sort of comments that would help to understand the project.\nHere is an example of a README file for a Project folder:\n# NGS Analysis Project: Exploring Gene Expression in Human Tissues\n\n## Aims\n\nThis project aims to investigate gene expression patterns across various human tissues using Next Generation Sequencing (NGS) data. By analyzing the transcriptomes of different tissues, we seek to uncover tissue-specific gene expression profiles and identify potential markers associated with specific biological functions or diseases.\n\n## Why It's Important\n\nUnderstanding tissue-specific gene expression is crucial for deciphering the molecular basis of health and disease. Identifying genes that are uniquely expressed in certain tissues can provide insights into tissue function, development, and potential therapeutic targets. This project contributes to our broader understanding of human biology and has implications for personalized medicine and disease research.\n\n## Datasets\n\nWe have used internal datasets with IDs: RNA_humanSkin_20201030, RNA_humanBrain_20210102, RNA_humanLung_20220304.\n\nIn addition, we utilized publicly available NGS datasets from the GTEx (Genotype-Tissue Expression) project, which provides comprehensive RNA-seq data across multiple human tissues. These datasets offer a wealth of information on gene expression levels and isoform variations across diverse tissues, making them ideal for our analysis.\n\n## Summary of Results\n\nOur analysis revealed distinct gene expression patterns among different human tissues. We identified tissue-specific genes enriched in brain tissues, highlighting their potential roles in neurodevelopment and function. Additionally, we found a set of genes that exhibit consistent expression across a range of tissues, suggesting their fundamental importance in basic cellular processes.\n\nFurthermore, our differential expression analysis unveiled significant changes in gene expression between healthy and diseased tissues, shedding light on potential molecular factors underlying various diseases. Overall, this project underscores the power of NGS data in unraveling intricate gene expression networks and their implications for human health.\n\n---\n\nFor more details, refer to our [Jupyter Notebook](link-to-jupyter-notebook.ipynb) for the complete analysis pipeline and code.\n\n\nmetadata.yml\nThe metadata file is a yml file, which is a text document that contains data formatted using a human-readable data format for data serialization.\n\n\n\nyaml file example\n\n\n\n\nMetadata fields\nThere is a ton of information you can collect regarding an NGS assay or a project. Some information fields are very general, such as author or date, while others are specific to the Assay or Project folder. Below, we will take a look at minimal information you should collect in each of the folders.\n\nGeneral metadata fields\nHere you can find a list of suggestions for general metadata fields that can be used for both assays and project folders:\n\nTitle: A brief yet informative name for the dataset.\nAuthor(s): The individual(s) or organization responsible for creating the dataset. You can use your ORCID\nDate Created: The date when the dataset was originally generated or compiled. Use YYYY-MM-DD format!\nDescription: A short narrative explaining the content, purpose, and context.\nKeywords: A set of descriptive terms or phrases that capture the folder‚Äôs main topics and attributes.\nVersion: The version number or identifier for the folder, useful for tracking changes.\nLicense: The type of license or terms of use associated with the dataset/project.\n\n\n\nAssay metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Assay folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nassay_ID\nIdentifier for the assay that is at least unique within the project\n&lt;Assay-ID\\&gt;_&lt;keyword\\&gt;_YYYYMMDD\nNA\nCHIP_Oct4_20200101\n\n\nassay_type\nThe type of experiment performed, eg ATAC-seq or seqFISH\nNA\nontology field- e.g. EFO or OBI\nChIPseq\n\n\nassay_subtype\nMore specific type or assay like bulk nascent RNAseq or single cell ATACseq\nNA\nontology field- e.g. EFO or OBI\nbulk ChIPseq\n\n\nowner\nOwner of the assay (who made the experiment?).\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nJose Romero\n\n\nplatform\nThe type of instrument used to perform the assay, eg Illumina HiSeq 4000 or Fluidigm C1 microfluidics platform\nNA\nontology field- e.g. EFO or OBI\nIllumina\n\n\nextraction_method\nTechnique used to extract the nucleic acid from the cell\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nlibrary_method\nTechnique used to amplify a cDNA library\nNA\nontology field- e.g. EFO or OBI\nNA\n\n\nexternal_accessions\nAccession numbers from external resources to which assay or protocol information was submitted\nNA\neg protocols.io, AE, GEO accession number, etc\nGSEXXXXX\n\n\nkeyword\nKeyword for easy identification\nwordWord\ncamelCase\nOct4ChIP\n\n\ndate\nDate of assay creation\nYYYYMMDD\nNA\n20200101\n\n\nnsamples\nNumber of samples analyzed in this assay\n&lt;integer\\&gt;\nNA\n9\n\n\nis_paired\nPaired fastq files or not\n&lt;single OR paired\\&gt;\nNA\nsingle\n\n\npipeline\nPipeline used to process data and version\nNA\nNA\nnf-core/chipseq -r 1.0\n\n\nstrandedness\nThe strandedness of the cDNA library\n&lt;+ OR - OR *\\&gt;\nNA\n*\n\n\nprocessed_by\nWho processed the data\n&lt;First Name\\&gt; &lt;Last Name\\&gt;\nNA\nSarah Lundregan\n\n\norganism\nOrganism origin\n&lt;Genus species\\&gt;\nTaxonomy name\nMus musculus\n\n\norigin\nIs internal or external (from a public resources) data\n&lt;internal OR external\\&gt;\nNA\ninternal\n\n\npath\nPath to files\n&lt;/path/to/file\\&gt;\nNA\nNA\n\n\nshort_desc\nShort description of the assay\nplain text\nNA\nOct4 ChIP after pERK activation\n\n\nELN_ID\nID of the experiment/assay in your Electronic Lab Notebook software, like labguru or benchling\nplain text\nNA\nNA\n\n\n\n\n\n\n\n\n\n\nProject metadata fields\nHere you will find a table with possible metadata fields that you can use to annotate and track your Project folders:\n\n\n\n\n\n\n\n\n\nMetadata field\nDefinition\nFormat\nOntology\nExample\n\n\n\n\nproject\nProject ID\n&lt;surname\\&gt;_et_al_2023\nNA\nproks_et_al_2023\n\n\nauthor\nOwner of the project\n&lt;First name\\&gt; &lt;Surname\\&gt;\nNA\nMartin Proks\n\n\ndate\nDate of creation\nYYYYMMDD\nNA\n20230101\n\n\ndescription\nShort description of the project\nPlain text\nNA\nThis is a project describing the effect of Oct4 perturbation after pERK activation\n\n\n\n\n\n\n\n\n\n\n\nMore info\nThe information provided in this lesson is not at all exhaustive. There might be many more fields and controlled vocabularies that could be useful for your NGS data. We recommend that you take a look at the following sources for more information!\n\nTranscriptomics metadata standards and fields\nBionty: Biological ontologies for data scientists.\n\n\n\n\n\n\n\nExercise 2: modify the metadata.yml files in your cookiecutter templates\n\n\n\n\n\n\n\nWe have seen some examples of metadata for NGS data. It is time now to customize your cookiecutter templates and modify the metadata.yml files so that they fit your needs!\n\nThink about what kind of metadata you would like to include.\nModify the cookiecutter.json file so that when you create a new folder template, all the metadata is filled accordingly.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n\n\ncookiecutter_json_example\n\n\n\n\n\n\n\n\nModify the metadata.yml file so that it includes the metadata recorded by the cookiecutter.json file.\n\n\n\n\n\n\n\nHint\n\n\n\n\n\n\n\n\n\n\nassay_metadata_example\n\n\n\n\n\n\n\n\nModify the README.md file so that it includes the short description recorded by the cookiecutter.json file.\nGit add, commit and push the changes of your template.\nTest your folders by using the command cookiecutter &lt;URL to your cookiecutter repository in GitHub&gt;"
  },
  {
    "objectID": "develop/practical_workshop.html#naming-conventions",
    "href": "develop/practical_workshop.html#naming-conventions",
    "title": "Practical material",
    "section": "3. Naming conventions",
    "text": "3. Naming conventions\nUsing consistent naming conventions is important in scientific research as it helps with the organization and retrieval of data or results. By adopting standardized naming conventions, researchers ensure that files, experiments, or data sets are labeled in a clear, logical manner. This makes it easier to locate and compare similar types of data or results, even when dealing with large datasets or multiple experiments. For instance, in genomics, employing uniform naming conventions for files related to specific experiments or samples allows for swift identification and comparison of relevant data, streamlining the research process and contributing to the reproducibility of findings. This practice promotes efficiency, collaboration, and the integrity of scientific work.\n\nGeneral tips\nBelow you will find a small list of general tips to follow when you name a folder or a file:\n\nUse only alphanumeric characters to write a word: a to z and 0 to 9\nAvoid special characters: ~!@#$%^&*()`‚Äú|\nDate format: use YYYYMMDD format. For example: 20230101.\nAuthors: use initials. For example: JARH\nDon‚Äôt use of spaces! Computers get very confused when you need to point a path to a file and it contains spaces! Instead:\n\nSeparate field sections are separated by underscores _.\nWords in each section are written in camelCase. It would look then like this: field1_word1Word2.txt. For example: heatmap_sampleCor_20230101.png. The first field indicates what this file is, i.e., a heatmap. Second field is what is being plotted, i.e, sample correlations; since the field contains two words, they are written in camelCase. The third field is the date of when the image was created.\n\nUse as short fields as possible. You can try to use understandable abbreviations, like LFC for LogFoldChange, Cor for correlations, Dist for distances, etc.\nAvoid long names as much as you can, be concise!\nAvoid creating many sublevels of folders.\nWrite down your naming convention pattern and document it in the README file\nWhen using a sequential numbering system, use leading zeros to make sure files sort in sequential order. Using 01 instead of just 1 if your sequence only goes up to 99.\nVersions should be used as the last element, and use at least two digits with a leading 0 (e.g.¬†v01, v02)\n\n\n\nSuggestions for NGS data\nMore info on naming conventions for different types of files and analysis is in development.\n\n\n\n\n\n\n\n\n\nname\ndescription\nnaming_convention\nfile format\nexample\n\n\n\n\n.fastq\nraw sequencing reads\nnan\nnan\nsampleID_run_read1.fastq\n\n\n.fastqc\nquality control from fastqc\nnan\nnan\nsampleID_run_read1.fastqc\n\n\n.bam\naligned reads\nnan\nnan\nsampleID_run_read1.bam\n\n\nGTF\nsequence annotation\nnan\nnan\none of https://www.gencodegenes.org/\n\n\nGFF\nsequence annotation\nnan\nnan\none of https://www.gencodegenes.org/\n\n\n.bed\ngenome locations\nnan\nnan\nnan\n\n\n.bigwig\ngenome coverage\nnan\nnan\nnan\n\n\n.fasta\nsequence data (nucleotide/aminoacid)\nnan\nnan\none of https://www.gencodegenes.org/\n\n\nMultiqc report\nQC aggregated report\n&lt;assayID\\&gt;_YYYYMMDD.multiqc\nmultiqc\nRNA_20200101.multiqc\n\n\nCount matrix\nfinal count matrix\n&lt;assayID\\&gt;_cm_aligner_YYYYMMDD.tsv\ntsv\nRNA_cm_salmon_20200101.tsv\n\n\nDEA\ndifferential expression analysis results\nDEA_&lt;condition1-condition2\\&gt;_LFC&lt;absolute_threshold\\&gt;_p&lt;pvalue decimals\\&gt;_YYYYMMDD.tsv\ntsv\nDEA_treat-untreat_LFC1_p01_20200101.tsv\n\n\nDBA\ndifferential binding analysis results\nDBA_&lt;condition1-condition2\\&gt;_LFC&lt;absolute_threshold\\&gt;_p&lt;pvalue decimals\\&gt;_YYYYMMDD.tsv\ntsv\nDBA_treat-untreat_LFC1_p01_20200101.tsv\n\n\nMAplot\nMA plot\nMAplot_&lt;condition1-condition2\\&gt;_YYYYMMDD.jpeg\njpeg\nMAplot_treat-untreat_20200101.jpeg\n\n\nHeatmap plot\nHeatmap plot of anything\nheatmap_&lt;type\\&gt;_YYYYMMDD.jpeg\njpeg\nheatmap_sampleCor_20200101.jpeg\n\n\nVolcano plot\nVolcano plot\nvolcano_&lt;condition1-condition2\\&gt;_YYYYMMDD.jpeg\njpeg\nvolcano_treat-untreat_20200101.jpeg\n\n\nVenn diagram\nVenn diagram\nvenn_&lt;type\\&gt;_YYYYMMDD.jpeg\njpeg\nvenn_consensus_20200101.jpeg\n\n\nEnrichment table\nEnrichment results\nnan\ntsv\nnan\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 3: Create your own naming conventions\n\n\n\n\n\n\n\nThink about the most common types of files and folders you will be working on, such as visualizations, results tables, processed files, etc. Then come up with a logical and clear way of naming those files using the tips suggested above. Remember to avoid making long and complicated names!"
  },
  {
    "objectID": "develop/practical_workshop.html#create-a-catalog-of-your-assays-folder",
    "href": "develop/practical_workshop.html#create-a-catalog-of-your-assays-folder",
    "title": "Practical material",
    "section": "4. Create a catalog of your assays folder",
    "text": "4. Create a catalog of your assays folder\nThe next step is to collect all the NGS datasets that you have created in the manner explained above. Since your folders all should contain the metadata.yml file in the same place with the same metadata, it should be very easy to iteratively go through all the folders and merge all the metadata.yml files into a one single table. This table can be then browsed easily with Microsoft Excel, for example. If you are interested in making a Shiny app or Python Panel tool to interactively browse the catalog, check out this lesson.\n\n\n\n\n\n\nExercise 4: create a metadata.tsv catalog\n\n\n\n\n\n\n\nWe will make a small script in R (or you can make one with python) that recursively goes through all the folders inside a input path (like your Assays folder), fetch all the metadata.yml files and merge them. Finally, it will write a tsv file as an output.\n\nCreate a folder call Assays\nUnder that folder, make three new Assay folders from your cookiecutter template\nRun the script below with R (or create your own with python). Modify the folder_path variable so it matches the path tot the folder Assays. The table will be written under the same folder_path.\nVisualize your Assays table with Excel\n\n\nlibrary(yaml)\nlibrary(dplyr)\nlibrary(lubridate)\n\n# Function to recursively fetch metadata.yml files\nget_metadata &lt;- function(folder_path) {\n    file_list &lt;- list.files(path = folder_path, pattern = \"metadata\\\\.yml$\", recursive = TRUE, full.names = TRUE)\n    metadata_list &lt;- lapply(file_list, yaml::yaml.load_file)\n    return(metadata_list)\n    }\n\n# Specify the folder path\n    folder_path &lt;- \"/path/to/your/folder\"\n\n    # Fetch metadata from the specified folder\n    metadata &lt;- get_metadata(folder_path)\n\n    # Convert metadata to a data frame\n    metadata_df &lt;- data.frame(matrix(unlist(metadata), ncol = length(metadata), byrow = TRUE))\n    colnames(metadata_df) &lt;- names(metadata[[1]])\n\n    # Save the data frame as a TSV file\n    output_file &lt;- paste0(\"database_\", format(Sys.Date(), \"%Y%m%d\"), \".tsv\")\n    write.table(metadata_df, file = output_file, sep = \"\\t\", quote = FALSE, row.names = FALSE)\n\n    # Print confirmation message\n    cat(\"Database saved as\", output_file, \"\\n\")"
  },
  {
    "objectID": "develop/practical_workshop.html#version-control-of-your-data-analysis-using-git-and-github",
    "href": "develop/practical_workshop.html#version-control-of-your-data-analysis-using-git-and-github",
    "title": "Practical material",
    "section": "5. Version control of your data analysis using Git and Github",
    "text": "5. Version control of your data analysis using Git and Github\nVersion control is a systematic approach to tracking changes made to a project over time. It provides a structured means of documenting alterations, allowing you to revisit and understand the evolution of your work. In research data management and data analytics, version control is very important and gives you a lot of advantages.\nGit is a distributed version control system that enables developers and researchers to efficiently manage their project‚Äôs history, collaborate seamlessly, and ensure data integrity. At its core, Git operates through the following principles and mechanisms: On the other hand, GitHub is a web-based platform that enhances Git‚Äôs capabilities by providing a collaborative and centralized hub for hosting Git repositories. It offers several key functionalities, such as tracking issues, security features to safeguard your repos, and GitHub Pages that allows you to create websites to showcase your projects.\n\n\n\n\n\n\nCreate a GitHub organization for your lab or department\n\n\n\nGitHub allows users to create organizations and teams that will collaborate together or create repositories under the same umbrella organization. If you would like to create an educational organization in GitHub, you can do so for free! For example, you could create a GitHub account for your lab.\nIn order to create a GitHub organization, follow these instructions\nAfter you have created the GitHub organization, make sure that you create your repositories under the organization space and not your own user!\n\n\n\nCreating a git repo online and copying your project folder\nVersion controlling your data analysis folders, a.k.a. Project folder, is very easy once you have set up your cookiecutter templates. The simplest way of doing this is to first create a remote GitHub repository from the webpage (or from the Desktop app, if you are using it) with a proper project name. Then git clone that repository you just made into your Projects main folder. Then, use cookiecutter to create a project folder template and copy-paste the contents of the folder template to your cloned repo. Remember to fill up your metadata and description files! If you wish, you could already git add, commit and push the first changes to the folders and continue from there on.\nGo back to the course material lesson 5 and read the differences between converting folders to git repositories and cloning a folder to an existing git repository.\n\n\n\n\n\n\nTips to write good commit messages\n\n\n\nIf you would like to know more about Git commits and the best way to make clear git messages, check out this post!\n\n\n\n\nGitHub Pages\nOnce you have created your repository (and put it in GitHub), you have now the opportunity to add your data analysis reports that you created, in either Jupyter Notebooks, Rmarkdowns or html reports, in a GitHub Page website. Creating a GitHub page is very simple, and we really recommend that you follow the nice tutorial that GitHub as put for you. Nonetheless, we will see the main steps in the exercise below.\nThere are many different ways to create your webpages. We recommend using Mkdocs and Mkdocs materials as a framework to create a nice webpage in a simple manner. The folder templates that we used as an example in the previous exercise already contain everything you need to start a webpage. Nonetheless, you will need to understand the basics of MkDocs and MkDocs materials to design a webpage to your liking. MkDocs is a static webpage generator that is very easy to use, while MkDocs materials is an extension of the tool that gives you many more options to customize your website. Check out their webpages to get started!\n\n\n\n\n\n\nExercise 5: make a project folder and publish a data analysis webpage\n\n\n\n\n\n\n\n\nConfigure your main GitHub Page and its repo\nThe first step is to set up the main GitHub Page site and the repository that will host it. This is very simple, as you will only need to follow these steps. In a Markdown document, outline the primary objectives of the organization and provide an overview of ongoing research projects. After you have created the organization/usernamegithub.io, it is time to configure your Project repository webpage using MkDocs!\nStart a new project from cookiecutter or use one from the previous exercise.\nIf you use a Project repo from the first exercise, go to the next paragraph. Using cookiecutter, create a new data analysis project. Remember to fill up your metadata and description files! After you have created the folder, it would be best to initialize a Git repo following the instructions from the previous section.\nNext, link your data of interest (or create a small fake dataset) and make an example of data analysis notebook/report (this could be just a scatter plot of a random matrix of values). Depending on your setup, you might be using Jupyter Notebooks or Rmarkdowns. The extensions that we have installed using pip allows you to directly add a Jupyter Notebook file to the mkdocs.yml navigation section. On the other hand, if you are using Rmarkdown, you will have to knit your document into either an html page or a github document.\nFor the purposes of this exercise, we have already included a basic index.md markdown file that can serve as the intro page of your repo, and a jupyter_example.ipynb with some code in it. You are welcome to modify them further to test them out!\nUse MkDocs to create your webpage\nWhen you are happy with your files and are ready too publish them, make sure to add, commit and push the changes to the remote. Then, build up your webpage using MkDocs and the mkdocs gh-deploy command from the same directory where the mkdocs.yml file is. For example, if your mkdocs.yml for your Project folder is in /Users/JARH/Projects/project1_JARH_20231010/mkdocs.yml, do cd /Users/JARH/Projects/project1_JARH_20231010/ and then mkdocs gh-deploy. This requires a couple of changes in your GitHub organization settings.\nRemember to make sure that your markdowns, images, reports, etc., are included in the docs folder and properly set up in the navigation section of your mkdocs.yml file.\nFinally, we only need to set up the GitHub Project repo settings.\nPublishing your GitHub Page\nGo to your GitHub repo settings and configure the Page section. Since you are using the mkdocs gh-deploy command to publish your site in the gh-pages branch (as explained the the mkdocs documentation), we need to change where GitHub is fetching the website. You will need to configure the settings of this repositories in GitHub, so that the Page is taken from the gh-pages branch and the root folder.\n\n\n\nGitHub Pages setup\n\n\n\nBranch should be gh-pages\nFolder should be root\n\nAfter a couple of minutes, your webpage should be ready! You should be able to see your webpage through the link provided in the Page section!\n\nNow it is also possible to include this repository webpage in your main webpage organizationgithub.io by including the link of the repo website (https://organizationgithub.io/repo-name) in the navigation section of the mkdocs.yml file in the main organizationgithub.io repo."
  },
  {
    "objectID": "develop/practical_workshop.html#archive-github-repositories-on-zenodo",
    "href": "develop/practical_workshop.html#archive-github-repositories-on-zenodo",
    "title": "Practical material",
    "section": "6. Archive GitHub repositories on Zenodo",
    "text": "6. Archive GitHub repositories on Zenodo\nArchives are dedicated digital platforms designed for the secure storage, curation, and dissemination of scientific data. These repositories hold great importance in the research community as they serve as reliable archives for preserving valuable datasets. Their standardized formats and robust curation processes ensure the long-term accessibility and citability of research findings. Researchers worldwide rely on these repositories to share, discover, and validate scientific information, thereby fostering transparency, collaboration, and the advancement of knowledge across various domains of study.\nThe next practical exercise will be to archive your Project folder that contains the data analyses performed on your NGS data in a repository like Zenodo. We can do this by linking your Zenodo account to your GitHub account.\n\n\n\n\n\n\nArchiving your NGS data\n\n\n\nIn this practical lesson, we will only archive our data analyses in the Project folders. Your actual NGS data should be deposited in a domain-specific archive such as Gene Expression Omnibus (GEO) or Annotare. If you want to know more about these archives, check out this lesson\n\n\n\nZenodo\nZenodo[https://zenodo.org/] is an open-access digital repository designed to facilitate the archiving of scientific research outputs. It operates under the umbrella of the European Organization for Nuclear Research (CERN) and is supported by the European Commission. Zenodo accommodates a broad spectrum of research outputs, including datasets, papers, software, and multimedia files. This versatility makes it an invaluable resource for researchers across a wide array of domains, promoting transparency, collaboration, and the advancement of knowledge on a global scale.\nOperating on a user-friendly web platform, Zenodo allows researchers to easily upload, share, and preserve their research data and related materials. Upon deposit, each item is assigned a unique Digital Object Identifier (DOI), granting it a citable status and ensuring its long-term accessibility. Additionally, Zenodo provides robust metadata capabilities, enabling researchers to enrich their submissions with detailed contextual information. In addition, it allows you to link you GitHub account, providing a streamlined way to archive a specific release of your GitHub repository directly into Zenodo. This integration simplifies the process of preserving a snapshot of your project‚Äôs progress for long-term accessibility and citation.\n\n\n\n\n\n\nExercise 6: Archive a Project GitHub repo in Zenodo\n\n\n\n\n\n\n\n\nIn order to archive your GitHub repos in Zenodo, you will first need to link your Zenodo and GitHub accounts.\nOnce your accounts are linked, go to your Zenodo GitHub account settings and turn on the GitHub repository you want to archive. \nCreating a Zenodo archive is now as simple as making a release in your GitHub repository. Remember to make a proper tag! NOTE: If you make a release before enabling the GitHub repository in Zenodo, it will not appear in Zenodo! \nZenodo will automatically detect the release and it should appear in your Zenodo upload page. \nThis archive is assigned a unique Digital Object Identifier (DOI), making it a citable reference for your work. \n\nBefore submitting your work in a journal, make sure to link your data analysis repository to Zenodo, get a DOI and cite it in your manuscript!"
  },
  {
    "objectID": "develop/practical_workshop.html#wrap-up",
    "href": "develop/practical_workshop.html#wrap-up",
    "title": "Practical material",
    "section": "Wrap up",
    "text": "Wrap up\nIn this small workshop we have learned how improve the FAIRability of your data, as well as organizing and structuring it in a way that will be much more useful in the future. This advantages do not serve yourself only, but your teammates, group leader and the general scientific population! We hope that you found this workshop useful. If you would like to leave us some comments or suggestions, feel free to answer this form!"
  },
  {
    "objectID": "practical_workflows.html",
    "href": "practical_workflows.html",
    "title": "Computational Research Data Management",
    "section": "",
    "text": "Workflow\nIf you develop your own software make sure you follow FAIR principles. We highly endorse these recommendations: https://fair-software.nl/endorse\nRegister your computational workflow here: https://workflowhub.eu/\n\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "cards/JARomero.html",
    "href": "cards/JARomero.html",
    "title": "Jos√© Alejandro Romero Herrera",
    "section": "",
    "text": "Alex is a former Sandbox data scientist at the University of Copenhagen. He is currently working at Lundbeck as a principal bioinformatician.\n\n\n\nCopyrightCC-BY-SA 4.0 license"
  },
  {
    "objectID": "use_cases.html",
    "href": "use_cases.html",
    "title": "RDM use cases",
    "section": "",
    "text": "Here, you will find practical examples demonstrating the application of omics data.\n\n\n\nCopyrightCC-BY-SA 4.0 license",
    "crumbs": [
      "Use cases",
      "RDM use cases"
    ]
  }
]